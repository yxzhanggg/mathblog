I"ùC<h1 id="basics">Basics</h1>

<h4 id="vector">vector</h4>

<p>A vector in $n$ dimension space has $n$ components $v_1, v_2,\ldots,v_n$.</p>

\[\boldsymbol{v}+\boldsymbol{w}=\left(v_{1}+w_{1}, v_{2}+w_{2}\right)\]

\[c \boldsymbol{v}=\left(c v_{1}, c v_{2}\right)\]

<h4 id="length-of-a-vector">Length of a vector</h4>

\[\text { length }=\|\boldsymbol{v}\|=\sqrt{\boldsymbol{v} \cdot \boldsymbol{v}}=\left(v_{1}^{2}+v_{2}^{2}+\cdots+v_{n}^{2}\right)^{1 / 2}\]

<h4 id="linear-combination">Linear combination</h4>

\[c \boldsymbol{u}+d \boldsymbol{v}+e \boldsymbol{w}\]

<h4 id="cosine-formula">Cosine formula</h4>

\[\cos \theta=\frac{\boldsymbol{v} \cdot \boldsymbol{w}}{\|\boldsymbol{v}\|\|\boldsymbol{w}\|}\]

<h4 id="schwarz-inequality">Schwarz inequality</h4>

\[\|\boldsymbol{v} \cdot \boldsymbol{w}\| \leq{\|\boldsymbol{v}\|\|\boldsymbol{w}\|}\]

<h4 id="triangle-inequality">Triangle Inequality</h4>

\[\|\boldsymbol{v}+\boldsymbol{w}\| \leq\|\boldsymbol{v}\|+\|\boldsymbol{w}\|\]

<h1 id="different-viewpoints">Different viewpoints</h1>

<h4 id="row-picture">Row picture</h4>

\[\begin{aligned}
x-2 y &amp;=1 \\
3 x+2 y &amp;=11
\end{aligned}\]

<p>The <strong>row picture</strong> shows two lines meeting at a single point (the solution).</p>

<p><img src="https://live.staticflickr.com/65535/52144395134_e9909f4531_o.png" alt="Screen Shot 2022-06-13 at 22.00.48" /></p>

<h4 id="column-picture">Column picture</h4>

\[\left[\begin{array}{rr}
1 &amp; -2 \\
3 &amp; 2
\end{array}\right]\left[\begin{array}{l}
3 \\
1
\end{array}\right]=\left[\begin{array}{r}
1 \\
11
\end{array}\right]\]

<p>The <strong>column picture</strong> combines the column vectors on the left side to produce the vector b on the right side.</p>

<p><img src="https://live.staticflickr.com/65535/52144183903_039eee5beb_o.png" alt="Screen Shot 2022-06-13 at 22.03.47" /></p>

<h4 id="hyperplane">Hyperplane</h4>

<p>the plane $n&gt;3$</p>

<h1 id="matrix-multiplication">Matrix multiplication</h1>

<h4 id="direct-method">Direct method</h4>

<p>The entry in row $i$ and column $j$ of $A B$ is $($ row $i$ of $A) \cdot($ column $j$ of $B)$.</p>

<p><img src="https://live.staticflickr.com/65535/52146378173_1a6be3cdab_o.png" alt="Screen Shot 2022-06-14 at 18.27.10" /></p>

<h4 id="column-picture-1">Column picture</h4>

<p>Matrix $A$ times every column of $B$</p>

\[A\left[\boldsymbol{b}_{1} \cdots \boldsymbol{b}_{p}\right]=\left[A \boldsymbol{b}_{1} \cdots A \boldsymbol{b}_{p}\right]\]

<h4 id="row-picture-1">Row picture</h4>

<p>Every row of $A$ times matrix $B$</p>

\[[\text { row } i \text { of } A]B=[\text { row } i \text { of } A B]\]

<p>$AB= (m \times n)(n \times p) = (m \times p)$, $mp$ dot products with $n$ steps each.</p>

<h4 id="columns-by-rows">Columns by rows</h4>

<p>Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices.</p>

\[\left[\begin{array}{ccc}
\operatorname{col} 1 &amp; \text { col } 2 &amp; \text { col 3} \\
\cdot &amp; \cdot &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdot
\end{array}\right]\left[\begin{array}{ccc}
\text { row 1 } &amp; \cdots \cdot \\
\text { row 2 } &amp; \cdots \cdot \\
\text { row } 3 &amp; \cdots \cdot
\end{array}\right]=(\operatorname{col} 1)(\text { row 1) }+(\operatorname{col} 2)(\text { row 2) }+(\operatorname{col} 3)(\text { row 3) }\]

<h4 id="schur-complement">Schur complement</h4>

\[S=D-C A^{-1} B\]

\[\left[\begin{array}{c|c}
I &amp; \mathbf{0} \\
\hline-C A^{-1} &amp; I
\end{array}\right]\left[\begin{array}{c|c}
A &amp; B \\
\hline C &amp; D
\end{array}\right]=\left[\begin{array}{c|c}
A &amp; B \\
\hline \mathbf{0} &amp; \boldsymbol{D}-\boldsymbol{C A}^{-\mathbf{1}} \boldsymbol{B}
\end{array}\right]\]

<h1 id="inversion">Inversion</h1>

<h4 id="inverse-matrix">Inverse matrix</h4>

<p>The matrix $A$ is invertible if there exists a matrix $A^{-1}$ that ‚Äúinverts‚Äù $A$:</p>

\[A^{-1} A=I \quad \text { and } \quad A A^{-1}=I\]

<ul>
  <li>Two-sided inversion: square matrix</li>
  <li>One-sided inversion: rectangular matrix</li>
</ul>

<h4 id="inversibility">Inversibility</h4>

<p>The inverse exists if and only if <strong>elimination</strong> produces $n$ pivots.</p>

<h5 id="properties">Properties</h5>

<ol>
  <li>
    <p>$(A B C)^{-1}=C^{-1} B^{-1} A^{-1}$</p>
  </li>
  <li>
    <p>$K$ is symmetric across its main diagonal. Then $K^{-1}$ is also symmetric.</p>
  </li>
  <li>
    <p>$K$ is tridiagonal (only three nonzero diagonals = band). But $K^{-1}$ is a dense matrix with no zeros.</p>
  </li>
  <li>
    <p>An invertible matrix cannot have a zero determinant.</p>
  </li>
  <li>
    <p>If $A$ is invertible and upper triangular, so is $A^{-1}$. Start with $A A^{-1}=I$.</p>
  </li>
  <li>
    <p>Diagonally dominant matrices are invertible:</p>

\[\left|\boldsymbol{a}_{i i}\right|&gt;\sum_{j \neq i}\left|\boldsymbol{a}_{i j}\right|\]
  </li>
</ol>

<p>For <strong>elimination matrix</strong> $E$</p>

<p><img src="https://live.staticflickr.com/65535/52171580522_a0f5588f13_o.png" alt="Screen Shot 2022-06-25 at 22.59.14" /></p>

<h4 id="gauss-jordan-method">Gauss-Jordan method</h4>

<p>The Gauss-Jordan method computes $A^{-1}$ by solving all $n$ equations together.</p>

\[A A^{-1}=A\left[\begin{array}{lll}
x_{1} &amp; x_{2} &amp; x_{3}
\end{array}\right]=\left[\begin{array}{lll}
e_{1} &amp; e_{2} &amp; e_{3}
\end{array}\right]=I\]

\[A^{-1}\left[\begin{array}{ll}
A &amp; I
\end{array}\right]=E\left[\begin{array}{ll}
A &amp; I
\end{array}\right]=\left[\begin{array}{ll}
I &amp; A^{-1}
\end{array}\right]\]

<p>The matrix $E$ keeps a record.</p>

<h4 id="from-cofactor-matrix">From Cofactor matrix</h4>

\[\left(A^{-1}\right)_{i j}=\frac{C_{j i}}{\operatorname{det} A} \quad \text { and } \quad A^{-1}=\frac{C^{\mathrm{T}}}{\operatorname{det} A}\]

<h1 id="transpose-and-permutation">Transpose and Permutation</h1>

<h4 id="transpose">Transpose</h4>

\[\text{row}\leftrightarrow\text{column} \quad \left(A^{\mathbf{T}}\right)_{i j}=A_{j i}\]

<p>Mathematical definition: $A^{\mathrm{T}}$ is the matrix that makes these two inner products equal for every $x$ and $y$:</p>

\[(A x)^{\mathrm{T}} y=x^{\mathrm{T}}\left(A^{\mathrm{T}} y\right) \quad \text { Inner product of } A x \text { with } y=\text { Inner product of } x \text { with } A^{\mathrm{T}} y\]

<h5 id="properties-1">Properties</h5>

<ol>
  <li>
    <p>SUM: The transpose of $A+B$ is $A^{\mathrm{T}}+B^{\mathrm{T}}$.</p>
  </li>
  <li>
    <p>PRODUCT: $(A B C)^{T}=C^{T}B^{T}A^{T}$.</p>

    <p>$A \boldsymbol{x}$ combines the columns of $A$ while $\boldsymbol{x}^{\mathrm{T}} A^{\mathrm{T}}$ combines the rows of $A^{\mathrm{T}}$ .</p>
  </li>
  <li>
    <p>INVERSE: The transpose of $A^{-1}$ is $\left(A^{-1}\right)^{\mathrm{T}}=\left(A^{\mathrm{T}}\right)^{-1}$.</p>
  </li>
  <li>
    <p>$A^{\mathrm{T}}$ is invertible exactly when $A$ is invertible.</p>
  </li>
  <li>
    <p>$A^\mathrm{T} A$ is invertible if and only if $A$ has linearly independent columns.</p>
  </li>
  <li>
    <p>$A^{\mathrm{T}} A$ is symmetric.</p>
  </li>
  <li>
    <p>Transformation perspective: transpose matrix is an n-dimensional scissors.</p>
  </li>
</ol>

<h4 id="inner-products--dot-product-mathbft-is-inside">Inner products / Dot product ($^\mathbf{T}$ is inside)</h4>

\[x^{\mathrm{T}} y \quad(1 \times n)(n \times 1)\]

<h4 id="outer-products--rank-one-product-mathbft-is-inside">Outer products / Rank one product ($^\mathbf{T}$ is inside)</h4>

\[x y^{\mathrm{T}} \quad(n \times 1)(1 \times n)\]

<h4 id="symmetric-matrix">Symmetric Matrix</h4>

<p>Definition: A symmetric matrix has $S^{\mathrm{T}}=S .$ This means that $s_{j i}=s_{i j}$.</p>

<h5 id="properties-2">Properties</h5>

<p>The inverse of a symmetric matrix is also symmetric.  $\left(S^{-1}\right)^{\mathrm{T}}=\left(S^{\mathrm{T}}\right)^{-1}=S^{-1}$</p>

<h4 id="samathrmt-a">$S=A^{\mathrm{T}} A$</h4>

<p>The transpose of $A^{\mathrm{T}} A$ is $A^{\mathrm{T}}\left(A^{\mathrm{T}}\right)^{\mathrm{T}}$ which is $A^{\mathrm{T}} A$ again.</p>

<h4 id="permutation-matrix">Permutation Matrix</h4>

<p>Definition: A permutation matrix $P$ has the rows of the identity $I$ in any order.</p>

\[P P^{\mathrm{T}}=I \leftrightarrow P^{\mathrm{T}}=P^{-1}\]

<h4 id="palu-quad--quad-alpu">$PA=LU \quad / \quad A=LPU$</h4>

<p>When there is a row exchange, the sign of determinant is reversed.</p>

<h1 id="determinant">Determinant</h1>

\[\begin{aligned}
&amp;\text {The determinant of an } n \text { by } n \text { matrix can be found in three ways: }\\
&amp;\begin{array}{ll}
1 \text { Multiply the } n \text { pivots (times } 1 \text { or }-1) &amp; \text { This is the pivot formula. } \\
2 \text { Add up } n \text { ! terms (times } 1 \text { or }-1) &amp; \text { This is the "big' formula. } \\
\mathbf{3} \text { Combine } n \text { smaller determinants (times } 1 \text { or }-1 \text { ) } &amp; \text { This is the cofactor formula. }
\end{array}
\end{aligned}\]

<h2 id="properties-3">Properties</h2>

\[\operatorname{det} A \text { and }|A|\]

<ol>
  <li>
    <p>The determinant of the $n$ by $n$ identity matrix is $1 .$</p>
  </li>
  <li>
    <p>The determinant changes sign when two rows are exchanged (sign reversal):</p>
  </li>
  <li>
    <p>The determinant is a linear function of each row separately (all other rows stay fixed).</p>

    <p><img src="https://live.staticflickr.com/65535/52201208012_3ee0721520_o.png" alt="Screen Shot 2022-07-08 at 3.19.48 PM" /></p>
  </li>
  <li>4 If two rows of $A$ are equal, then $\operatorname{det} A=0$.</li>
  <li>Subtracting a multiple of one row from another row leaves $\operatorname{det} A$ unchanged.</li>
  <li>A matrix with a row of zeros has $\operatorname{det} A=0$.</li>
  <li>If $A$ is triangular then $\operatorname{det} A=a_{11} a_{22} \cdots a_{n n}= \text{product of diagonal entries}$.</li>
  <li>If $A$ is singular then $\operatorname{det} A=0$. If $A$ is invertible then $\operatorname{det} A \neq 0$.</li>
  <li>The determinant of $A B$ is $\operatorname{det} A$ times $\operatorname{det} B:|A B|=|A||B|$.</li>
  <li>
\[A^{\mathrm{T}}=\operatorname{det} A\]
  </li>
</ol>

<h2 id="calculate-the-determinant">Calculate the determinant</h2>

<h4 id="the-pivot-formula">The Pivot Formula</h4>

\[\operatorname{det} A=(\operatorname{det} L)(\operatorname{det} U)=(1)\left(d_{1} d_{2} \cdots d_{n}\right)\]

<p>Pivots from determinants:</p>

<p>The $k$ th pivot is $\boldsymbol{d}<em>{\boldsymbol{k}}=\frac{d</em>{1} d_{2} \cdots d_{k}}{d_{1} d_{2} \cdots d_{k-1}}=\frac{\operatorname{det} A_{k}}{\operatorname{det} A_{k-1}}$.</p>

<h4 id="the-big-formula-for-determinants">The Big Formula for Determinants</h4>

<p>$\operatorname{det} A=$ sum over all $\mathbf{n} !$ column permutations $P=(\alpha, \beta, \ldots, \omega)$
\(=\sum(\operatorname{det} P) a_{1 \alpha} a_{2 \beta} \cdots a_{n \omega}=\text { BIG FORMULA. }\)</p>

<h4 id="determinant-by-cofactors">Determinant by Cofactors</h4>

<p>The determinant is the dot product of any row $i$ of $A$ with its cofactors using other rows: COFACTOR FORMULA
\(\operatorname{det} A=a_{i 1} C_{i 1}+a_{i 2} C_{i 2}+\cdots+a_{i n} C_{i n} \text {. }\)
Each cofactor $C_{i j}$ (order $n-1$, without row $i$ and column $j$ ) includes its correct sign:
Cofactor $C_{i j}=(-1)^{i+j} \operatorname{det} M_{i j}$</p>

<h1 id="solving">Solving</h1>

<h4 id="elimination">Elimination</h4>

<p>To <strong>eliminate</strong> $x$: Subtract a multiple of equation $1$ from equation $2$.</p>

<p><strong>Pivot</strong> = first nonzero in the row that does the elimination</p>

<p><strong>Multiplier</strong> = (entry to eliminate) divided by (pivot) = $\ell_{i j}=\frac{\text { entry to eliminate in row } i}{\text { pivot in row } j}$</p>

<p><img src="https://live.staticflickr.com/65535/52144200581_0e7dc90af3_o.png" alt="Screen Shot 2022-06-13 at 22.19.20" /></p>

<h5 id="possible-result">Possible result</h5>

<ol>
  <li>
    <p>Permanent failure with no solution.<img src="https://live.staticflickr.com/65535/52144226868_1a0fc2ddbd_o.png" alt="Screen Shot 2022-06-13 at 22.21.18" /></p>
  </li>
  <li>
    <p>Failure with infinitely many solutions.</p>
  </li>
</ol>

<p><img src="https://live.staticflickr.com/65535/52196122935_d6b8d13a41_o.png" alt="Screen Shot 2022-07-05 at 3.38.19 PM" /></p>

<h4 id="a--lu">$A = LU$</h4>

<p>This is elimination without row exchanges. The multipliers $l_{ij}$ are below the diagonal of $L$.</p>

<h5 id="elimination-factorization">Elimination: factorization</h5>

\[A=L U=\text { (lower triangular) (upper triangular) }=(E_1^{-1}E_2^{-1}\ldots E_n^{-1})U\]

<p>Elimination on $A$ requires about $\frac{1}{3} n^{3}$ multiplications and $\frac{1}{3} n^{3}$ subtractions.</p>

<h5 id="substitution-solve">Substitution: solve</h5>

\[\text { Forward and backward: Solve } \quad L c=b \quad \text { and then solve } \quad U x=c\]

<p>Each right side $b/c$ needs $n^{2}$ multiplications and $n^{2}$ subtractions.</p>

<ul>
  <li>Band matrix $B$ (with $w$ nonzero diagonals)</li>
</ul>

<p>$A$ to $U$: $\frac{1}{3} n^{3}$ reduces to $n w^{2}$</p>

<p>Solve: $n^{2}$ reduces to $2 n w$</p>

<h4 id="a--ldu">$A = LDU$</h4>

\[\text { Split } U \text { into }\left[\begin{array}{llll}
d_{1} &amp; &amp; &amp; \\
&amp; d_{2} &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; d_{n}
\end{array}\right]\left[\begin{array}{cccc}
1 &amp; u_{12} / d_{1} &amp; u_{13} / d_{1} &amp; \cdot \\
&amp; 1 &amp; u_{23} / d_{2} &amp; \cdot \\
&amp; &amp; \ddots &amp; \vdots \\
&amp; &amp; &amp; 1
\end{array}\right]\]

<h4 id="sldlmathrmt">$S=LDL^{\mathrm{T}}$</h4>

<p>If $S=S^{\mathrm{T}}$ is factored into $L D U$ with no row exchanges, then $U$ is exactly $L^{\mathrm{T}}$.</p>

<h4 id="a--lpu">$A = LPU$</h4>

<p>permutation</p>

<h4 id="inversion-1">Inversion</h4>

\[\text { Multiply } A x=b \text { by } A^{-1} \text {. Then } x=A^{-1} A x=A^{-1} b \text {. }\]

<h4 id="cramers-rule">Cramer‚Äôs Rule</h4>

<p>explicit formula but huge complexity $(n+1) !$</p>

<p>If $\operatorname{det} A$ is not zero, $A \boldsymbol{x}=\boldsymbol{b}$ is solved by determinants:
\(x_{1}=\frac{\operatorname{det} B_{1}}{\operatorname{det} A} \quad x_{2}=\frac{\operatorname{det} B_{2}}{\operatorname{det} A} \quad \ldots \quad x_{n}=\frac{\operatorname{det} B_{n}}{\operatorname{det} A}\)
The matrix $B_{j}$ has the jth column of A replaced by the vector $b$.</p>

<p>Set $B=I$ we can find inverse matrix.</p>

\[[A]\left[\begin{array}{lll}
\boldsymbol{x}_{1} &amp; 0 &amp; 0 \\
\boldsymbol{x}_{2} &amp; 1 &amp; 0 \\
\boldsymbol{x}_{3} &amp; 0 &amp; 1
\end{array}\right]=\left[\begin{array}{lll}
\boldsymbol{b}_{\mathbf{1}} &amp; a_{12} &amp; a_{13} \\
\boldsymbol{b}_{\mathbf{2}} &amp; a_{22} &amp; a_{23} \\
\boldsymbol{b}_{\mathbf{3}} &amp; a_{32} &amp; a_{33}
\end{array}\right]=B_{1}\]

\[(\operatorname{det} A)\left(x_{1}\right)=\operatorname{det} B_{1} \quad \text { or } \quad x_{1}=\frac{\operatorname{det} B_{1}}{\operatorname{det} A} .\]

<p>Take determinants of the three matrices to find $x_1$:</p>

<p>Product rule
\((\operatorname{det} A)\left(x_{1}\right)=\operatorname{det} B_{1} \quad \text { or } \quad x_{1}=\frac{\operatorname{det} B_{1}}{\operatorname{det} A}\)</p>

<h1 id="derivative-a">Derivative $A$</h1>

<h4 id="inner-product-of-functions">Inner product of functions</h4>

<ol>
  <li>
\[x^{\mathbf{T}} y=(x, y)=\int_{-\infty}^{\infty} x(t) y(t) d t\]
  </li>
  <li>
\[(A \boldsymbol{x})^{\mathrm{T}} \boldsymbol{y}=\boldsymbol{x}^{\mathrm{T}}\left(A^{\mathrm{T}} \boldsymbol{y}\right)\]
  </li>
  <li>
\[(A x, y)=\int_{-\infty}^{\infty} \frac{d x}{d t} y(t) d t=\int_{-\infty}^{\infty} x(t)\left(-\frac{d y}{d t}\right) d t=\left(x, A^{\mathrm{T}} y\right)\]
  </li>
</ol>

<p>the transpose of the derivative is minus the derivative</p>

\[A=d / d t \quad A^{\mathrm{T}}=-d / d t\]

<h4 id="forward-difference-matrix-to-backward-difference-matrix">Forward difference matrix to backward difference matrix</h4>

\[A=\left[\begin{array}{rrrr}
0 &amp; 1 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; -1 &amp; 0
\end{array}\right] \quad \text { transposes to } \quad A^{\mathrm{T}}=\left[\begin{array}{rrrr}
0 &amp; -1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right]=-A\]

<ul>
  <li>1st derivative is antisymmetric</li>
  <li>2st derivative as symmetric</li>
</ul>

<h1 id="cross-product">Cross Product</h1>

<p><img src="https://live.staticflickr.com/65535/52203010674_da70e69066_o.png" alt="Screen Shot 2022-07-08 at 8.28.32 PM" /></p>

\[\|\boldsymbol{u} \times \boldsymbol{v}\|=\|\boldsymbol{u}\|\|\boldsymbol{v}\||\sin \theta| \quad \text { and } \quad|\boldsymbol{u} \cdot \boldsymbol{v}|=\|\boldsymbol{u}\|\|\boldsymbol{v}\||\cos \theta|\]

<p><img src="https://live.staticflickr.com/65535/52202748236_4471781510_o.png" alt="Screen Shot 2022-07-08 at 8.32.14 PM" /></p>

<p><img src="https://live.staticflickr.com/65535/52202750006_a2f5cbf4bf_o.png" alt="Screen Shot 2022-07-08 at 8.33.19 PM" /></p>

<h4 id="turning-force-or-torque">turning force or torque</h4>

\[u \times F\]

<p>position of a mass: $\left(u_{1}, u_{2}, u_{3}\right)$</p>

<p>force acting on it: $\left(F_{x}, F_{y}, F_{z}\right)$</p>

<h4 id="moment">moment</h4>

<p>$|\boldsymbol{u}||\boldsymbol{F}| \sin \theta$</p>
:ET