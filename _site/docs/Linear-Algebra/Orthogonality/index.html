<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/mathblog/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/mathblog/assets/css/just-the-docs-default.css"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2709176-10"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-2709176-10', { 'anonymize_ip': true }); </script> <script type="text/javascript" src="/mathblog/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/mathblog/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Orthogonality | Yaoxin’s Notes</title><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Orthogonality" /><meta property="og:locale" content="en_US" /><link rel="canonical" href="http://localhost:4000/mathblog/docs/Linear-Algebra/Orthogonality/" /><meta property="og:url" content="http://localhost:4000/mathblog/docs/Linear-Algebra/Orthogonality/" /><meta property="og:site_name" content="Yaoxin’s Notes" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Orthogonality" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","headline":"Orthogonality","url":"http://localhost:4000/mathblog/docs/Linear-Algebra/Orthogonality/"}</script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/mathblog/" class="site-title lh-tight"> Yaoxin's Notes </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/mathblog/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Fundamentals" class="nav-list-link">Fundamentals</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Logic/" class="nav-list-link">Logic</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Complex-Numbers/" class="nav-list-link">Complex Numbers</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Norms/" class="nav-list-link">Norms</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Mechanics/" class="nav-list-link">Mechanics</a></ul><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Linear-Algebra" class="nav-list-link">Linear Algebra</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Fundamentals/" class="nav-list-link">What is matrix (Ax = b)</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Spaces/" class="nav-list-link">Spaces</a><li class="nav-list-item active"><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Orthogonality/" class="nav-list-link active">Orthogonality</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Eigen/" class="nav-list-link">Eigens</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/SVD/" class="nav-list-link">SVD</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Transformation/" class="nav-list-link">Transformation</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Complex/" class="nav-list-link">Complex</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/App/" class="nav-list-link">Applications</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Num/" class="nav-list-link">Numerical</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/PS/" class="nav-list-link">Probability & Statistics</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Info-Prob" class="nav-list-link">Information and Probability</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Info-Prob/Com/" class="nav-list-link">Communication</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Signals-Systems" class="nav-list-link">Signals and Systems</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Fundamentals/" class="nav-list-link">Fundamentals</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Transformations/" class="nav-list-link">Transformations</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Process%20Dynamics%20and%20Control/" class="nav-list-link">Process Dynamics and Control</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Convex-Optimization" class="nav-list-link">Convex Optimization</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Convex-Optimization/Convex-set/" class="nav-list-link">Convex set</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Machine-Learning" class="nav-list-link">Machine Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Machine-Learning/SuLearning/" class="nav-list-link">x</a></ul></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Yaoxin's Notes" aria-label="Search Yaoxin's Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/mathblog/docs/Linear-Algebra">Linear Algebra</a><li class="breadcrumb-nav-list-item"><span>Orthogonality</span></ol></nav><div id="main-content" class="main-content" role="main"><h4 id="orthogonal-vectors"> <a href="#orthogonal-vectors" class="anchor-heading" aria-labelledby="orthogonal-vectors"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Orthogonal vectors</h4>\[\boldsymbol{v}^{\mathrm{T}} \boldsymbol{w}=0 \quad \text { and } \quad\|\boldsymbol{v}\|^{2}+\|\boldsymbol{w}\|^{2}=\|\boldsymbol{v}+\boldsymbol{w}\|^{2}\]<h4 id="orthogonal"> <a href="#orthogonal" class="anchor-heading" aria-labelledby="orthogonal"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Orthogonal</h4><p>Definition: Two subspaces $\boldsymbol{V}$ and $\boldsymbol{W}$ of a vector space are <strong>orthogonal</strong> if every vector $\boldsymbol{v}$ in $V$ is perpendicular to every vector $w$ in $W$.</p><p>The unit vectors $q_{1}, \ldots, q_{n}$ are <strong>orthonormal</strong> if</p>\[\boldsymbol{q}_{i}^{\mathrm{T}} \boldsymbol{q}_{j}=\left\{\begin{array}{lll} 0 &amp; \text { when } i \neq j &amp; \text { (orthogonal vectors) } \\ 1 &amp; \text { when } i=j &amp; \left(\text { unit vectors: }\left\|\boldsymbol{q}_{i}\right\|=1\right) \end{array}\right.\]<p>A matrix with <strong>unit</strong> orthonormal columns is assigned the special letter $Q$.</p>\[\boldsymbol{Q}^{\mathrm{T}} \boldsymbol{Q}=\left[\begin{array}{c}-\boldsymbol{q}_{1}^{\mathrm{T}}- \\ -\boldsymbol{q}_{2}^{\mathrm{T}}- \\ -\boldsymbol{q}_{n}^{\mathrm{T}}-\end{array}\right]\left[\begin{array}{ccc}1 &amp; \mid &amp; 1 \\ \boldsymbol{q}_{1} &amp; \boldsymbol{q}_{2} &amp; \boldsymbol{q}_{n} \\ 1 &amp; \mid &amp; 1\end{array}\right]=\left[\begin{array}{cccc}1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1\end{array}\right]=\boldsymbol{I}\] \[\boldsymbol{v}^{\mathrm{T}} \boldsymbol{w}=0 \text { for all } v \text { in } \boldsymbol{V} \text { and all } \boldsymbol{w} \text { in } W\]<p><img src="https://live.staticflickr.com/65535/52192216197_e291ca7e32_o.png" alt="Screen Shot 2022-07-04 at 4.54.11 PM" /></p><p><img src="https://live.staticflickr.com/65535/52192387322_eaa6507e54_o.png" alt="Screen Shot 2022-07-04 at 6.07.08 PM" /></p><p><img src="https://live.staticflickr.com/65535/52193422546_ff2f26b778_o.png" alt="Screen Shot 2022-07-04 at 6.18.39 PM" /></p><h4 id="orthogonal-complements"> <a href="#orthogonal-complements" class="anchor-heading" aria-labelledby="orthogonal-complements"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Orthogonal Complements</h4><p>Definition:The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^{\perp}$ (pronounced “ $V$ perp”).</p><h4 id="orthogonal-matrix"> <a href="#orthogonal-matrix" class="anchor-heading" aria-labelledby="orthogonal-matrix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Orthogonal matrix</h4><p>When $Q$ is square, $Q^{\mathrm{T}} Q=I$ means that $Q^{\mathrm{T}}=Q^{-1}$ : transpose $=$ inverse, the square matrix is called <strong>orthogonal matrix</strong>.</p><p>Does multiplication by any orthogonal matrix $Q$ - <strong>lengths</strong> and <strong>angles</strong> don’t change.</p><h5 id="rotation"> <a href="#rotation" class="anchor-heading" aria-labelledby="rotation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Rotation</h5><p>$Q$ rotates every vector in the plane by the angle $\theta$ :</p><p>$Q=\left[\begin{array}{rr}\cos \theta &amp; -\sin \theta \ \sin \theta &amp; \cos \theta\end{array}\right] \quad$ and $\quad Q^{\mathrm{T}}=Q^{-1}=\left[\begin{array}{rr}\cos \theta &amp; \sin \theta \ -\sin \theta &amp; \cos \theta\end{array}\right]$</p><h5 id="permutation"> <a href="#permutation" class="anchor-heading" aria-labelledby="permutation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <img src="https://live.staticflickr.com/65535/52202440925_0f72d0ca30_o.png" alt="Screen Shot 2022-07-08 at 11.59.46 AM" />Permutation</h5><p>Every permutation matrix is an orthogonal matrix.</p><h5 id="reflection"> <a href="#reflection" class="anchor-heading" aria-labelledby="reflection"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Reflection</h5><p>If $\boldsymbol{u}$ is any unit vector, set $Q=I-2 u u^{\mathrm{T}}$</p><p>$Q^{\mathrm{T}}=I-2 \boldsymbol{u} \boldsymbol{u}^{\mathrm{T}}=Q \quad$ and $\quad Q^{\mathrm{T}} Q=I-4 \boldsymbol{u} \boldsymbol{u}^{\mathrm{T}}+4 \boldsymbol{u} \boldsymbol{u}^{\mathrm{T}} \boldsymbol{u} \boldsymbol{u}^{\mathrm{T}}=I$</p><p><img src="https://live.staticflickr.com/65535/52202450280_9a9863135b_o.png" alt="Screen Shot 2022-07-08 at 12.07.19 PM" /></p><hr /><h1 id="fundamental-theorem-of-linear-algebra-pt2"> <a href="#fundamental-theorem-of-linear-algebra-pt2" class="anchor-heading" aria-labelledby="fundamental-theorem-of-linear-algebra-pt2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fundamental Theorem of Linear Algebra. Pt.2</h1><p>$\boldsymbol{N}(\boldsymbol{A})$ is the orthogonal complement of the row space $\boldsymbol{C}\left(A^{\mathrm{T}}\right)$ (in $\mathbf{R}^{n}$ ).</p><p>$\boldsymbol{N}\left(A^{\mathrm{T}}\right)$ is the orthogonal complement of the column space $C(A)\left(\right.$ in $\left.\mathbf{R}^{m}\right) .$</p><p><img src="https://live.staticflickr.com/65535/52194314620_5b60203db5_o.png" alt="Screen Shot 2022-07-04 at 9.16.06 PM" /></p><p><strong>After the transformation, the information of nullspace is annihilated. For example after the transformation of first differentiation, the information of position has dissappeared, then second differentiation crash the information of velocity, third accelaration…</strong></p><p>And if we do integration, we need to add the constant C, that integration process is the back-transformation of differentiation, which bring back to the general null space - C position, C velocity, C acceleration…</p><p>Remember the particular solution of integration, it is the information brought back by the transformation feature of the system from the “b”; the general solution is the information of nullspace brought back by the annihilation feature of the system from 0!</p><hr /><h1 id="projection"> <a href="#projection" class="anchor-heading" aria-labelledby="projection"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Projection</h1><h4 id="projection-matrix-p"> <a href="#projection-matrix-p" class="anchor-heading" aria-labelledby="projection-matrix-p"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Projection Matrix $P$</h4>\[\boldsymbol{p}_{1}=P_{1} \boldsymbol{b}=\left[\begin{array}{lll} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right]\left[\begin{array}{l} x \\ y \\ z \end{array}\right]=\left[\begin{array}{l} \mathbf{0} \\ \mathbf{0} \\ \boldsymbol{z} \end{array}\right] \quad \boldsymbol{p}_{2}=P_{2} \boldsymbol{b}=\left[\begin{array}{lll} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]\left[\begin{array}{l} x \\ y \\ z \end{array}\right]=\left[\begin{array}{l} \boldsymbol{x} \\ \boldsymbol{y} \\ \mathbf{0} \end{array}\right]\]<p><img src="https://live.staticflickr.com/65535/52197523148_22dcc3fa63_o.png" alt="Screen Shot 2022-07-06 at 10.51.51 AM" /></p><p>The vectors give $\boldsymbol{p}<em>{1}+\boldsymbol{p}</em>{2}=\boldsymbol{b}$.</p><p>The matrices give $P_{1}+P_{2}=I$.</p><p>The vector $b$ is being split into the projection $p$ and the error $e = b- p$.</p>\[P \boldsymbol{b}=\boldsymbol{p}\]<p>Projection again is still the projection:</p>\[P^{2}=P\] \[P^{\mathrm{T}}=P\]<h4 id="error-e"> <a href="#error-e" class="anchor-heading" aria-labelledby="error-e"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Error $e$</h4><p>The distance from $b$ to the the subspace $C(A)$ is $||e||$</p>\[e=b-\widehat{x} a\]<h4 id="bestclosest-choice-widehatx"> <a href="#bestclosest-choice-widehatx" class="anchor-heading" aria-labelledby="bestclosest-choice-widehatx"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> best/closest choice $\widehat{x}$</h4>\[\begin{gathered} \boldsymbol{a}_{1}^{\mathrm{T}}(\boldsymbol{b}-A \widehat{\boldsymbol{x}})=0 \\ \vdots \\ \boldsymbol{a}_{n}^{\mathrm{T}}(\boldsymbol{b}-A \widehat{\boldsymbol{x}})=0 \end{gathered} \quad \text { or } \quad\left[\begin{array}{c} -\boldsymbol{a}_{1}^{\mathrm{T}}- \\ \vdots \\ -\boldsymbol{a}_{n}^{\mathrm{T}}- \end{array}\right][\boldsymbol{b}-A \widehat{\boldsymbol{x}}]=\left[\begin{array}{l} 0 \end{array}\right]\]<p>Rewrite above (orthogonality):</p>\[A^{\mathrm{T}}(\boldsymbol{b}-A \widehat{\boldsymbol{x}})=\mathbf{0}\] \[A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b}\]<p><img src="https://live.staticflickr.com/65535/52197560778_a0c086264e_o.png" alt="Screen Shot 2022-07-06 at 11.21.18 AM" /></p><p>When $P$ projects onto one subspace, $I - P$ projects onto the perpendicular subspace.</p><h4 id="3-steps-to-find-p"> <a href="#3-steps-to-find-p" class="anchor-heading" aria-labelledby="3-steps-to-find-p"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3 steps to find $P$</h4><ol><li><p>Find the vector $\widehat{x}$</p><li>Find the projection $p=A \widehat{x}$<li>Find the projection matrix $P$</ol><p><img src="https://live.staticflickr.com/65535/52198611826_429549c22f_o.png" alt="Screen Shot 2022-07-06 at 9.16.26 PM" /></p><p><img src="https://live.staticflickr.com/65535/52198893479_c7fa4329f6_o.png" alt="Screen Shot 2022-07-06 at 9.19.41 PM" /></p><h1 id="least-squares-approximations"> <a href="#least-squares-approximations" class="anchor-heading" aria-labelledby="least-squares-approximations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Least Squares Approximations</h1><p>When the length of $\boldsymbol{e}$ is as small as possible, $\widehat{\boldsymbol{x}}$ is $a$ least squares solution.</p><h4 id="minimizing-the-error"> <a href="#minimizing-the-error" class="anchor-heading" aria-labelledby="minimizing-the-error"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Minimizing the error</h4><ol><li><p>By geometry</p><p>The smallest possible error is $e = b - p$, perpendicular to the columns space of $A$.</p><li><p>By algebra</p>\[A \boldsymbol{x}=\boldsymbol{b}=\boldsymbol{p}+\boldsymbol{e} \text { is impossible } \quad A \widehat{\boldsymbol{x}}=\boldsymbol{p} \text { is solvable } \quad \widehat{\boldsymbol{x}} \text { is }\left(A^{\mathrm{T}} A\right)^{-1} A^{\mathrm{T}} \boldsymbol{b} \text {. }\]<p>Squared length for any $x$</p>\[\|A \boldsymbol{x}-\boldsymbol{b}\|^2=\|A \boldsymbol{x}-\boldsymbol{p}\|^2+\|\boldsymbol{e}\|^2\] \[\text { The least squares solution } \widehat{x} \text { makes } E=\|A x-b\|^{2} \text { as small as possible. }\] \[\|A \boldsymbol{x}-\boldsymbol{b}\|^{2}=\boldsymbol{x}^{\mathrm{T}} A^{\mathrm{T}} A \boldsymbol{x}-2 \boldsymbol{x}^{\mathrm{T}} A^{\mathrm{T}} \boldsymbol{b}+\boldsymbol{b}^{\mathrm{T}} \boldsymbol{b}\]<li><p>By calculus</p>\[\text { The partial derivatives of }\|A x-b\|^{2} \text { are zeno when } A^{\mathrm{T}} A \hat{x}=A^{\mathrm{T}} b \text {. } \\\]</ol><p><img src="https://live.staticflickr.com/65535/52199933392_5ae30a21fd_o.png" alt="Screen Shot 2022-07-07 at 10.23.43 PM" /></p><h2 id="fitting-a-straight-line"> <a href="#fitting-a-straight-line" class="anchor-heading" aria-labelledby="fitting-a-straight-line"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fitting a Straight Line</h2>\[\begin{equation} A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \end{equation}\] \[\begin{equation} \text { Dot-product matrix } \boldsymbol{A}^{\mathbf{T}} \boldsymbol{A}=\left[\begin{array}{ccc} 1 &amp; \cdots &amp; 1 \\ t_{1} &amp; \cdots &amp; t_{m} \end{array}\right]\left[\begin{array}{cc} 1 &amp; t_{1} \\ \vdots &amp; \vdots \\ 1 &amp; t_{m} \end{array}\right]=\left[\begin{array}{cc} m &amp; \sum t_{i} \\ \sum t_{i} &amp; \sum t_{i}^{2} \end{array}\right] \text {. } \end{equation}\] \[\begin{equation} A^{\mathrm{T}} \boldsymbol{b}=\left[\begin{array}{ccc} 1 &amp; \cdots &amp; 1 \\ t_{1} &amp; \cdots &amp; t_{m} \end{array}\right]\left[\begin{array}{c} b_{1} \\ \vdots \\ b_{m} \end{array}\right]=\left[\begin{array}{c} \sum b_{i} \\ \sum t_{i} b_{i} \end{array}\right] \end{equation}\] \[\begin{equation} A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \quad\left[\begin{array}{cc} m &amp; \sum t_{i} \\ \sum t_{i} &amp; \sum t_{i}^{2} \end{array}\right]\left[\begin{array}{l} C \\ D \end{array}\right]=\left[\begin{array}{c} \sum b_{i} \\ \sum t_{i} b_{i} \end{array}\right] \end{equation}\]<h2 id="fitting-by-a-parabola"> <a href="#fitting-by-a-parabola" class="anchor-heading" aria-labelledby="fitting-by-a-parabola"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fitting by a Parabola</h2>\[\begin{equation} A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \end{equation}\] \[\begin{array}{cl}C+D t_{1}+E t_{1}^{2}=b_{1} &amp; \text { is } A \boldsymbol{x}=\boldsymbol{b} \text { with } \\ \vdots &amp; \text { the } m \text { by } 3 \text { matrix } \\ C+D t_{m}+E t_{m}^{2}=b_{m} &amp; \end{array} A=\left[\begin{array}{ccc}1 &amp; t_{1} &amp; t_{1}^{2} \\ \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; t_{m} &amp; t_{m}^{2}\end{array}\right]\]<p>Least squares: The closest parabola $C+D t+E t^{2}$ chooses $\widehat{\boldsymbol{x}}=(C, D, E)$ to satisfy the three normal equations $A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b}$.</p><h2 id="q-replace-a"> <a href="#q-replace-a" class="anchor-heading" aria-labelledby="q-replace-a"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Q replace A</h2>\[\begin{equation} A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \end{equation}\] \[A^{\mathrm{T}} A=Q^{\mathrm{T}} Q=I\] \[\widehat{\boldsymbol{x}}=Q^{\mathrm{T}} \boldsymbol{b}\] \[P=A\left(A^{\mathrm{T}} A\right)^{-1} A^{\mathrm{T}}=Q Q^{\mathrm{T}}\]<p>When $Q$ is square and $m=n$. In this case $\boldsymbol{p}=\boldsymbol{b}$ and $P=Q Q^{\mathrm{T}}=I$.</p>\[\boldsymbol{p}=Q \widehat{\boldsymbol{x}}=Q Q^{\mathrm{T}} \boldsymbol{b}\]<p>Every $\boldsymbol{b}=Q Q^{\mathrm{T}} \boldsymbol{b}$ is the sum of its components along the q’s:</p>\[\boldsymbol{b}=\boldsymbol{q}_{1}\left(\boldsymbol{q}_{1}^{\mathrm{T}} \boldsymbol{b}\right)+\boldsymbol{q}_{2}\left(\boldsymbol{q}_{2}^{\mathrm{T}} \boldsymbol{b}\right)+\cdots+\boldsymbol{q}_{n}\left(\boldsymbol{q}_{n}^{\mathrm{T}} \boldsymbol{b}\right)\]<p>Then $b$ is decomposited into the composition of different orthogonal basis.</p><p>Transforms: $Q Q^{\mathrm{T}}=I$ is the foundation of Fourier series and all the great “transforms” of applied mathematics. They break vectors $\boldsymbol{b}$ or functions $f(x)$ into perpendicular pieces. Then by adding the pieces, the inverse transform puts $b$ and $f(x)$ back together.</p><h2 id="use-qr-decomposition"> <a href="#use-qr-decomposition" class="anchor-heading" aria-labelledby="use-qr-decomposition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Use QR decomposition</h2>\[\begin{equation} A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \end{equation}\] \[\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}=(\boldsymbol{Q} \boldsymbol{R})^{\mathrm{T}} \boldsymbol{Q} R=\boldsymbol{R}^{\mathrm{T}} \boldsymbol{Q}^{\mathrm{T}} \boldsymbol{Q} \boldsymbol{R}=\boldsymbol{R}^{\mathrm{T}} \boldsymbol{R}\] \[R^{\mathrm{T}} R \widehat{\boldsymbol{x}}=R^{\mathrm{T}} Q^{\mathrm{T}} \boldsymbol{b} \text { or } R \widehat{\boldsymbol{x}}=Q^{\mathrm{T}} \boldsymbol{b} \text { or } \widehat{\boldsymbol{x}}=R^{-1} Q^{\mathrm{T}} \boldsymbol{b}\]<p>We can solve it by backward substitution. The cost is the $mn^2$ multiplications in the Gram-Schmidt process, which are needed to construct the orthogonal $Q$ and the triangular $R$ with $A = QR$.</p><h1 id="the-gram-schmidt-process"> <a href="#the-gram-schmidt-process" class="anchor-heading" aria-labelledby="the-gram-schmidt-process"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Gram-Schmidt Process</h1><p>One and only one idea: Subtract from every new vector its projections in the directions already set.</p><p>Producing unit orthogonal vectors from existing vectors.</p><ol><li><p>choosing $A=a$</p><li><p>Start with $b$ and subtract its projection along $A$</p></ol><p>$\boldsymbol{B}=\boldsymbol{b}-\frac{\boldsymbol{A}^{\mathrm{T}} \boldsymbol{b}}{\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}} \boldsymbol{A}$</p><ol><li>same process with $c, d, e,\ldots$</ol>\[\boldsymbol{C}=\boldsymbol{c}-\frac{\boldsymbol{A}^{\mathrm{T}} \boldsymbol{c}}{\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}} \boldsymbol{A}-\frac{\boldsymbol{B}^{\mathrm{T}} \boldsymbol{c}}{\boldsymbol{B}^{\mathrm{T}} \boldsymbol{B}} \boldsymbol{B}\]<p>Final step. divide the orthogonal vectors by their lengths.</p><h4 id="aqr"> <a href="#aqr" class="anchor-heading" aria-labelledby="aqr"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> $A=QR$</h4><p>Gram-Schmidt in a nutshell</p>\[A=Q R=(\text { orthogonal } Q)(\text { triangular } R)\] \[\left[\begin{array}{lll}a &amp; b &amp; c\end{array}\right]=\left[\begin{array}{lll} &amp; &amp; \\ \boldsymbol{q}_{1} &amp; \boldsymbol{q}_{2} &amp; \boldsymbol{q}_{3}\\ &amp; &amp; \end{array}\right]\left[\begin{array}{lll}\boldsymbol{q}_{1}^{\mathrm{T}} \boldsymbol{a} &amp; \boldsymbol{q}_{1}^{\mathrm{T}} \boldsymbol{b} &amp; \boldsymbol{q}_{1}^{\mathrm{T}} \boldsymbol{c} \\ &amp; \boldsymbol{q}_{2}^{\mathrm{T}} \boldsymbol{b} &amp; \boldsymbol{q}_{2}^{\mathrm{T}} \boldsymbol{c} \\ &amp; &amp; \boldsymbol{q}_{3}^{\mathrm{T}} \boldsymbol{c}\end{array}\right] \quad \text{or} \quad \boldsymbol{A}=\boldsymbol{Q} \boldsymbol{R}\]<p>Any $m$ by $n$ matrix $A$ with independent columns can be factored into $A = QR$.</p><h2 id="gram-schmidt"> <a href="#gram-schmidt" class="anchor-heading" aria-labelledby="gram-schmidt"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gram-Schmidt</h2><p>From independent vectors $a_{1}, \ldots, a_{n}$, Gram-Schmidt constructs orthonormal vectors $q_{1}, \ldots, q_{n}$.</p><p>The matrices with these columns satisfy $A=Q R$. Then $R=Q^{\mathrm{T}} A$ is upper triangular because later $\boldsymbol{q}$ ‘s are orthogonal to earlier $\boldsymbol{a}$ ‘s.</p><hr><footer><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div>
