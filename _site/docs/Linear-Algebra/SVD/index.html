<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/mathblog/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/mathblog/assets/css/just-the-docs-default.css"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2709176-10"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-2709176-10', { 'anonymize_ip': true }); </script> <script type="text/javascript" src="/mathblog/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/mathblog/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>SVD | Yaoxin’s Notes</title><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="SVD" /><meta property="og:locale" content="en_US" /><link rel="canonical" href="http://localhost:4000/mathblog/docs/Linear-Algebra/SVD/" /><meta property="og:url" content="http://localhost:4000/mathblog/docs/Linear-Algebra/SVD/" /><meta property="og:site_name" content="Yaoxin’s Notes" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="SVD" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","headline":"SVD","url":"http://localhost:4000/mathblog/docs/Linear-Algebra/SVD/"}</script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/mathblog/" class="site-title lh-tight"> Yaoxin's Notes </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/mathblog/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Fundamentals" class="nav-list-link">Fundamentals</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Logic/" class="nav-list-link">Logic</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Complex-Numbers/" class="nav-list-link">Complex Numbers</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Norms/" class="nav-list-link">Norms</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Fundamental/Mechanics/" class="nav-list-link">Mechanics</a></ul><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Linear-Algebra" class="nav-list-link">Linear Algebra</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Fundamentals/" class="nav-list-link">What is matrix (Ax = b)</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Spaces/" class="nav-list-link">Spaces</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Orthogonality/" class="nav-list-link">Orthogonality</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Eigen/" class="nav-list-link">Eigens</a><li class="nav-list-item active"><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/SVD/" class="nav-list-link active">SVD</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Transformation/" class="nav-list-link">Transformation</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Complex/" class="nav-list-link">Complex</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/App/" class="nav-list-link">Applications</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/Num/" class="nav-list-link">Numerical</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Linear-Algebra/PS/" class="nav-list-link">Probability & Statistics</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Info-Prob" class="nav-list-link">Information and Probability</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Info-Prob/Com/" class="nav-list-link">Communication</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Signals-Systems" class="nav-list-link">Signals and Systems</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Fundamentals/" class="nav-list-link">Fundamentals</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Transformations/" class="nav-list-link">Transformations</a><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Signals-Systems/Process%20Dynamics%20and%20Control/" class="nav-list-link">Process Dynamics and Control</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Convex-Optimization" class="nav-list-link">Convex Optimization</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Convex-Optimization/Convex-set/" class="nav-list-link">Convex set</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/mathblog/docs/Machine-Learning" class="nav-list-link">Machine Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/mathblog/docs/Machine-Learning/SuLearning/" class="nav-list-link">x</a></ul></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Yaoxin's Notes" aria-label="Search Yaoxin's Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/mathblog/docs/Linear-Algebra">Linear Algebra</a><li class="breadcrumb-nav-list-item"><span>SVD</span></ol></nav><div id="main-content" class="main-content" role="main"><p>The singular value theorem for $A$ is the eigenvalue theorem for $A^{\mathrm{T}} A$ and $A A^{\mathrm{T}}$.</p>\[\text { Use the eigenvectors } u \text { of } A A^{\mathrm{T}} \text { and the eigenvectors } v \text { of } A^{\mathrm{T}} A\]<h4 id="left-singular-vectors-us"> <a href="#left-singular-vectors-us" class="anchor-heading" aria-labelledby="left-singular-vectors-us"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> left singular vectors $u$’s</h4><p>the principal components</p>\[\text { unit eigenvectors of } A A^{\mathrm{T}}\]<h4 id="right-singular-vectors-vs"> <a href="#right-singular-vectors-vs" class="anchor-heading" aria-labelledby="right-singular-vectors-vs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Right singular vectors $v$’s</h4><p>the combination if principal components</p>\[\text { unit eigenvectors of } A^{\mathrm{T}} A\] \[A A^{\mathrm{T}} \boldsymbol{u}_{i}=\sigma_{i}^{2} \boldsymbol{u}_{i} \quad A^{\mathrm{T}} A \boldsymbol{v}_{i}=\sigma_{i}^{2} \boldsymbol{v}_{i} \quad A \boldsymbol{v}_{i}=\sigma_{i} \boldsymbol{u}_{i}\]<p>$\sigma_{i}$is the length of $A \boldsymbol{v}_{i}$</p><p>A is diagonalized:</p>\[A =\sum_i\boldsymbol{u}_{i}\sigma_{i} \boldsymbol{v}_{i}^\mathrm{T}\]<p><img src="https://s2.loli.net/2022/07/14/GOsTvMWzKrgeExQ.png" alt="image-20220714171111844" /></p><p><img src="https://live.staticflickr.com/65535/52211227194_4448d8f338_o.png" alt="Screen Shot 2022-07-12 at 10.35.14 AM" /></p>\[\sigma_{1} \geq \sigma_{2} \geq \ldots \sigma_{r}&gt;0\]<p>include all the $v$’s and $u$’s:</p><p><img src="https://live.staticflickr.com/65535/52211481700_1576e7f06e_o.png" alt="Screen Shot 2022-07-12 at 11.05.47 AM" /></p><hr /> \[A=U \boldsymbol{\Sigma} V^{\mathbf{T}}=u_{1} \sigma_{1} v_{1}^{\mathbf{T}}+\cdots+u_{r} \sigma_{r} v_{r}^{\mathbf{T}}\]<hr /><p>If $A$ is positive semidefinite (or definite) symmetric matrix,</p>\[A=X \Lambda X^{-1}=Q \Lambda Q^{\mathrm{T}}=U \Sigma V^{\mathrm{T}}\] \[\begin{array}{ll} \text { Symmetric } \boldsymbol{S} &amp; S=Q \Lambda Q^{\mathrm{T}}=\lambda_{1} \boldsymbol{q}_{1} \boldsymbol{q}_{1}^{\mathrm{T}}+\lambda_{2} \boldsymbol{q}_{2} \boldsymbol{q}_{2}^{\mathrm{T}}+\cdots+\lambda_{r} \boldsymbol{q}_{r} \boldsymbol{q}_{r}^{\mathrm{T}} \\ \text { Any matrix } \boldsymbol{A} &amp; A=U \Sigma V^{\mathrm{T}}=\sigma_{1} \boldsymbol{u}_{1} \boldsymbol{v}_{1}^{\mathrm{T}}+\sigma_{2} \boldsymbol{u}_{2} \boldsymbol{v}_{2}^{\mathrm{T}}+\cdots+\sigma_{r} \boldsymbol{u}_{r} \boldsymbol{v}_{r}^{\mathrm{T}} \end{array}\]<h4 id="normal-matrix"> <a href="#normal-matrix" class="anchor-heading" aria-labelledby="normal-matrix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> normal matrix</h4>\[A^{\mathrm{T}} A=A A^{\mathrm{T}}\]<p>the eigenvectors of $A$ are orthogonal and the eigenvalues of $A$ are totally stable.</p><p>but the singular values of any matrix are stable.</p><h4 id="rayleigh-quotient"> <a href="#rayleigh-quotient" class="anchor-heading" aria-labelledby="rayleigh-quotient"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Rayleigh quotient</h4>\[r(\boldsymbol{x})=\boldsymbol{x}^{\mathrm{T}} S \boldsymbol{x} / \boldsymbol{x}^{\mathrm{T}} \boldsymbol{x}\]<p>The derivatives of $r(x)=\frac{x^{\mathrm{T}} S x}{x^{\mathrm{T}} x}$ are zero when $S \boldsymbol{x}=r(\boldsymbol{x}) \boldsymbol{x}$</p><h1 id="principal-component-analysis-singular-value--variance"> <a href="#principal-component-analysis-singular-value--variance" class="anchor-heading" aria-labelledby="principal-component-analysis-singular-value--variance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Principal Component Analysis (singular value = variance)</h1><h4 id="sample-covariance-matrix"> <a href="#sample-covariance-matrix" class="anchor-heading" aria-labelledby="sample-covariance-matrix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> sample covariance matrix</h4><p>The covariance matrix predicts the spread of future measurements around their mean.</p><p>Find the average (the mean) $\mu_{1}, \mu_{2}, \ldots, \mu_{m}$ of each row. Subtract each mean $\mu_{i}$ from row $i$ to center the data.</p>\[S=\frac{A A^{\mathrm{T}}}{n-1}\]<p>$A$ shows the distance $a_{i j}-\mu_{i}$ from each measurement to the row average $\mu_{i}$.</p>\[\left(A A^{\mathrm{T}}\right)_{11}, \left(A A^{\mathrm{T}}\right)_{22}\]<p>show the sum of squared distances (sample variances $s_1^2, s_2^2$ ).</p><p>$\left(A A^{\mathrm{T}}\right)_{12}$ shows the sample covariance:</p>\[s_{12}=(\operatorname{row} 1 \text { of } A) \cdot(\operatorname{row} 2 \text { of } A)\]<p>one degree of freedom was used by the mean so $n-1$</p><p>Since the rows of $A$ have $n$ entries, the numbers in $A A^{\mathrm{T}}$ have size growing like $n$ and the division by $n-1$ keeps them steady.</p><ul><li>The total variance in the data is the sum of all eigenvalues and of sample variances $s^{2}$ : Total variance $T=\sigma_1^2+\cdots+\sigma_m^2=s_1^2+\cdots+s_m^2=$ trace (diagonal sum).<li>The first eigenvector $\boldsymbol{u}_1$ of $S$ points in the most significant direction of the data. That direction accounts for (or explains) a fraction $\sigma_1^2 / T$ of the total variance.<li>The next eigenvector $\boldsymbol{u}_2$ (orthogonal to $\boldsymbol{u}_1$ ) accounts for a smaller fraction $\sigma_2^2 / T$.<li>Stop when those fractions are small. You have the $R$ directions that explain most of the data. The $n$ data points are very near an $R$-dimensional subspace with basis $\boldsymbol{u}_1$ to $\boldsymbol{u}_R$. These $\boldsymbol{u}$ ‘s are the principal components in $m$-dimensional space.<li>$R$ is the “effective rank” of $A$. The true rank $r$ is probably $m$ or $n$ : full rank matrix.</ul><h4 id="perpendicular-least-squares--orthogonal-regression"> <a href="#perpendicular-least-squares--orthogonal-regression" class="anchor-heading" aria-labelledby="perpendicular-least-squares--orthogonal-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Perpendicular Least Squares / orthogonal regression</h4>\[\text { Right triangles } \sum_{j=1}^{n}\left\|\boldsymbol{a}_{j}\right\|^{2}=\sum_{j=1}^{n}\left|\boldsymbol{a}_{j}^{\mathrm{T}} \boldsymbol{u}_{1}\right|^{2}+\sum_{j=1}^{n}\left|\boldsymbol{a}_{j}^{\mathrm{T}} \boldsymbol{u}_{2}\right|^{2}\]<p>RHS is fixed, choosing maximum $u_1$ term will result in minimum $u_2$ term.</p><h4 id="sample-correlation-matrix-nomalized"> <a href="#sample-correlation-matrix-nomalized" class="anchor-heading" aria-labelledby="sample-correlation-matrix-nomalized"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sample Correlation Matrix (nomalized)</h4><p>A diagonal matrix $D$ rescales $A$. Each row of $D A$ has length $\sqrt{n-1}$. The sample correlation matrix $C=D A A^{\mathrm{T}} D /(n-1)$ has $1$ ‘s on its diagonal.</p><h1 id="model-order-reduction"> <a href="#model-order-reduction" class="anchor-heading" aria-labelledby="model-order-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Model Order Reduction</h1><p>A reduced model tries to identify important states of the system.</p><h4 id="dynamic"> <a href="#dynamic" class="anchor-heading" aria-labelledby="dynamic"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Dynamic</h4><p>“Dynamic” means that the solution $u ( t )$ evolves as time goes forward.</p><h4 id="snapshot"> <a href="#snapshot" class="anchor-heading" aria-labelledby="snapshot"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> snapshot</h4><p>A snapshot is a column vector that describes the state of the system</p><p>It can be an approximation to a typical true state $\boldsymbol{u}\left(t^{*}\right)$</p><p>From $n$ snapshots, build a matrix $A$ whose columns span a useful range of states</p><h4 id="proper-orthogonal-decomposition"> <a href="#proper-orthogonal-decomposition" class="anchor-heading" aria-labelledby="proper-orthogonal-decomposition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proper Orthogonal Decomposition</h4><p>The first $R$ left singular vectors $\boldsymbol{u}_1$ to $\boldsymbol{u}_R$ of snapshot $A$ are a basis for POD.</p><p>Variance $\approx$ Energy</p><p>$\sigma_{1}^{2}+\cdots+\sigma_{R}^{2}$ is $99 \%$ or $99.9 \%$ of $\sigma_{1}^{2}+\cdots+\sigma_{n}^{2}$</p><h1 id="the-geometry-of-the-svd"> <a href="#the-geometry-of-the-svd" class="anchor-heading" aria-labelledby="the-geometry-of-the-svd"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Geometry of the SVD</h1>\[(\text { orthogonal }) \times(\text { diagonal }) \times(\text { orthogonal })\] \[(\text { rotation }) \times(\text { stretching }) \times(\text { rotation })\] \[A \boldsymbol{x}=U \Sigma V^{\mathrm{T}} \boldsymbol{x}\]<p><img src="https://live.staticflickr.com/65535/52210447702_746752d194_o.png" alt="Screen Shot 2022-07-12 at 4.05.46 PM" /></p><h4 id="the-norm-of-a-matrix"> <a href="#the-norm-of-a-matrix" class="anchor-heading" aria-labelledby="the-norm-of-a-matrix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Norm of a Matrix</h4><p>largest singular value $\sigma_{1}$</p>\[\text { The norm }\|A\| \text { is the largest ratio } \frac{\|A x\|}{\|x\|} \quad\|A\|=\max _{x \neq 0} \frac{\|A x\|}{\|x\|}=\sigma_{1}\]<h4 id="eckart-young-mirsky-theorem"> <a href="#eckart-young-mirsky-theorem" class="anchor-heading" aria-labelledby="eckart-young-mirsky-theorem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Eckart-Young-Mirsky Theorem</h4>\[\text { The closest rank } \boldsymbol{k} \text { matrix to } \boldsymbol{A} \text { is } \boldsymbol{A}_{\boldsymbol{k}}=\sigma_{1} \boldsymbol{u}_{1} \boldsymbol{v}_{1}^{\mathrm{T}}+\cdots+\sigma_{k} \boldsymbol{u}_{k} \boldsymbol{v}_{k}^{\mathrm{T}}\] \[\|A-B\| \geq\left\|A-A_{k}\right\|=\sigma_{k+1} \text { for all matrices } B \text { of rank } k \text {. }\]<h4 id="polar-decomposition-aqs"> <a href="#polar-decomposition-aqs" class="anchor-heading" aria-labelledby="polar-decomposition-aqs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Polar Decomposition $A=QS$</h4><p>Every real square matrix can be factored into $A=Q S$, where $Q$ is orthogonal and $S$ is symmetric positive semidefinite. If $A$ is invertible, $S$ is positive definite.</p>\[A=U \Sigma V^{\mathrm{T}}=\left(U V^{\mathrm{T}}\right)\left(V \Sigma V^{\mathrm{T}}\right)=(Q)(S)\] \[A=U \Sigma V^{\mathrm{T}}=\left(U \Sigma U^{\mathrm{T}}\right)\left(U V^{\mathrm{T}}\right)=(K)(Q)\]<p>The product of orthogonal matrices is orthogonal.</p><p>In mechanics, the polar decomposition separates the rotation (in $Q$ ) from the stretching (in $S$ ). The eigenvalues of $S$ give the stretching factors. The eigenvectors of $S$ give the stretching directions (the principal axes of the ellipse). The orthogonal matrix $Q$ includes both rotations $U$ and $V^{\mathrm{T}}$.</p><p>Here is a fact about rotations. $Q=U V^{\mathrm{T}}$ is the nearest orthogonal matrix to $A$. This $Q$ makes the norm $|Q-A|$ as small as possible. That corresponds to the fact that $e^{i \theta}$ is the nearest number on the unit circle to $r e^{i \theta}$.</p>\[\text { The nearest singular matrix } A_{0} \text { to } A \text { comes by changing the smallest } \sigma_{\min } \text { to zero. }\]<p>So $\sigma_{\min }$ is measuring the distance from $A$ to singularity.</p><h4 id="pseudoinverse-a"> <a href="#pseudoinverse-a" class="anchor-heading" aria-labelledby="pseudoinverse-a"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pseudoinverse $A^{+}$</h4><p><img src="https://live.staticflickr.com/65535/52212104513_44d3acbfd5_o.png" alt="Screen Shot 2022-07-12 at 9.24.24 PM" /></p>\[A^{+} \boldsymbol{u}_{i}=\frac{1}{\sigma_{i}} \boldsymbol{v}_{i} \quad \text { for } i \leq r \quad \text { and } \quad A^{+} \boldsymbol{u}_{i}=\mathbf{0} \text { for } i&gt;r\]<p><img src="https://live.staticflickr.com/65535/52211148447_4e54e2b572_o.png" alt="Screen Shot 2022-07-12 at 10.00.08 PM" /></p><p><img src="https://live.staticflickr.com/65535/52212431914_ed5e2d33bf_o.png" alt="Screen Shot 2022-07-12 at 10.02.04 PM" /></p>\[\boldsymbol{x}^{+}=\boldsymbol{A}^{+} \boldsymbol{b} \text { is the shortest solution to } A^{\mathrm{T}} A \widehat{\boldsymbol{x}}=A^{\mathrm{T}} \boldsymbol{b} \text { and } A \widehat{\boldsymbol{x}}=\boldsymbol{p}\]<p>The least square approximation decomposite the vectors, psudoinverse faked a matrix that can operate like a real inverse matrix.</p><p>Principally, the inverse will bring the left null space to a certain space, but the psudoinverse brings it to zero.</p><hr><footer><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div>
