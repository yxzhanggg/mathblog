{"0": {
    "doc": "Applications",
    "title": "Applications",
    "content": "Incidence matrix . complete . every pair of nodes is connected by an edge . maximum edges $\\frac{1}{2}n(n-1)$ . Tree . no closed loops . minimum edges $n-1$ . | Elimination reduces every graph to a tree. | . Kirchhoff’s Voltage Law: . he components of $Ax = b$ add to zero around every loop. Kirchhoff’s Current Law: . \\(A^\\mathbf{T} y = 0\\) Flow in equals flow out at each node. the matrix in this balance equation is the transpose of the incidence matrix A. Euler’s formula . \\[\\text{(number of nodes) - (number of edges) + (number of small loops)} = 1\\] \\[( n) - ( m) + ( m - n + 1) = 1\\] Laplacian matrix . stiffness matrix . Markov Matrix . | Every entry of $A$ is positive: $a_{ij} &gt; 0$. | Every column of $A$ adds to 1. | . Perron-Frobenius Theorem . Linear Programming . Linear programming is linear algebra plus two new ideas: inequalities and minimization. he cost is $c_1 x_1 + · · · + c_nx_n$. The winning vector $x^*$ is the nonnegative solution of $Ax = b$ that has smallest cost. Solution is the corners! . Duality . \\[\\text{max=min}\\] The number of independent rows equals the number of independent columns. Simplex method (moves along the edges of the feasible set) . The simplex method goes from one corner to a neighboring corner of lower cost. (move along edge) . A corner is a vector $x\\geq0$ that satisfies the $m$ equations $Ax = b$ with at most $m$ positive components. The other $n - m$ components are zero. The simplex method must decide which component “enters” by becoming positive, and which component “leaves” by becoming zero. The entering variable is the one that gives the most negative reduced cost $r$. Solve the $Ax=b$ by iterately knock off the free column from $A$, then solve the $Bx=b$. (a column enters and an old column leaves) . Interior Point Methods (move inside the feasible set) . . Hilbert spaces . Homogeneous coordinate . Field $F$ . a set of scalars that can be added and multiplied and inverted (except 0 can’t be inverted). ",
    "url": "http://localhost:4000/docs/Linear-Algebra/App/",
    "relUrl": "/docs/Linear-Algebra/App/"
  },"1": {
    "doc": "Communication",
    "title": "Communication",
    "content": ". Source Model . The source is assumed to produce symbols at a rate of $R$ symbols per second. preﬁx-condition code / instantaneous code . An important property of such codewords is that none can be the same as the ﬁrst portion of another, longer, codeword—otherwise the same bit pattern might result from two or more diﬀerent messages, and there would be ambiguity. Kraft Inequality . $L$: Length of code . ",
    "url": "http://localhost:4000/docs/Info-Prob/Com/",
    "relUrl": "/docs/Info-Prob/Com/"
  },"2": {
    "doc": "Complex Numbers",
    "title": "Complex Numbers",
    "content": "\\[z=\\mathcal{Re}\\{z\\}+j\\mathcal{Im}\\{z\\}\\] . Euler’s Formula . \\[\\mathrm{e}^{j\\theta}=\\cos\\theta+j\\sin\\theta\\] . Modulus / Magnitudes . \\[r=|z|=\\sqrt{|\\mathcal{Re}\\{z\\}|^2+|\\mathcal{Im}\\{z\\}|^2}\\] Properties . \\[|r\\mathrm{e}^{j\\theta}|=|r||\\cos\\theta+j\\sin\\theta|=\\sqrt{|r|^2}\\sqrt{|\\cos\\theta|^2+|\\sin\\theta|^2}=r\\] \\[|\\mathrm{e}^{j\\theta}|=|\\cos\\theta+j\\sin\\theta|=\\sqrt{|\\cos\\theta|^2+|\\sin\\theta|^2}=1\\] . Argument / Phase . \\[\\theta=\\arg (r\\mathrm{e}^{j\\theta})\\] Properties . \\[\\arg (ab)=\\arg a+\\arg b\\] \\[z_1z_2=r_1\\mathrm{e}^{j\\theta_1}r_2\\mathrm{e}^{j\\theta_2}=r_1r_2r\\mathrm{e}^{j(\\theta_1+\\theta_2)}\\] \\[z_1+z_2=r_1\\mathrm{e}^{j\\theta_1}+r_2\\mathrm{e}^{j\\theta_2}\\] . Complex conjugate . \\[z^*=\\mathcal{Re}\\{z\\}-j\\mathcal{Im}\\{z\\}\\] Properties . \\[(\\mathrm{e}^{j\\theta})^*=\\mathrm{e}^{-j\\theta}\\] \\[\\mathcal{Re}\\{z\\}=\\frac{z+z^*}{2} \\ \\ \\ \\mathcal{Im}\\{z\\}=\\frac{z-z^*}{2j}\\] \\[\\cos\\theta=\\frac{\\mathrm{e}^{j\\theta}+\\mathrm{e}^{-j\\theta}}{2} \\ \\ \\ \\sin\\theta=\\frac{\\mathrm{e}^{j\\theta}-\\mathrm{e}^{-j\\theta}}{2j}\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Complex-Numbers/",
    "relUrl": "/docs/Fundamental/Complex-Numbers/"
  },"3": {
    "doc": "Complex",
    "title": "Complex",
    "content": ". trigonometry (double angle rule) . \\[(\\cos\\theta+i\\sin\\theta) x (\\cos\\theta+i\\sin\\theta) =\\cos2 \\theta+i^2 \\sin2 \\theta+2i\\sin\\theta\\cos\\theta= \\cos 2\\theta+i\\sin 2\\theta\\] Euler’s Formula . \\[\\cos \\theta = 1 - \\frac{1}{2}\\theta^2+\\ldots\\] \\[\\sin \\theta = \\theta - \\frac{1}{6}\\theta^3\\] \\[e^{i\\theta}=\\cos \\theta+ i \\sin \\theta=1+i\\theta+\\frac{1}{2}i^2\\theta^2+\\frac{1}{6}i^3\\theta^3+\\ldots\\] nth roots of 1: $\\omega=e^{2\\pi i /n}$ . \\[e^{2\\pi i /n}=1\\rightarrow z^n=1\\rightarrow \\omega^n=1\\] . Conjugate transpose $z^\\mathbf{H}$ and $A^\\mathbf{H}$ (“adjoint” of A) . \\[(Au)^\\mathbf{H}v=u^\\mathbf{H}(A^\\mathbf{H}v)\\] \\[(AB)^\\mathbf{H}=B^\\mathbf{H}A^\\mathbf{H}\\] . Length . Inner product . $v^\\mathbf{H}u$ is the complex conjungate of $u^\\mathbf{H}v$ . Hermitian Matrices . \\[S=S^\\mathbf{H}\\] Properties . | energy : f $S = S^\\mathbf{H}$ and $z$ is any real or complex column vector, the number $z^\\mathbf{H}Sz$ is real. | Every eigenvalue of a Hermitian matrix is real. | The eigenvectors of a Hermitian matrix are orthogonal (when they correspond to different eigenvalues). If $Sz = \\lambda z$ and $Sy=\\beta y$ then $y^\\mathbf{H}z = 0$ . | . Unitary Matrices . A unitary matrix $Q$ is a (complex) square matrix that has orthonormal columns. \\[|\\lambda|=1\\] Fast Fourier Transform (a matrix multiplying a vector) . Polynomial: . Discrete Fourier Transform Matrix: . \\[F_n\\overline{F}_n=nI\\] \\[F_n^{-1}=\\overline{F}_n/n\\] . algebra form to calculate even and odd part: . \\[\\omega^2_n=\\omega_m\\] \\[\\omega^{2jk}_n=\\omega^{jk}_m\\] . FFT use the convinience of even/odd function (symmetry) and the symmetry of complex number based on polynomial representation to reduce the complexity of the DFT. Interpolation . If function $f(x)$ has period $2\\pi$, change $x$ to $e^{i\\theta}$, then find the polynomial $p(z) = c_0 + c_1 z + · · · +C_{n -1}Z^{n -1}$ that match the value $f_0,\\ldots,f_{n-1}$. The Fourier matrix is the Vandermonde matrix for interpolation at those $n$ special points. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Complex/",
    "relUrl": "/docs/Linear-Algebra/Complex/"
  },"4": {
    "doc": "Convex Optimization",
    "title": "Convex Optimization",
    "content": "Reference: Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. Mathematical optimization problem . $\\begin{array}{ll} \\operatorname{minimize} &amp; f_{0}(x) \\text {subject to } &amp; f_{i}(x) \\leq b_{i}, \\quad i=1, \\ldots, m \\end{array}$ . Optimization variable $x=(x_1, \\ldots, x_n)$ . Objective function $f_{0}: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ . Constraint functions $f_{i}: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}, \\quad i=1, \\ldots, m$ . Limits/bounds for the constraints $b_1, \\ldots, b_m$ . Optimal vector / solution $x^\\star$ . Linear program . If objective and constraint functions $f_0, \\ldots, f_m$ are linear. \\[f_{i}(\\alpha x+\\beta y)=\\alpha f_{i}(x)+\\beta f_{i}(y) \\ \\text{for all} \\ x, y\\in\\mathbb{R}^n\\ \\text{and all} \\ \\alpha, \\beta\\in\\mathbb{R}\\] \\[\\begin{array}{ll} \\operatorname{minimize} &amp; c^{T} x \\\\ \\text { subject to } &amp; a_{i}^{T} x \\leq b_{i}, \\quad i=1, \\ldots, m \\end{array}\\] Here the vector $c, a_{1}, \\ldots, a_{m} \\in \\mathbf{R}^{n}$ and the scalar $b_{1}, \\ldots, b_{m} \\in \\mathbf{R}$ are problem parameters that specify the objective and constraint functions.a . Nonlinear program . If not above but not convex. Convex optimization problems . \\[f_{i}(\\alpha x+\\beta y) \\leq \\alpha f_{i}(x)+\\beta f_{i}(y)$ for all $x, y\\in\\mathbb{R}^n$ And all $\\alpha, \\beta\\in\\mathbb{R}$ with $\\alpha+\\beta=1, \\alpha \\geq 0, \\beta \\geq 0\\] We can consider convex optimization to be a generalization of linear programming. Application . The optimization problem is an abstraction of the problem of making the best possible choice of a vector in $R^n$ from a set of candidate choices. Least-squares problems . A least-squares problem is an optimization problem with no constraints and an objective which is a sum of squares of terms of the form $a_{i}^{T} x-b_{i}$: . \\[\\operatorname{minimize} \\quad f_{0}(x)=\\|A x-b\\|_{2}^{2}=\\sum_{i=1}^{k}\\left(a_{i}^{T} x-b_{i}\\right)^{2} .\\] Here $A \\in \\mathbf{R}^{k \\times n} \\text { (with } k \\geq n \\text { ) }$, $a_i^T$ are the rows of $A$, vector $x\\in R$ is the optimization variable. \\[\\left(A^{T} A\\right) x=A^{T} b\\] \\[x=\\left(A^{T} A\\right)^{-1} A^{T} b\\] Rocognizing least-squares problem the objective is a quadratic function; the associated quadratic form is positive semideﬁnite. Weighted least-squares $\\sum_{i=1}^{k} w_{i}\\left(a_{i}^{T} x-b_{i}\\right)^{2}$, where $w_{1}, \\ldots, w_{k}$ are positive, is minimized. Regularization $\\sum_{i=1}^{k}\\left(a_{i}^{T} x-b_{i}\\right)^{2}+\\rho \\sum_{i=1}^{n} x_{i}^{2}$ Where $\\rho&gt;0$. Regularization comes up in statistical estimation when the vector x to be estimated is given a prior distribution. ",
    "url": "http://localhost:4000/docs/Convex-Optimization",
    "relUrl": "/docs/Convex-Optimization"
  },"5": {
    "doc": "Convex set",
    "title": "Convex set",
    "content": "affine set . A set $C \\subseteq \\mathbf{R}^{n}$ is affine if the line through any two distinct points in $C$ lies in $ C$. Every affine set can be expressed as the solution set of a system of linear equations. Aﬃne combination of $x_1, \\ldots x_k$ . $\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k}$ where $\\theta_{1}+\\cdots+\\theta_{k}=1$ . an aﬃne set contains every aﬃne combination of its points. Affine hull . The set of all aﬃne combinations of points in some set $C \\subseteq \\mathbf{R}^{n}$ . \\[\\text { aff } C=\\left\\{\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k} \\mid x_{1}, \\ldots, x_{k} \\in C, \\theta_{1}+\\cdots+\\theta_{k}=1\\right\\}\\] Affine dimension . We deﬁne the affine dimension of a set C as the dimension of its aﬃne hull. Relative interior of set $C$ . $\\text { relint } C={x \\in C \\mid B(x, r) \\cap \\text { aff } C \\subseteq C \\text { for some } r&gt;0}$ Where $B(x, r)={y \\mid|y-x| \\leq r}$ . Convex sets . A set $C$ is convex if the line segment between any two points in $C$ lies in $C$. A convex set every locally optimal solution is global. Convex combination of $x_1, \\ldots x_k$ . $\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k}$ where $\\theta_{1}+\\cdots+\\theta_{k}=1$, and $\\theta_{i} \\geq 0, i=1, \\ldots, k$. A convex combination of points can be thought of as a mixture or weighted average of the points. Convex hull . is the smallest convex set that contains $C$ . \\[\\operatorname{conv} C=\\left\\{\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k} \\mid x_{i} \\in C, \\theta_{i} \\geq 0, i=1, \\ldots, k, \\theta_{1}+\\cdots+\\theta_{k}=1\\right\\}\\] . cone . A set $C$ is called a cone, or nonnegative homogeneous, if for every $x \\in C$ and $\\theta \\geq 0$ we have $\\theta x \\in C$. convex cone . A set $C$ is a convex cone if it is convex and a cone, which means that for any $x_{1}, x_{2} \\in C$ and $\\theta_{1}, \\theta_{2} \\geq 0$, we have $\\theta_{1} x_{1}+\\theta_{2} x_{2} \\in C$. conic combination . A point of the form $\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k}$ with $\\theta_{1}, \\ldots, \\theta_{k} \\geq 0$ is called a conic combination (or a nonnegative linear combination) of $x_{1}, \\ldots, x_{k}$. conic hull . The conic hull of a set $C$ is the set of all conic combinations of points in $C, i . e .$ . \\[\\left\\{\\theta_{1} x_{1}+\\cdots+\\theta_{k} x_{k} \\mid x_{i} \\in C, \\theta_{i} \\geq 0, i=1, \\ldots, k\\right\\}\\] . Affine &gt; cone &gt; convex . hyperplane . Analytically it is the solution set of a nontrivial linear equation among the components of x (and hence an affine set). half spaces . Euclidian Norm . Euclidean balls . # . ellipsoids . Norm ball . Nrom corn . Second order corn . Polyhedra . A polyhedron is defined as the solution set of a finite number of linear equalities and inequalities. the intersection of a finite number of halfspaces and hyperplanes. polytope . A bounded polyhedron . Simplex . affinely independent $v_0,\\ldots ,v_k$: . Unit simplex - n dimension . Probability simplex - n-1 dimension . ",
    "url": "http://localhost:4000/docs/Convex-Optimization/Convex-set/",
    "relUrl": "/docs/Convex-Optimization/Convex-set/"
  },"6": {
    "doc": "Eigens",
    "title": "Diagonalization (factorization)",
    "content": "Eigenvector matrix $X$ . Eigenvalue matrix $\\Lambda$ . \\[X^{-1} A X=\\Lambda=\\left[\\begin{array}{lll} \\lambda_{1} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_{n} \\end{array}\\right]\\] \\[A X=X \\Lambda \\quad \\text { is } \\quad \\boldsymbol{X}^{-1} \\boldsymbol{A} \\boldsymbol{X}=\\boldsymbol{\\Lambda} \\quad \\text { or } \\quad \\boldsymbol{A}=\\boldsymbol{X} \\boldsymbol{\\Lambda} \\boldsymbol{X}^{-1}\\] Each eigenvalue has at least one eigenvector. | Invertibility is concerned with the eigenvalues $(\\lambda=0$ or $\\lambda \\neq 0) .$ | Diagonalizability is concerned with the eigenvectors (too few or enough for $X$ ). | . Independent $\\boldsymbol{x}$ from different $\\lambda$ . Eigenvectors $x_{1}, \\ldots, \\boldsymbol{x}_{j}$ that correspond to distinct (all different) eigenvalues are linearly independent. An $n$ by $n$ matrix that has $n$ different eigenvalues (no repeated $\\lambda$ ‘s) must be diagonalizable. Similar Matrices . | Same Eigenvalues | . \\[A=X \\Lambda X^{-1}\\] . | all invertible matrices $B$ | . \\[A=B C B^{-1}\\] Follow the eigenvectors . Decomposite the vecotors into combination of eigenvecotors, and every transformation make the eigenvectors grow by eigenvalues. \\[\\boldsymbol{u}_{0}=c_{1} \\boldsymbol{x}_{1}+\\cdots+c_{n} \\boldsymbol{x}_{n}\\] \\[\\boldsymbol{u}_{k}=c_{1}\\left(\\lambda_{1}\\right)^{k} \\boldsymbol{x}_{1}+\\cdots+c_{n}\\left(\\lambda_{n}\\right)^{k} \\boldsymbol{x}_{n}\\] \\[A^{k} u_{0}=X \\Lambda^{k} X^{-1} u_{0}=X \\Lambda^{k} c=\\left[\\begin{array}{lll} x_{1} &amp; \\ldots &amp; x_{n} \\end{array}\\right]\\left[\\begin{array}{ccc} \\left(\\lambda_{1}\\right)^{k} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\left(\\lambda_{n}\\right)^{k} \\end{array}\\right]\\left[\\begin{array}{c} c_{1} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right]\\] Geometric Multiplicity GM . Number of independent eigenvectors for specific $\\lambda$. Then GM is the dimension of the nullspace of $A-\\lambda I$. Algebraic Multiplicity = AM . number of repetitions of specific $\\lambda$, . $n$ roots of $\\operatorname{det}(A-\\lambda I)=0$. Diagonalizability . $A$ is diagonalizable if every eigenvalue has enough eigenvectors (GM= AM) . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#diagonalization-factorization",
    "relUrl": "/docs/Linear-Algebra/Eigen/#diagonalization-factorization"
  },"7": {
    "doc": "Eigens",
    "title": "Systems of Differential Equations",
    "content": "Choose $\\boldsymbol{u}=e^{\\lambda t} \\boldsymbol{x}$ when $\\boldsymbol{A x}=\\boldsymbol{\\lambda} \\boldsymbol{x}$ . \\[\\frac{d u}{d t}=A \\boldsymbol{u}=A e^{\\lambda t} \\boldsymbol{x}=\\lambda e^{\\lambda t} \\boldsymbol{x}\\] . | Write $\\boldsymbol{u}(0)$ as a combination $c_{1} \\boldsymbol{x}{1}+\\cdots+c{n} \\boldsymbol{x}_{n}$ of the eigenvectors of $\\boldsymbol{A}$. | Multiply each eigenvector $x_{i}$ by its growth factor $e^{\\lambda_{i} t}$. | The solution is the same combination of those pure solutions $e^{\\lambda t} \\boldsymbol{x}$ : | . \\[\\frac{d u}{d t}=A u \\quad u(t)=c_{1} e^{\\lambda_{1} t} x_{1}+\\cdots+c_{n} e^{\\lambda_{n} t} x_{n}\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#systems-of-differential-equations",
    "relUrl": "/docs/Linear-Algebra/Eigen/#systems-of-differential-equations"
  },"8": {
    "doc": "Eigens",
    "title": "Second order",
    "content": "\\[m \\frac{d^{2} y}{d t^{2}}+b \\frac{d y}{d t}+k y=0 \\quad \\text { becomes } \\quad\\left(m \\lambda^{2}+b \\lambda+k\\right) e^{\\lambda t}=0\\] \\[m=1\\] \\[\\begin{aligned} &amp;\\boldsymbol{d} \\boldsymbol{y} / \\boldsymbol{d} t=\\boldsymbol{y}^{\\prime} \\\\ &amp;\\boldsymbol{d} \\boldsymbol{y}^{\\prime} / \\boldsymbol{d} t=-\\boldsymbol{k} y-\\boldsymbol{b} \\boldsymbol{y}^{\\prime} \\end{aligned} \\quad \\text { converts to } \\quad \\frac{d}{d t}\\left[\\begin{array}{c} y \\\\ y^{\\prime} \\end{array}\\right]=\\left[\\begin{array}{rr} \\mathbf{0} &amp; \\mathbf{1} \\\\ -\\boldsymbol{k} &amp; -\\boldsymbol{b} \\end{array}\\right]\\left[\\begin{array}{c} y \\\\ y^{\\prime} \\end{array}\\right]=A \\boldsymbol{u} .\\] \\[A-\\lambda I=\\left[\\begin{array}{cc} -\\lambda &amp; 1 \\\\ -k &amp; -b-\\lambda \\end{array}\\right] \\quad \\text { has determinant } \\quad \\lambda^{2}+b \\lambda+k=0\\] \\[\\boldsymbol{x}_{1}=\\left[\\begin{array}{c} 1 \\\\ \\lambda_{1} \\end{array}\\right] \\quad \\boldsymbol{x}_{2}=\\left[\\begin{array}{c} 1 \\\\ \\lambda_{2} \\end{array}\\right] \\quad \\boldsymbol{u}(t)=c_{1} e^{\\lambda_{1} t}\\left[\\begin{array}{c} 1 \\\\ \\lambda_{1} \\end{array}\\right]+c_{2} e^{\\lambda_{2} t}\\left[\\begin{array}{c} 1 \\\\ \\lambda_{2} \\end{array}\\right]\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#second-order",
    "relUrl": "/docs/Linear-Algebra/Eigen/#second-order"
  },"9": {
    "doc": "Eigens",
    "title": "Defference equations",
    "content": ". real below, imaginary above the parabola . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#defference-equations",
    "relUrl": "/docs/Linear-Algebra/Eigen/#defference-equations"
  },"10": {
    "doc": "Eigens",
    "title": "Matrix exponential",
    "content": "\\[e^{x}=1+x+\\frac{1}{2} x^{2}+\\frac{1}{6} x^{3}+\\cdots\\] \\[e^{A t}=I+A t+\\frac{1}{2}(A t)^{2}+\\frac{1}{6}(A t)^{3}+\\cdots\\] take derivative w.r.t. $t$: . \\[A+A^{2} t+\\frac{1}{2} A^{3} t^{2}+ \\cdots=A e^{A t}\\] Its eigenvalues are $e^{\\boldsymbol{\\lambda} t}$ . \\[e^{A t}x=e^{\\lambda t}x\\] \\[\\left(I+A t+\\frac{1}{2}(A t)^{2}+\\cdots\\right) \\boldsymbol{x}=\\left(1+\\lambda t+\\frac{1}{2}(\\lambda t)^{2}+\\cdots\\right) \\boldsymbol{x}\\] . | $e^{A t}$ always has the inverse $e^{-A t}$. | The eigenvalues of $e^{A t}$ are always $e^{\\lambda t}$. | When $A$ is antisymmetric, $e^{A t}$ is orthogonal. Inverse $=$ transpose $=e^{-A t}$. | . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#matrix-exponential",
    "relUrl": "/docs/Linear-Algebra/Eigen/#matrix-exponential"
  },"11": {
    "doc": "Eigens",
    "title": "find $\\boldsymbol{u}(t)=e^{A t} \\boldsymbol{u}(0)$",
    "content": "\\[e^{A t}=X e^{\\Lambda t} X^{-1}\\] . | $\\boldsymbol{u}(0)=c_1 \\boldsymbol{x}_1+\\cdots+c_n \\boldsymbol{x}_n=X \\boldsymbol{c}$. Here we need $n$ independent eigenvectors. | Multiply each $\\boldsymbol{x}i$ by its growth factor $e^{\\lambda{i} t}$ to follow it forward in time. | The best form of $e^{A t} \\boldsymbol{u}(0)$ is . \\[\\boldsymbol{u}(t)=\\boldsymbol{c}_\\boldsymbol{1} e^{\\lambda_\\boldsymbol{1} t} \\boldsymbol{x}_\\boldsymbol{1}+\\cdots+\\boldsymbol{c}_\\boldsymbol{n} e^{\\lambda_n t} \\boldsymbol{x}_\\boldsymbol{n}\\] | . It is a short form solution compatible with the original one ($u(t)=c_{1} e^{\\lambda_{1} t} x_{1}+\\cdots+c_{n} e^{\\lambda_{n} t} x_{n}$), . \\[e^{A t} u(0)=X e^{\\Lambda t} X^{-1} u(0)=\\left[\\begin{array}{lll} \\boldsymbol{x}_{1} &amp; \\cdots &amp; \\boldsymbol{x}_{n} \\end{array}\\right]\\left[\\begin{array}{ccc} e^{\\lambda_{1} t} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; e^{\\lambda_{n} t} \\end{array}\\right]\\left[\\begin{array}{c} c_{1} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right]\\] we can use it to find the solution when the eigenvectors are not enough to produce the solution in original form: . (where the $t e^{t}$ comes from) . if diagonalization is not possible . there are repeated eigenvalues and less eigenvectors. we cannot express solution as: . \\[u(t)=c_{1} e^{\\lambda_{1} t} x_{1}+\\cdots+c_{n} e^{\\lambda_{n} t} x_{n}\\] use series to express the solution: . \\[e^{A t}=e^{I t} e^{(A-I) t}=e^{t}[I+(A-I) t]\\] \\[\\left[\\begin{array}{l} y \\\\ y^{\\prime} \\end{array}\\right]=e^{t}\\left[I+\\left[\\begin{array}{ll} -1 &amp; 1 \\\\ -1 &amp; 1 \\end{array}\\right] t\\right]\\left[\\begin{array}{r} y(0) \\\\ y^{\\prime}(0) \\end{array}\\right] \\quad y(t)=e^{t} y(0)-t e^{t} y(0)+t e^{t} y^{\\prime}(0)\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#find-boldsymbolutea-t-boldsymbolu0",
    "relUrl": "/docs/Linear-Algebra/Eigen/#find-boldsymbolutea-t-boldsymbolu0"
  },"12": {
    "doc": "Eigens",
    "title": "Symmetric Matrices",
    "content": ". | A symmetric matrix has only real eigenvalues. | The eigenvectors can be chosen orthonormal. | . $S=Q \\Lambda Q^{\\mathrm{T}}$ . Spectral Theorem / Principal axis theorem . \\[\\text{symmetric matrix=(orthonormal eigenvectors)(eigenvalue)(orthonormal eigenvectors)}^\\mathrm{T}\\] Every symmetric matrix has the factoriztion $S=Q \\Lambda Q^{\\mathrm{T}}$ with real eigenvalues in $\\Lambda$ and orthonormal eigenvectors in the columns of $Q$: . Symmetric diagonalization $S=Q \\Lambda Q^{-1}=Q \\Lambda Q^{\\mathrm{T}} \\quad$ with $\\quad Q^{-1}=Q^{\\mathrm{T}}$ . Orthogonal Eigenvectors . Eigenvectors of a real symmetric matrix (when they correspond to different $\\lambda$ ‘s) are always perpendicular. Every 2 by 2 symmetric matrix is (rotation)(stretch)(rotate back) . \\[S=Q \\Lambda Q^{\\mathrm{T}}=\\left[\\begin{array}{ll} \\boldsymbol{q}_{1} &amp; \\boldsymbol{q}_{2} \\end{array}\\right]\\left[\\begin{array}{ll} \\lambda_{1} &amp; \\\\ &amp; \\lambda_{2} \\end{array}\\right]\\left[\\begin{array}{l} \\boldsymbol{q}_{1}^{\\mathrm{T}} \\\\ \\boldsymbol{q}_{2}^{\\mathrm{T}} \\end{array}\\right]\\] \\[S=Q \\Lambda Q^{\\mathrm{T}}=\\lambda_{1} q_{1} \\boldsymbol{q}_{1}^{\\mathrm{T}}+\\cdots+\\lambda_{n} \\boldsymbol{q}_{n} \\boldsymbol{q}_{n}^{\\mathrm{T}}\\] Complex Eigenvalues of Real Matrices . Eigenvalues versus Pivots . \\[\\text { product of pivots }=\\text { determinant }=\\text { product of eigenvalues. }\\] For symmetric matrices, (same signs) the number of positive eigenvalues of $S=S^{\\mathrm{T}}$ equals the number of positive pivots. All Symmetric Matrices are Diagonalizable. Schur’s Theorem (triangularization) . Every square $A$ factors into $Q T Q^{-1}$ where $T$ is upper triangular and $\\bar{Q}^{\\mathrm{T}}=Q^{-1}$. If $A$ has real eigenvalues then $Q$ and $T$ can be chosen real: $Q^{\\mathrm{T}} Q=I$. If $A=S$ then $T=\\Lambda$. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#symmetric-matrices",
    "relUrl": "/docs/Linear-Algebra/Eigen/#symmetric-matrices"
  },"13": {
    "doc": "Eigens",
    "title": "Positive Definite Matrices",
    "content": "the symmetric matrices that have positive eigenvalues . The eigenvalues of $S$ are positive if and only if the pivots are positive . Energy-based Definition . \\[\\text{energy}=\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}=\\lambda \\boldsymbol{x}^{\\mathrm{T}} \\boldsymbol{x}=\\lambda \\|\\boldsymbol{x}\\|^{2}&gt;0\\] for all nonzero vectors $x$ . If $S$ and $T$ are symmetric positive definite, so is $S + T$. If the columns of $A$ are independent, then $S=A^{\\mathrm{T}} A$ is positive definite. Properties . When a symmetric matrix $S$ has one of these five properties, it has them all: . | All $n$ pivots of $S$ are positive. | All n upper left determinants are positive. | All n eigenvalues of $S$ are positive. | $\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}$ is positive except at $\\boldsymbol{x}=\\mathbf{0}$. This is the energy-based definition. | $S$ equals $A^{\\mathrm{T}} A$ for a matrix $A$ with independent columns. | . split up $\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}$ . Positive Semidefinite Matrices . dependent columns . The Ellipse $\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}=a x^{2}+2 b x y+c y^{2}=1$ . | The tilted ellipse is associated with $S$. Its equation is $\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}=1$. | The lined-up ellipse is associated with $\\Lambda$. Its equation is $\\boldsymbol{X}^{\\mathrm{T}} \\Lambda \\boldsymbol{X}=1$. | The rotation matrix that lines up the ellipse is the eigenvector matrix $Q$. | . $\\boldsymbol{S}=Q \\Lambda Q^{\\mathrm{T}}$ is positive definite when all $\\lambda_{i}&gt;0$. The graph of $\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}=1$ is an ellipse: . Ellipse . \\[\\left[\\begin{array}{ll}x &amp; y\\end{array}\\right] Q \\Lambda Q^{\\mathrm{T}}\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]=\\left[\\begin{array}{ll}X &amp; Y\\end{array}\\right] \\Lambda\\left[\\begin{array}{l}X \\\\ Y\\end{array}\\right]=\\boldsymbol{\\lambda}_{1} \\boldsymbol{X}^{2}+\\boldsymbol{\\lambda}_{2} Y^{2}=\\mathbf{1}\\] The axes point along eigenvectors of $S$. The half-lengths are $1 / \\sqrt{\\lambda_{1}}$ and $1 / \\sqrt{\\lambda_{2}}$. Hyperbola . one eigen value is negative . Ellipsoid . $S$ is $n$ by $n$. Minimum . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/#positive-definite-matrices",
    "relUrl": "/docs/Linear-Algebra/Eigen/#positive-definite-matrices"
  },"14": {
    "doc": "Eigens",
    "title": "Eigens",
    "content": "Eigenvectors . Certain exceptional vectors $x$ are in the same direction as $Ax$. \\[A \\boldsymbol{x}=\\lambda \\boldsymbol{x}\\] . Eigenvalues . | Markov matrix: Each column of $P$ adds to 1 , so $\\lambda=1$ is an eigenvalue. | The matrix is singular, so $\\lambda=0$ is an eigenvalue. | . Projection . \\[\\lambda=1 \\text { and } 0\\] Permutation . \\[|\\lambda|=1\\] Reflection . \\[\\lambda=1 \\text { and } -1\\] \\[\\text { reflection }=2(\\text { projection })-I\\] \\[\\text { When a matrix is shifted by } I \\text {, each } \\lambda \\text { is shifted by } 1 \\text {. No change in eigenvectors. }\\] . Calculation to get eigenvalues and eigenvectors . | Compute the determinant of $A-\\lambda I$. With $\\lambda$ subtracted along the diagonal, this determinant starts with $\\lambda^{n}$ or $-\\lambda^{n}$. It is a polynomial in $\\lambda$ of degree $n$. | Find the roots of this polynomial, by solving $\\operatorname{det}(A-\\lambda I)=0$. The $n$ roots are the $n$ eigenvalues of $A$. They make $A-\\lambda I$ singular. | For each eigenvalue $\\lambda$, solve $(A-\\lambda I) \\boldsymbol{x}=\\mathbf{0}$ to find an eigenvector $\\boldsymbol{x}$. | . \\[\\lambda_{1}\\lambda_{2}\\cdots\\lambda_{n}=\\operatorname{determinant}\\] trace . \\[\\lambda_{1}+\\lambda_{2}+\\cdots+\\lambda_{n}=\\operatorname{trace}=a_{11}+a_{22}+\\cdots+a_{n n}\\] symmetric matrix . real number . \\[S^{\\mathrm{T}}=S\\] skew-symmetric matrix . imaginary number . \\[A^{\\mathrm{T}}=-A\\] orthogonal matrix . \\[\\text{complex number with}|\\lambda|=1\\] \\[Q^{\\mathrm{T}} Q=I\\] \\[A \\text { and } B \\text { share the same } \\boldsymbol{n} \\text { independent eigenvectors if and only if } \\boldsymbol{A B}=\\boldsymbol{B A} \\text {. }\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Eigen/",
    "relUrl": "/docs/Linear-Algebra/Eigen/"
  },"15": {
    "doc": "Fundamentals",
    "title": "Fundamentals",
    "content": "  . ",
    "url": "http://localhost:4000/docs/Fundamentals",
    "relUrl": "/docs/Fundamentals"
  },"16": {
    "doc": "What is matrix (Ax = b)",
    "title": "Basics",
    "content": "vector . A vector in $n$ dimension space has $n$ components $v_1, v_2,\\ldots,v_n$. \\[\\boldsymbol{v}+\\boldsymbol{w}=\\left(v_{1}+w_{1}, v_{2}+w_{2}\\right)\\] \\[c \\boldsymbol{v}=\\left(c v_{1}, c v_{2}\\right)\\] Length of a vector . \\[\\text { length }=\\|\\boldsymbol{v}\\|=\\sqrt{\\boldsymbol{v} \\cdot \\boldsymbol{v}}=\\left(v_{1}^{2}+v_{2}^{2}+\\cdots+v_{n}^{2}\\right)^{1 / 2}\\] Linear combination . \\[c \\boldsymbol{u}+d \\boldsymbol{v}+e \\boldsymbol{w}\\] Cosine formula . \\[\\cos \\theta=\\frac{\\boldsymbol{v} \\cdot \\boldsymbol{w}}{\\|\\boldsymbol{v}\\|\\|\\boldsymbol{w}\\|}\\] Schwarz inequality . \\[\\|\\boldsymbol{v} \\cdot \\boldsymbol{w}\\| \\leq{\\|\\boldsymbol{v}\\|\\|\\boldsymbol{w}\\|}\\] Triangle Inequality . \\[\\|\\boldsymbol{v}+\\boldsymbol{w}\\| \\leq\\|\\boldsymbol{v}\\|+\\|\\boldsymbol{w}\\|\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#basics",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#basics"
  },"17": {
    "doc": "What is matrix (Ax = b)",
    "title": "Different viewpoints",
    "content": "Row picture . \\[\\begin{aligned} x-2 y &amp;=1 \\\\ 3 x+2 y &amp;=11 \\end{aligned}\\] The row picture shows two lines meeting at a single point (the solution). Column picture . \\[\\left[\\begin{array}{rr} 1 &amp; -2 \\\\ 3 &amp; 2 \\end{array}\\right]\\left[\\begin{array}{l} 3 \\\\ 1 \\end{array}\\right]=\\left[\\begin{array}{r} 1 \\\\ 11 \\end{array}\\right]\\] The column picture combines the column vectors on the left side to produce the vector b on the right side. Hyperplane . the plane $n&gt;3$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#different-viewpoints",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#different-viewpoints"
  },"18": {
    "doc": "What is matrix (Ax = b)",
    "title": "Matrix multiplication",
    "content": "Direct method . The entry in row $i$ and column $j$ of $A B$ is $($ row $i$ of $A) \\cdot($ column $j$ of $B)$. Column picture . Matrix $A$ times every column of $B$ . \\[A\\left[\\boldsymbol{b}_{1} \\cdots \\boldsymbol{b}_{p}\\right]=\\left[A \\boldsymbol{b}_{1} \\cdots A \\boldsymbol{b}_{p}\\right]\\] Row picture . Every row of $A$ times matrix $B$ . \\[[\\text { row } i \\text { of } A]B=[\\text { row } i \\text { of } A B]\\] $AB= (m \\times n)(n \\times p) = (m \\times p)$, $mp$ dot products with $n$ steps each. Columns by rows . Multiply columns $1$ to $n$ of $A$ times rows $1$ to $n$ of $B$. Add those matrices. \\[\\left[\\begin{array}{ccc} \\operatorname{col} 1 &amp; \\text { col } 2 &amp; \\text { col 3} \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\end{array}\\right]\\left[\\begin{array}{ccc} \\text { row 1 } &amp; \\cdots \\cdot \\\\ \\text { row 2 } &amp; \\cdots \\cdot \\\\ \\text { row } 3 &amp; \\cdots \\cdot \\end{array}\\right]=(\\operatorname{col} 1)(\\text { row 1) }+(\\operatorname{col} 2)(\\text { row 2) }+(\\operatorname{col} 3)(\\text { row 3) }\\] Schur complement . \\[S=D-C A^{-1} B\\] \\[\\left[\\begin{array}{c|c} I &amp; \\mathbf{0} \\\\ \\hline-C A^{-1} &amp; I \\end{array}\\right]\\left[\\begin{array}{c|c} A &amp; B \\\\ \\hline C &amp; D \\end{array}\\right]=\\left[\\begin{array}{c|c} A &amp; B \\\\ \\hline \\mathbf{0} &amp; \\boldsymbol{D}-\\boldsymbol{C A}^{-\\mathbf{1}} \\boldsymbol{B} \\end{array}\\right]\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#matrix-multiplication",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#matrix-multiplication"
  },"19": {
    "doc": "What is matrix (Ax = b)",
    "title": "Inversion",
    "content": "Inverse matrix . The matrix $A$ is invertible if there exists a matrix $A^{-1}$ that “inverts” $A$: . \\[A^{-1} A=I \\quad \\text { and } \\quad A A^{-1}=I\\] . | Two-sided inversion: square matrix | One-sided inversion: rectangular matrix | . Inversibility . The inverse exists if and only if elimination produces $n$ pivots. Properties . | $(A B C)^{-1}=C^{-1} B^{-1} A^{-1}$ . | $K$ is symmetric across its main diagonal. Then $K^{-1}$ is also symmetric. | $K$ is tridiagonal (only three nonzero diagonals = band). But $K^{-1}$ is a dense matrix with no zeros. | An invertible matrix cannot have a zero determinant. | If $A$ is invertible and upper triangular, so is $A^{-1}$. Start with $A A^{-1}=I$. | Diagonally dominant matrices are invertible: . \\[\\left|\\boldsymbol{a}_{i i}\\right|&gt;\\sum_{j \\neq i}\\left|\\boldsymbol{a}_{i j}\\right|\\] | . For elimination matrix $E$ . Gauss-Jordan method . The Gauss-Jordan method computes $A^{-1}$ by solving all $n$ equations together. \\[A A^{-1}=A\\left[\\begin{array}{lll} x_{1} &amp; x_{2} &amp; x_{3} \\end{array}\\right]=\\left[\\begin{array}{lll} e_{1} &amp; e_{2} &amp; e_{3} \\end{array}\\right]=I\\] \\[A^{-1}\\left[\\begin{array}{ll} A &amp; I \\end{array}\\right]=E\\left[\\begin{array}{ll} A &amp; I \\end{array}\\right]=\\left[\\begin{array}{ll} I &amp; A^{-1} \\end{array}\\right]\\] The matrix $E$ keeps a record. From Cofactor matrix . \\[\\left(A^{-1}\\right)_{i j}=\\frac{C_{j i}}{\\operatorname{det} A} \\quad \\text { and } \\quad A^{-1}=\\frac{C^{\\mathrm{T}}}{\\operatorname{det} A}\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#inversion",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#inversion"
  },"20": {
    "doc": "What is matrix (Ax = b)",
    "title": "Transpose and Permutation",
    "content": "Transpose . \\[\\text{row}\\leftrightarrow\\text{column} \\quad \\left(A^{\\mathbf{T}}\\right)_{i j}=A_{j i}\\] Mathematical definition: $A^{\\mathrm{T}}$ is the matrix that makes these two inner products equal for every $x$ and $y$: . \\[(A x)^{\\mathrm{T}} y=x^{\\mathrm{T}}\\left(A^{\\mathrm{T}} y\\right) \\quad \\text { Inner product of } A x \\text { with } y=\\text { Inner product of } x \\text { with } A^{\\mathrm{T}} y\\] Properties . | SUM: The transpose of $A+B$ is $A^{\\mathrm{T}}+B^{\\mathrm{T}}$. | PRODUCT: $(A B C)^{T}=C^{T}B^{T}A^{T}$. $A \\boldsymbol{x}$ combines the columns of $A$ while $\\boldsymbol{x}^{\\mathrm{T}} A^{\\mathrm{T}}$ combines the rows of $A^{\\mathrm{T}}$ . | INVERSE: The transpose of $A^{-1}$ is $\\left(A^{-1}\\right)^{\\mathrm{T}}=\\left(A^{\\mathrm{T}}\\right)^{-1}$. | $A^{\\mathrm{T}}$ is invertible exactly when $A$ is invertible. | $A^\\mathrm{T} A$ is invertible if and only if $A$ has linearly independent columns. | $A^{\\mathrm{T}} A$ is symmetric. | Transformation perspective: transpose matrix is an n-dimensional scissors. | . Inner products / Dot product ($^\\mathbf{T}$ is inside) . \\[x^{\\mathrm{T}} y \\quad(1 \\times n)(n \\times 1)\\] Outer products / Rank one product ($^\\mathbf{T}$ is inside) . \\[x y^{\\mathrm{T}} \\quad(n \\times 1)(1 \\times n)\\] Symmetric Matrix . Definition: A symmetric matrix has $S^{\\mathrm{T}}=S .$ This means that $s_{j i}=s_{i j}$. Properties . The inverse of a symmetric matrix is also symmetric. $\\left(S^{-1}\\right)^{\\mathrm{T}}=\\left(S^{\\mathrm{T}}\\right)^{-1}=S^{-1}$ . $S=A^{\\mathrm{T}} A$ . The transpose of $A^{\\mathrm{T}} A$ is $A^{\\mathrm{T}}\\left(A^{\\mathrm{T}}\\right)^{\\mathrm{T}}$ which is $A^{\\mathrm{T}} A$ again. Permutation Matrix . Definition: A permutation matrix $P$ has the rows of the identity $I$ in any order. \\[P P^{\\mathrm{T}}=I \\leftrightarrow P^{\\mathrm{T}}=P^{-1}\\] $PA=LU \\quad / \\quad A=LPU$ . When there is a row exchange, the sign of determinant is reversed. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#transpose-and-permutation",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#transpose-and-permutation"
  },"21": {
    "doc": "What is matrix (Ax = b)",
    "title": "Determinant",
    "content": "\\[\\begin{aligned} &amp;\\text {The determinant of an } n \\text { by } n \\text { matrix can be found in three ways: }\\\\ &amp;\\begin{array}{ll} 1 \\text { Multiply the } n \\text { pivots (times } 1 \\text { or }-1) &amp; \\text { This is the pivot formula. } \\\\ 2 \\text { Add up } n \\text { ! terms (times } 1 \\text { or }-1) &amp; \\text { This is the \"big' formula. } \\\\ \\mathbf{3} \\text { Combine } n \\text { smaller determinants (times } 1 \\text { or }-1 \\text { ) } &amp; \\text { This is the cofactor formula. } \\end{array} \\end{aligned}\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#determinant",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#determinant"
  },"22": {
    "doc": "What is matrix (Ax = b)",
    "title": "Properties",
    "content": "\\[\\operatorname{det} A \\text { and }|A|\\] . | The determinant of the $n$ by $n$ identity matrix is $1 .$ . | The determinant changes sign when two rows are exchanged (sign reversal): . | The determinant is a linear function of each row separately (all other rows stay fixed). | 4 If two rows of $A$ are equal, then $\\operatorname{det} A=0$. | Subtracting a multiple of one row from another row leaves $\\operatorname{det} A$ unchanged. | A matrix with a row of zeros has $\\operatorname{det} A=0$. | If $A$ is triangular then $\\operatorname{det} A=a_{11} a_{22} \\cdots a_{n n}= \\text{product of diagonal entries}$. | If $A$ is singular then $\\operatorname{det} A=0$. If $A$ is invertible then $\\operatorname{det} A \\neq 0$. | The determinant of $A B$ is $\\operatorname{det} A$ times $\\operatorname{det} B:|A B|=|A||B|$. | \\[A^{\\mathrm{T}}=\\operatorname{det} A\\] | . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#properties-3",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#properties-3"
  },"23": {
    "doc": "What is matrix (Ax = b)",
    "title": "Calculate the determinant",
    "content": "The Pivot Formula . \\[\\operatorname{det} A=(\\operatorname{det} L)(\\operatorname{det} U)=(1)\\left(d_{1} d_{2} \\cdots d_{n}\\right)\\] Pivots from determinants: . The $k$ th pivot is $\\boldsymbol{d}{\\boldsymbol{k}}=\\frac{d{1} d_{2} \\cdots d_{k}}{d_{1} d_{2} \\cdots d_{k-1}}=\\frac{\\operatorname{det} A_{k}}{\\operatorname{det} A_{k-1}}$. The Big Formula for Determinants . $\\operatorname{det} A=$ sum over all $\\mathbf{n} !$ column permutations $P=(\\alpha, \\beta, \\ldots, \\omega)$ \\(=\\sum(\\operatorname{det} P) a_{1 \\alpha} a_{2 \\beta} \\cdots a_{n \\omega}=\\text { BIG FORMULA. }\\) . Determinant by Cofactors . The determinant is the dot product of any row $i$ of $A$ with its cofactors using other rows: COFACTOR FORMULA \\(\\operatorname{det} A=a_{i 1} C_{i 1}+a_{i 2} C_{i 2}+\\cdots+a_{i n} C_{i n} \\text {. }\\) Each cofactor $C_{i j}$ (order $n-1$, without row $i$ and column $j$ ) includes its correct sign: Cofactor $C_{i j}=(-1)^{i+j} \\operatorname{det} M_{i j}$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#calculate-the-determinant",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#calculate-the-determinant"
  },"24": {
    "doc": "What is matrix (Ax = b)",
    "title": "Solving",
    "content": "Elimination . To eliminate $x$: Subtract a multiple of equation $1$ from equation $2$. Pivot = first nonzero in the row that does the elimination . Multiplier = (entry to eliminate) divided by (pivot) = $\\ell_{i j}=\\frac{\\text { entry to eliminate in row } i}{\\text { pivot in row } j}$ . Possible result . | Permanent failure with no solution. | Failure with infinitely many solutions. | . $A = LU$ . This is elimination without row exchanges. The multipliers $l_{ij}$ are below the diagonal of $L$. Elimination: factorization . \\[A=L U=\\text { (lower triangular) (upper triangular) }=(E_1^{-1}E_2^{-1}\\ldots E_n^{-1})U\\] Elimination on $A$ requires about $\\frac{1}{3} n^{3}$ multiplications and $\\frac{1}{3} n^{3}$ subtractions. Substitution: solve . \\[\\text { Forward and backward: Solve } \\quad L c=b \\quad \\text { and then solve } \\quad U x=c\\] Each right side $b/c$ needs $n^{2}$ multiplications and $n^{2}$ subtractions. | Band matrix $B$ (with $w$ nonzero diagonals) | . $A$ to $U$: $\\frac{1}{3} n^{3}$ reduces to $n w^{2}$ . Solve: $n^{2}$ reduces to $2 n w$ . $A = LDU$ . \\[\\text { Split } U \\text { into }\\left[\\begin{array}{llll} d_{1} &amp; &amp; &amp; \\\\ &amp; d_{2} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; d_{n} \\end{array}\\right]\\left[\\begin{array}{cccc} 1 &amp; u_{12} / d_{1} &amp; u_{13} / d_{1} &amp; \\cdot \\\\ &amp; 1 &amp; u_{23} / d_{2} &amp; \\cdot \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; 1 \\end{array}\\right]\\] $S=LDL^{\\mathrm{T}}$ . If $S=S^{\\mathrm{T}}$ is factored into $L D U$ with no row exchanges, then $U$ is exactly $L^{\\mathrm{T}}$. $A = LPU$ . permutation . Inversion . \\[\\text { Multiply } A x=b \\text { by } A^{-1} \\text {. Then } x=A^{-1} A x=A^{-1} b \\text {. }\\] Cramer’s Rule . explicit formula but huge complexity $(n+1) !$ . If $\\operatorname{det} A$ is not zero, $A \\boldsymbol{x}=\\boldsymbol{b}$ is solved by determinants: \\(x_{1}=\\frac{\\operatorname{det} B_{1}}{\\operatorname{det} A} \\quad x_{2}=\\frac{\\operatorname{det} B_{2}}{\\operatorname{det} A} \\quad \\ldots \\quad x_{n}=\\frac{\\operatorname{det} B_{n}}{\\operatorname{det} A}\\) The matrix $B_{j}$ has the jth column of A replaced by the vector $b$. Set $B=I$ we can find inverse matrix. \\[[A]\\left[\\begin{array}{lll} \\boldsymbol{x}_{1} &amp; 0 &amp; 0 \\\\ \\boldsymbol{x}_{2} &amp; 1 &amp; 0 \\\\ \\boldsymbol{x}_{3} &amp; 0 &amp; 1 \\end{array}\\right]=\\left[\\begin{array}{lll} \\boldsymbol{b}_{\\mathbf{1}} &amp; a_{12} &amp; a_{13} \\\\ \\boldsymbol{b}_{\\mathbf{2}} &amp; a_{22} &amp; a_{23} \\\\ \\boldsymbol{b}_{\\mathbf{3}} &amp; a_{32} &amp; a_{33} \\end{array}\\right]=B_{1}\\] \\[(\\operatorname{det} A)\\left(x_{1}\\right)=\\operatorname{det} B_{1} \\quad \\text { or } \\quad x_{1}=\\frac{\\operatorname{det} B_{1}}{\\operatorname{det} A} .\\] Take determinants of the three matrices to find $x_1$: . Product rule \\((\\operatorname{det} A)\\left(x_{1}\\right)=\\operatorname{det} B_{1} \\quad \\text { or } \\quad x_{1}=\\frac{\\operatorname{det} B_{1}}{\\operatorname{det} A}\\) . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#solving",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#solving"
  },"25": {
    "doc": "What is matrix (Ax = b)",
    "title": "Derivative $A$",
    "content": "Inner product of functions . | \\[x^{\\mathbf{T}} y=(x, y)=\\int_{-\\infty}^{\\infty} x(t) y(t) d t\\] | \\[(A \\boldsymbol{x})^{\\mathrm{T}} \\boldsymbol{y}=\\boldsymbol{x}^{\\mathrm{T}}\\left(A^{\\mathrm{T}} \\boldsymbol{y}\\right)\\] | \\[(A x, y)=\\int_{-\\infty}^{\\infty} \\frac{d x}{d t} y(t) d t=\\int_{-\\infty}^{\\infty} x(t)\\left(-\\frac{d y}{d t}\\right) d t=\\left(x, A^{\\mathrm{T}} y\\right)\\] | . the transpose of the derivative is minus the derivative . \\[A=d / d t \\quad A^{\\mathrm{T}}=-d / d t\\] Forward difference matrix to backward difference matrix . \\[A=\\left[\\begin{array}{rrrr} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{array}\\right] \\quad \\text { transposes to } \\quad A^{\\mathrm{T}}=\\left[\\begin{array}{rrrr} 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array}\\right]=-A\\] . | 1st derivative is antisymmetric | 2st derivative as symmetric | . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#derivative-a",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#derivative-a"
  },"26": {
    "doc": "What is matrix (Ax = b)",
    "title": "Cross Product",
    "content": ". \\[\\|\\boldsymbol{u} \\times \\boldsymbol{v}\\|=\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\||\\sin \\theta| \\quad \\text { and } \\quad|\\boldsymbol{u} \\cdot \\boldsymbol{v}|=\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\||\\cos \\theta|\\] . turning force or torque . \\[u \\times F\\] position of a mass: $\\left(u_{1}, u_{2}, u_{3}\\right)$ . force acting on it: $\\left(F_{x}, F_{y}, F_{z}\\right)$ . moment . $|\\boldsymbol{u}||\\boldsymbol{F}| \\sin \\theta$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/#cross-product",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/#cross-product"
  },"27": {
    "doc": "What is matrix (Ax = b)",
    "title": "What is matrix (Ax = b)",
    "content": " ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Fundamentals/",
    "relUrl": "/docs/Linear-Algebra/Fundamentals/"
  },"28": {
    "doc": "Fundamentals",
    "title": "Transfromations of Independent Variable",
    "content": "Time Shift . \\[x[n]\\longrightarrow x[n-\\color{red}{n_0}]\\] \\[x(t)\\longrightarrow x(t-\\color{red}{t_0})\\] The origin moves form 0 to $\\color{red}{n_0}$ or $\\color{red}{t_0}$. delayed: $\\color{red}{n_0}/\\color{red}{t_0}&gt;0$. advanced: $\\color{red}{n_0}/\\color{red}{t_0}&lt;0$. Time Reversal . \\[x[n]\\longrightarrow x[\\color{red}{-}n]\\] \\[x(t)\\longrightarrow x(\\color{red}{-}t)\\] Time Scaling . \\[x[\\alpha n]\\longrightarrow \\mathrm{harizontally \\ scaled \\ by \\ a \\ factor \\ of} \\ \\frac{1}{\\alpha}\\] \\[x(\\alpha t)\\longrightarrow \\mathrm{harizontally \\ scaled \\ by \\ a \\ factor \\ of} \\ \\frac{1}{\\alpha}\\] Properties of Functions . Parity . \\[even \\ \\ x[n]=-x[n] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x(t)=-x(t)\\] \\[odd \\ x[-n]=-x[n] \\ \\ \\ \\ \\ \\ \\ \\ x(-t)=-x(t)\\] Any signal can be broken into a sum of two signals: . \\[x(t)=\\mathcal{Ev}\\{x(t)\\}+\\mathcal{Od}\\{x(t)\\}\\] \\[\\mathcal{Ev}\\{x(t)\\}=\\frac{1}{2}[x(t)+x(-t)]\\] \\[\\mathcal{Od}\\{x(t)\\}=\\frac{1}{2}[x(t)-x(-t)]\\] Periodicity . \\[x(t)=x(t+T)\\] Exponential signals and Sinusoidal signals . Real exponential signals . \\[x(t)=C\\mathrm{e}^{at}\\] Sinusoidal signals . \\[x(t)=A\\cos(\\omega_0t+\\phi)\\] amplitude: $A [-]$ . fundamental period: $T_0=\\frac{2\\pi}{\\omega_0}=\\frac{1}{f}$ . ordinary frequency: $f \\ [\\mathrm{s}^{-1}]$ (rate of oscillation / cycles per second) . fundamental/angular frequency: $[\\mathrm{rad \\ s}^{-1}] \\ \\omega_0=\\frac{2\\pi}{T_0}=2\\pi f$ . phase: $\\phi \\ [\\mathrm{rad}]$ (${\\pi} \\ \\mathrm{rad}=180^\\mathrm{o}$) . Forms of signals expression . Using Euler’s Formula, we can express or transform signals between exponential form and sinusoidal form. \\[A\\cos(\\omega_0t+\\phi)=\\frac{A}{2}\\mathrm{e}^{j(\\omega_0t+\\phi)}+\\frac{A}{2}\\mathrm{e}^{-j(\\omega_0t+\\phi)}=\\frac{A}{2}\\mathrm{e}^{j\\phi}\\mathrm{e}^{j\\omega_0t}+\\frac{A}{2}\\mathrm{e}^{-j\\phi}\\mathrm{e}^{-j\\omega_0t}\\] Complex exponential form . \\[\\color{red}{C\\mathrm{e}^{j(\\omega_0t+\\phi)}}=|C|\\cos(\\omega_0t+\\phi)+|C|j\\sin(\\omega_0t+\\phi)\\] Sinusoidal form . \\[\\color{red}{A\\cos(\\omega_0t+\\phi)}=A\\mathcal{Re}\\{\\mathrm{e}^{j(\\omega_0t+\\phi)}\\}\\] \\[\\color{red}{A\\sin(\\omega_0t+\\phi)}=A\\mathcal{Im}\\{\\mathrm{e}^{j(\\omega_0t+\\phi)}\\}\\] We showed the properties using Continuous-time $x(t)$, the properties are also satisfied for discrete-time $x[n]$ but periodicity and phase change causing time shift. It is easy to understand because of discreate signals are not always uniformly distributed relative to its scale. ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#transfromations-of-independent-variable",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#transfromations-of-independent-variable"
  },"29": {
    "doc": "Fundamentals",
    "title": "Properties of Basic Signals and Systems",
    "content": " ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#properties-of-basic-signals-and-systems",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#properties-of-basic-signals-and-systems"
  },"30": {
    "doc": "Fundamentals",
    "title": "Signals",
    "content": "Unit Impulse . \\[\\delta[n]= \\begin{cases} 0&amp; \\text{n$\\neq$0}\\\\ 1&amp; \\text{n=0} \\end{cases} \\quad \\delta(t)=\\frac{\\mathrm{d}u(t)}{\\mathrm{d}t} \\, (\\mathrm{Area=1})\\] Unit impulse at $n=k$: $\\delta[n-k]$ . Unit step . \\[u[n]= \\begin{cases} 0&amp; \\text{n&lt;0}\\\\ 1&amp; \\text{n$\\geq$0} \\end{cases} \\quad u(t)= \\begin{cases} 0&amp; \\text{t&lt;0}\\\\ 1&amp; \\text{t&gt;0} \\end{cases}\\] Relations . | First backward difference: $\\delta[n]=u[n]-u[n-1]$ . | Running sum $u[n]=\\sum^n_{m=-\\infty}\\delta[m]$ . | . ​ Running integral $u(t)=\\int_{-\\infty}^t\\delta(\\tau)\\mathrm{d}\\tau$ . | Sequence superposition expression $u[n]=\\sum^\\infty_{k=0}\\delta[n-k]$ | . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#signals",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#signals"
  },"31": {
    "doc": "Fundamentals",
    "title": "Systems",
    "content": "Memoryless . \\[y(t)=x(t)\\] Its output for each value of the independent variable at a given time is dependent on the input at only that same time. Invertibility . \\[y(t)\\leftrightarrow x(t)\\] If distinct inputs lead to distinct outputs. Causality . \\[y(t)=x(t)+x(t-1)+...\\] If the output at any time depends on values of the input at only the present and past times. | “nonanticipative”: the system output does not anticipate future values of the input. | All memoryless systems are causal, since the output responds only to the current value of the input. | Initial rest: if the input to a causal system is 0 up to some point in time, then the output must also be 0 up to that time. | . Stability . Do not diverge. Time Invariance . \\[y(t+k)=x(t+k)\\] If a time shift in the input signal results in an identical time shift in the output signal.\\ . Not sensitive to the time origin. Linearity . \\[y_1(t)+y_2(t)=x_1(t)+x_2(t)\\] \\[ay_1(t)=ax_1(t)\\] The response to $x_1(t)+x_2(t)$ is $y_1(t)+y_2(t)$.\\ . The response to $ax_1(t)$ is $ay_1(t)$, where a is any complex constant. ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#systems",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#systems"
  },"32": {
    "doc": "Fundamentals",
    "title": "Convolution - Scaled Superposition",
    "content": ". | System: linear (superposition) \\&amp; time-invariant (LTI Systems) (for many physical processes) . | Signal: delayed impulses . | time scale: $k/\\tau$ time in impulse response; $n/t$ time in system . | . We find the signal can be constructed by scaled units: Signal is superposition of scaled and shifted unit impulse. Then the output signal can be represented by the superposition of response signal corresponding to the desired time point scaled by input signal: . \\[\\mathrm{Output signal \\ is \\ the \\ superposition \\ of \\ response signals \\ scaled \\ by \\ input signals.}\\] That is the convolution. Sifting property . \\[\\mathrm{arbitrary \\ sequence=linear\\ conbination \\ of \\ shifted \\ unit \\ impulses} \\ \\delta[n-k]\\] Discrete-time signal can be constructed by a a sequence of individual impulses. \\[\\mathrm{expresion\\ of \\ signal} \\ [\\color{red}{n}] = \\ &lt;\\color{blue}{\\mathrm{sifting \\ from -\\infty \\ to +\\infty}}&gt; \\ ( \\ \\mathrm{individual \\ impulse \\ at} \\ [\\color{blue}{k}]\\ )\\] The impulses can be constructed by linear conbination of unit impulses at that time. \\[( \\ \\mathrm{individual \\ impulse \\ at} \\ [\\color{blue}{k}]\\ )= ( \\ \\mathrm{magnitude \\ of \\ signal \\ at} \\ [\\color{blue}{k}]\\times \\mathrm{unit \\ impulse \\ at} \\ [\\color{blue}{k}] \\ )\\] So Discrete-time signal can be constructed by additional shifted, scaled impulses. \\[\\mathrm{expresion\\ of \\ signal} \\ [\\color{red}{n}]=\\color{blue}{\\mathrm{sifting \\ from -\\infty \\ to +\\infty}} \\ ( \\ \\mathrm{magnitude \\ of \\ signal \\ at} \\ [\\color{blue}{k}]\\times \\mathrm{unit \\ impulse \\ at} \\ [\\color{blue}{k}] \\ )\\] In summary: . By that we get the \\emph{sifting property} of discrete-time unit impulse: . \\[x[\\color{red}{n}]=\\color{blue}{\\sum_{k=-\\infty}^{+\\infty}}x[\\color{blue}{k}]\\delta[\\color{red}{n}-\\color{blue}{k}]\\] \\[x(t)=\\int_{-\\infty}^{+\\infty}x(\\tau)\\delta(t-\\tau)\\mathrm{d}\\tau\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#convolution---scaled-superposition",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#convolution---scaled-superposition"
  },"33": {
    "doc": "Fundamentals",
    "title": "Convolution Sum / Superpositional Sum",
    "content": "\\[y[n]=\\sum_{+\\infty}^{-\\infty}x[k]h[n-k]\\] \\[y[n]=x[n]*h[n]\\] Intuitively, $y[n]$ is a superposition of the response scaled by each corresponding impulse; mathematically, $y[n]$ is the sifting and sum of product between response and impluse at point n. This two interpretation is equivalent is because of the property of LTI System. It makes each response shifted without any change of profile, and signals is simply Addable. ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#convolution-sum--superpositional-sum",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#convolution-sum--superpositional-sum"
  },"34": {
    "doc": "Fundamentals",
    "title": "Convolution Integral / Superpositional Integral",
    "content": "Similarly, we firstly develop the \\emph{sifting property} of the continuous-time impulse: . \\[y(t)=\\int_{-\\infty}^{+\\infty}x(\\tau)h(t-\\tau)\\mathrm{d}\\tau\\] \\[y(t)=x(t)*h(t)\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#convolution-integral--superpositional-integral",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#convolution-integral--superpositional-integral"
  },"35": {
    "doc": "Fundamentals",
    "title": "Properties of Linear Time-invariance (LTI) System",
    "content": "Commutative property . \\[x[n]*h[n]=h[n]*x[n]\\] \\[x(t)*h(t)=h(t)*x(t)\\] Distributive property . \\[x[n]*(h_1[n]+h_2[n])=x[n]*h_1[n]+x[n]*h_2[n]\\] \\[x(t)*(h_1(t)+h_2(t))=x(t)*h_1(t)+x(t)*h_2(t)\\] Associative property . \\[x[n]*(h_1[n]*h_2[n])=(x[n]*h_1[n])*h_2[n]\\] \\[x(t)*[h_1(t)*h_2[t)]=[x(t)*h_1(t)]*h_2[t)\\] Is it memoryless? . A LTI system is only memoryless if the impulse response: . \\[h[n\\neq 0]=0\\] \\[h(t\\neq 0)=0\\] Then the impulse response has the form: . \\[h[n]=K\\delta [n]\\] \\[h(t)=K\\delta (t)\\] then the LTI system has the form: . \\[y[n]=Kx[n]\\] \\[y(t)=Kx(t)\\] Invertibility (Analogy: $AA^{-1}=E$) . \\[h[n]*h_i[n]=\\delta[n]\\] \\[h(t)*h_i(t)=\\delta(t)\\] Convolving a signal with a shifted impulse is the same as shifting the signal. \\[x(t-t_0)=x(t)*\\delta(t-t_0)\\] \\[h(t)*h_i(t)=\\delta(t-t_0)*\\delta(t+t_0)=\\delta(t)\\] Is it causal (initial rest) ? . A LTI system is only causal if and only if: . \\[h[n&lt;0]=0\\] \\[h(t&lt;0)=0\\] Stability . For bounded input the system has bounded output only if the impulse response is . Aabsolutely summable . \\[\\sum_{k=-\\infty}^{+\\infty}|h[k]|&lt;\\infty\\] Absolutely integrable . \\[\\int_{-\\infty}^{+\\infty}|h(\\tau)|\\mathrm{d}\\tau&lt;\\infty\\] Unit Step Response . The response of unit step. \\[s[n]=u[n]*h[n]\\] \\[s[n]=\\sum_{k=-\\infty}^nh[k]\\] \\[h[n]=s[n]-s[n-1]\\] \\[s(t)=\\int_{-\\infty}^th(\\tau)\\mathrm{d}\\tau\\] \\[h(t)=\\frac{\\mathrm{d}s(t)}{\\mathrm{d}t}=s'(t)\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#properties-of-linear-time-invariance-lti-system",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#properties-of-linear-time-invariance-lti-system"
  },"36": {
    "doc": "Fundamentals",
    "title": "Causal LTI system",
    "content": "Linear Constant-Coefficient Differential and Difference Equations (Process) . A constraint between the input and the output of a system. \\[\\mathrm{solution = particular \\ solution + homogeneous \\ solution}\\] particular: of, relating to, or concerned with details; denoting an individual member or subclass in logic. | Homogeneous solution / natural response: with source turned off, initial condition zero. (input is zero) . | Particular solution / forced response: with source turned on, initial condition zero. (have a source as input) . | . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#causal-lti-system",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#causal-lti-system"
  },"37": {
    "doc": "Fundamentals",
    "title": "Nth-order Linear Cnstant-Coefficient Differential Equations",
    "content": "\\[\\sum_{k=0}^Na_k\\frac{\\mathrm{d}^ky(t)}{\\mathrm{d}t^k}=\\sum_{k=0}^Nb_k\\frac{\\mathrm{d}^kx(t)}{\\mathrm{d}t^k}\\quad\\mathrm{solution}\\quad y_p(t)+y_h(t)\\] Homogeneous . \\[\\sum_{k=0}^Na_k\\frac{\\mathrm{d}^ky(t)}{\\mathrm{d}t^k}=0\\quad\\mathrm{solution}\\quad y_h(t)\\] Guess solution . \\[y_h(t)=A\\mathrm{e}^{st}\\] \\[\\sum_{k=0}^Na_kAs_k\\mathrm{e}^{st}=0\\] \\[\\sum_{k=0}^Na_ks_k=0\\quad N \\ \\mathrm{roots} \\ s_i(i=1,2,...,N)\\] \\[y_h(t)=A_1\\mathrm{e}^{s_1t}+A_2\\mathrm{e}^{s_2t}+...+A_N\\mathrm{e}^{s_Nt}\\] Auxiliary Conditions . \\[y(t_0),\\frac{\\mathrm{d}y(t_0)}{\\mathrm{d}t},...,\\frac{\\mathrm{d}^{N-1}y(t_0)}{\\mathrm{d}t^{N-1}}\\] Initial Rest Condition . \\[y(t_0)=\\frac{\\mathrm{d}y(t_0)}{\\mathrm{d}t}=...=\\frac{\\mathrm{d}^{N-1}y(t_0)}{\\mathrm{d}t^{N-1}}=0\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#nth-order-linear-cnstant-coefficient-differential-equations",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#nth-order-linear-cnstant-coefficient-differential-equations"
  },"38": {
    "doc": "Fundamentals",
    "title": "Nth-order Linear Constant-coefficient Difference Equations",
    "content": "\\[\\sum_{k=0}^Na_ky[n-k]=\\sum_{k=0}^Nb_kx[n-k]\\quad\\mathrm{solution}\\quad y_p[t]+y_h[t]\\] Homogeneous . \\[\\sum_{k=0}^Na_ky[n-k]=\\=0\\quad\\mathrm{solution}\\quad y_h[n]\\] Guess solution . \\[y_h[n]=Az^{n}\\] \\[\\sum_{k=0}^Na_kAz^nz^{-k}=0\\] \\[\\sum_{k=0}^Na_kz^{-k}=0\\quad N \\ \\mathrm{roots} \\ z_i(i=1,2,...,N)\\] \\[y_h[n]=A_1z_1^n+A_2z_2^n+...+A_Nz_N^n\\] Auxiliary Conditions . \\[y[n_0],y[n_0-1],...y[n_0-N+1]\\] Recursive Equation - Infinite Impulse Response (IIR) Systems . \\[y[n]=\\frac{1}{a_0}\\left\\{\\sum_{k-0}^Mb_kx[n-k]-\\sum_{k=1}^Na_ky[n-k]\\right\\}\\] It specifies a recursive procedure for determining the output in terms of the input and previous outputs. | Recursion: the determination of a succession of elements (such as numbers or functions) by operation on one or more preceding elements according to a rule or formula involving a finite number of steps. | . Nonrecursive Equation - Finite Impulse Response (FIR) Systems . \\[y[n]=\\sum_{k=0}^M\\left(\\frac{b_k}{a_0}\\right)x[n-k]\\] Block Diagram Representations . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#nth-order-linear-constant-coefficient-difference-equations",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#nth-order-linear-constant-coefficient-difference-equations"
  },"39": {
    "doc": "Fundamentals",
    "title": "Singularity Functions",
    "content": "This part relate the convolution and differentiation/integration together. And the tool is the singularity function. Operational definition of singularity function - differentiator \\&amp; integrator . Singular functions can be defined operationally in terms of its behavior under convolution. Belows are the examples of operational definitions of singularity functions. Unit impulse $\\delta(t)$ . \\[x(t)=x(t)*\\delta(t)\\] Unit doublet $u_1(t)$ . \\[y_1(t)=\\frac{\\mathrm{d}x(t)}{\\mathrm{d}t}=x(t)*u_1(t)\\] Similarly, we can define a $u_2(t)$: . \\[y_2(t)=\\frac{\\mathrm{d}^2x(t)}{\\mathrm{d}t^2}=x(t)*u_2(t)\\] It is easy to find that: . \\[y_2(t)=\\frac{\\mathrm{d}^2x(t)}{\\mathrm{d}t^2}=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{\\mathrm{d}x(t)}{\\mathrm{d}t}\\right)=(x(t)*u_1(t))*u_1(t)\\] Then . \\[u_2(t)=u_1(t)*u_1(t)\\] Cascades of differentiators $u_{k}(t)$ of order $k$ . $u_k(t)$ (k&gt;0) is the kth derivative of $\\delta(t)$ and thus is the impulse response of a system that takes the kth derivative of the input. \\[u_k(t)=u_1(t)*...(k \\ \\mathrm{times})...*u_1(t)\\] Cascades of integrators $u_{-k}(t)$ of order $k$ . An integrator: . \\[y(t)=\\int_{-\\infty}^{t}x(\\tau)\\mathrm{d}\\tau\\] Then we can use the character of integrator to represent a unit step impulse response: . \\[u(t)=\\int_{-\\infty}^{t}\\delta(\\tau)\\mathrm{d}\\tau\\] Then the following operation definition of $u(t)$ (operate on an input $x(t)$: . \\[x(t)*u(t)=\\int_{-\\infty}^{t}x(\\tau)\\mathrm{d}\\tau\\] Similarly, define a system of a cascade of two integrators: . \\[u_{-2}(t)=u(t)*u(t)=\\int_{-\\infty}^{t}u(\\tau)\\mathrm{d}\\tau\\] By the way, since $u(t)$ is a unit step impulse response, here is the \\emph{unit ramp function}: . \\[u_{-2}(t)=tu(t)\\] And we get the cascades of integrators: . \\[u_{-k}(t)=u(t)*...(k \\ \\mathrm{times})...*u(t)=\\int_{-\\infty}^tu_{-(k-1)}(\\tau)\\mathrm{d}\\tau\\] \\[\\delta(t)=u_0(t)\\] \\[u(t)=u_{-1}(t)\\] \\[u_k(t)*u_r(t)=u_{k+r}(t)\\] Invertibility $u(t)u_1(t)=u_{-1}(t)u_1(t)=\\delta(t)$ . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#singularity-functions",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#singularity-functions"
  },"40": {
    "doc": "Fundamentals",
    "title": "Fourier Series Representation of Periodic Signals",
    "content": "Tigonometric sum Sums of harmonically related sines and cosines or periodic . complex exponentials. ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#fourier-series-representation-of-periodic-signals",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#fourier-series-representation-of-periodic-signals"
  },"41": {
    "doc": "Fundamentals",
    "title": "Response of LTI Systems to Complex Exponentials",
    "content": "The response of an LTI system to a complex exponential input is the same complex . exponential with only a change in amplitude. \\[\\mathrm{contunuous \\ time} \\quad e^{st}\\longrightarrow H(s)e^{st}\\] \\[\\mathrm{discrete \\ time} \\quad z^n \\longrightarrow H(z)z^n\\] \\[y(t)=\\int_{-\\infty}^{+\\infty}h(\\tau)x(t-\\tau)\\mathrm{d}\\tau=\\int_{-\\infty}^{+\\infty}h(\\tau)\\mathrm{e}^{s(t-\\tau)}\\mathrm{d}\\tau\\] \\[y(t)=e^{st}\\int_{-\\infty}^{+\\infty}h(\\tau)\\mathrm{e}^{-s\\tau}\\mathrm{d}\\tau\\] Assuming that the integral on the RHS converges, . \\[y(t)=H(s)e^{st}\\] Hence, we have shown that complex exponentials are eigenfunctions of LTI systems.The constant $H(s)$ for a specific value of $s$ bis then the eigenvalue associated with the eigenfunction $e^{st}$. \\paragraph{Eigenfunction $e^{st}$} A signal for which the system output is a (possibly complex) constant times the input. Eigenvalue $H(s)$ Amplitude (constant). System Functions . \\[H(s)=\\int_{-\\infty}^{+\\infty}h(\\tau)\\mathrm{e}^{-s\\tau}\\mathrm{d}\\tau\\] \\[H(z)=\\sum_{-\\infty}^{+\\infty}h[k]z^{-k}\\] Frequency Response Response in frequency domain? $\\mathcal{Re}{(s)}=0\\rightarrow s=j\\omega$ . \\[H(j\\omega)=\\int_{-\\infty}^{+\\infty}h(t)\\mathrm{e}^{-j\\omega t}\\mathrm{d}t\\] \\[H(\\mathrm{e}^{j\\omega})=\\sum_{-\\infty}^{+\\infty}h[n]\\mathrm{e}^{j\\omega n}\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#response-of-lti-systems-to-complex-exponentials",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#response-of-lti-systems-to-complex-exponentials"
  },"42": {
    "doc": "Fundamentals",
    "title": "Fourier Series",
    "content": "\\[\\mathrm{synthesis \\ equation}\\quad x(t)=\\sum_{k=-\\infty}^{+\\infty}a_ke^{jk\\omega_0t}=\\sum_{k=-\\infty}^{+\\infty}a_ke^{jk(2\\pi/T)t}\\] Fourier Series Coefficients / Spectral Coefficients of $x(t)$ . \\[\\mathrm{analysis \\ equation}\\quad a_k=\\frac{1}{T}\\int_Tx(t)e^{-jk\\omega_0t}\\mathrm{d}T=\\frac{1}{t}\\int_Tx(t)e^{-jk(2\\pi/T)t}\\mathrm{d}t\\] Fundamental Harmonic Component/First Harmonic Components $x(t)$ when $k=\\pm 1$ . Nth Harmonic Components $x(t)$ when $k=\\pm N$ . \\[a_0=\\frac{1}{T}\\int_Tx(t)\\mathrm{d}t\\] Real Periodic Signals $a_k=A_k$ . \\[x(t)=a_0+2\\sum_{k=1}^\\infty A_k\\cos (k\\omega_0t+\\theta_k)\\] Complex Periodic Signals $a_k=B_k+jC_k$ . \\[x(t)=a_0+2\\sum_{k=1}^\\infty[B_k\\cos k\\omega_0t-C_k\\sin k\\omega_0t]\\] Periodic Square Wave . \\[a_0=\\frac{2T_1}{T}\\quad k=0\\] \\[a_k=\\frac{\\sin (k\\omega_0T_1)}{k\\pi}\\quad k\\neq 0\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#fourier-series",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#fourier-series"
  },"43": {
    "doc": "Fundamentals",
    "title": "Convergence Conditions of the Fourier Series",
    "content": "Finite Energy over a Single Period . \\[\\int_T|x(t)|^2\\mathrm{d}t&lt;\\infty\\] Dirichlet Conditions . | Condition 1: Over any period, $x(t)$ must be absolutely integrable; this guarantees that each coefficient ak will be finite. | . \\[\\int_T|x(t)|\\mathrm{d}t&lt;\\infty\\] . | Condition 2: In any finite interval of time, $x(t)$ is of bounded variation; that is, there are no more than a finite number of maxima and minima during any single period of the signal. | Condition 3: In any finite interval of time, there are only a finite number of discontinuities. Furthermore, each of these discontinuities is finite. | . Gibbs Phenomenon . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#convergence-conditions-of-the-fourier-series",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#convergence-conditions-of-the-fourier-series"
  },"44": {
    "doc": "Fundamentals",
    "title": "Properties of Continuous-time Fourier Series",
    "content": "Same period $T$ . \\[x(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow a_k\\] \\[y(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow b_k\\] Linearity . \\[Ax(t)+By(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow Aa_k+Bb_k\\] Time Shifting . \\[x(t-t_0)\\stackrel{\\mathcal{FS}}\\longleftrightarrow a_k\\mathrm{e}^{-jk\\omega_0t_0}=a_ke^{-jk(2\\pi/T)t_0}\\] Frequency Shifting . \\[e^{jM\\omega_0t}x(t)=e^{jM(2\\pi/T)t}x(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow a_{k-M}\\] Conjugation . \\[x^*(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow a^*_{-k}\\] Time Reversal . \\[x(-t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow a_{-k}\\] Time Scaling $\\alpha&gt;0$, $x(\\alpha t)\\longleftrightarrow T/\\alpha\\quad\\alpha\\omega_0$ . \\[x(\\alpha t)=\\sum_{k=-\\infty}^{+\\infty}a_k\\mathrm{e}^{jk\\alpha\\omega_0t}\\stackrel{\\mathcal{FS}}\\longleftrightarrow a_{k}\\] Periodic Convolution . \\[\\int_Tx(\\tau)y(t-\\tau)\\mathrm{d}\\tau\\stackrel{\\mathcal{FS}}\\longleftrightarrow Ta_kb_k\\] Multiplication . Discrete-time convolution of the sequence representing the Fourier coefficients of $x(t)$ and the sequence representing the Fourier coefficients of $y(t)$. \\[x(t)y(t)\\stackrel{\\mathcal{FS}}\\longleftrightarrow \\sum_{l=-\\infty}^{+\\infty} a_lb_{k-l}\\] Differentiation . \\[\\frac{\\mathrm{d}x(t)}{\\mathrm{d}t}\\stackrel{\\mathcal{FS}}\\longleftrightarrow jk\\omega_0a_k=jk\\frac{2\\pi}{T}a_k\\] Integration Finite valued and periodic only if $a_0=0$. \\[\\int_{-\\infty}^tx(t)\\mathrm{d}t\\stackrel{\\mathcal{FS}}\\longleftrightarrow \\left(\\frac{1}{jk\\omega_0}\\right)a_k=\\left(\\frac{1}{jk(2\\pi/T)}\\right)a_k\\] Conjugate symmetry for Real Signals . $x(t)$ real $x(t)=x^*(t)$. \\[a_k=a_{-k}^*\\] \\[\\mathcal{Re}\\{a_k\\}=\\mathcal{Re}\\{a_{-k}\\}\\] \\[\\mathcal{Im}\\{a_k\\}=-\\mathcal{Im}\\{a_{-k}\\}\\] \\[|a_k|=|a_{-k}|\\] \\[\\arg a_k=-\\arg a_{-k}\\] Real and Even Signals . $x(t)$ real and even, $a_k$ real and even. Real and Odd Signals . $x(t)$ real and odd, $a_k$ purely imaginary and odd. Even-Odd Decomposition of Real Signals . \\[\\begin{cases} x_e(t)=\\mathcal{Ev}\\{x(t)\\} \\ [x(t) \\ \\mathrm{real}]&amp; \\mathcal{Re}\\{a_k\\}\\\\ x_o(t)=\\mathcal{Od}\\{x(t)\\} \\ [x(t) \\ \\mathrm{real}]&amp; j\\mathcal{Im}\\{a_k\\} \\end{cases}\\] Parseval’s Relation for Continuous-Time Periodic Signals . $|a_k|^2$ is the average power in the $k$th harmonic component of $x(t)$. Thus, what Parseval’s relation states is that the total average power in a periodic signal equals the sum of the average powers in all of its harmonic components. \\[\\frac{1}{T}\\int_T|x(t)|^2\\mathrm{d}t=\\frac{1}{T}\\int_T|a_ke^{jk\\omega_0t}|^2\\mathrm{d}t=\\sum_{k=-\\infty}^{+\\infty}|a_k|^2\\] Impulse Train . \\[x(t)=\\sum_{k=-\\infty}^{+\\infty}\\delta(t-kT)\\] Discrete-time Fourier Series . \\[\\phi_k[n]=\\mathrm{e}^{jk\\omega_0n}=\\mathrm{e}^{jk(2\\pi/T)n},k=0,\\pm 1,\\pm 2,...\\] \\[\\phi_k[n]=\\phi_{k+rN}[n]\\] \\[\\mathrm{synthesis \\ equation}\\quad x[n]=\\sum_{k=\\langle N\\rangle}a_k\\phi(n)=\\sum_{k=\\langle N\\rangle}a_k\\mathrm{e}^{jk\\omega_0n}=\\sum_{k=\\langle N\\rangle}a_k\\mathrm{e}^{jk(2\\pi/T)n}\\] \\[\\mathrm{analysis \\ equation}\\quad a_k=\\frac{1}{N}\\sum_{n=\\langle N\\rangle}x[n]\\mathrm{e}^{-jk\\omega_0n}=\\frac{1}{N}\\sum_{n=\\langle N\\rangle}x[n]\\mathrm{e}^{-jk(2\\pi/N)n}\\] First Difference . \\[x[n]-x[n-1]\\stackrel{\\mathcal{FS}}\\longleftrightarrow(1-\\mathrm{e}^{-jk(2\\pi/N)})a_k\\] Parseval’s Relation . Parseval’s relation states that the average power in a periodic signal equals the sum of the average powers in all of its harmonic components. \\[\\frac{1}{N}\\sum_{n=\\langle N\\rangle}|x[n]|^2=\\sum_{k=\\langle N\\rangle}|a_k|^2\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#properties-of-continuous-time-fourier-series",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#properties-of-continuous-time-fourier-series"
  },"45": {
    "doc": "Fundamentals",
    "title": "Filtering",
    "content": "Filtering . Fhange the relative amplitudes of the frequency components in a signal or perhaps eliminate some frequency components entirely. | Frequency-shaping Filters: Linear time-invariant systems that change the shape of the spectrum. | Frequency-selective Filters: Systems that are designed to pass some frequencies essentially undistorted and significantly attenuate or eliminate others. | . \\[x(t)=\\sum_ka_ke^{s_kt}\\longrightarrow y(t)=\\sum_ka_kH(s_k)e^{s_kt}\\] \\[x[n]=\\sum_ka_kz_k^n\\longrightarrow y[n]=\\sum_ka_kH(z_k)z_k^n\\] ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/#filtering",
    "relUrl": "/docs/Signals-Systems/Fundamentals/#filtering"
  },"46": {
    "doc": "Fundamentals",
    "title": "Fundamentals",
    "content": " ",
    "url": "http://localhost:4000/docs/Signals-Systems/Fundamentals/",
    "relUrl": "/docs/Signals-Systems/Fundamentals/"
  },"47": {
    "doc": "Information and Probability",
    "title": "Information theory",
    "content": ". conveying information . | “setup”: agree on what they will communicate about, , and exactly what each sequence of bits means - code. | “outcome”: sequences of information are sent - data. | . uncertainty . Boolean functions . Properties of Boolean Algebra . Logic gates . quantum bit / qubit . a model of an object that can store a single bit but is so small that it is subject to the limitations quantum mechanics places on measurements . | Reversibility: transformation between each state. | Superposition | Entanglement | . classical bit . an abstraction in which the bit can be measured without perturbing it. \\[0V-0.2V=0,0.8V-1V=1\\] encode and decode . symbol space size . the number of symbols that need to be encoded . Integer codes . Morse code . Compression . | Variable-Length Encoding . | Run Length Encoding . | Static Dictionary . | Semi-adaptive Dictionary . | Dynamic Dictionary - LZW compression technique . | Discrete Cosine Transformation . | . Hamming Distance . the diﬀerence between two bit patterns is the number of bits that are diﬀerent between the two. code rate . the number of bits before channel coding divided by the number after the encoder. Parity . “check bit”added bit would be 1 if the number of bits equal to 1 is odd, and 0 otherwise. Rectangular Codes . ",
    "url": "http://localhost:4000/docs/Info-Prob#information-theory",
    "relUrl": "/docs/Info-Prob#information-theory"
  },"48": {
    "doc": "Information and Probability",
    "title": "Probability theory",
    "content": "Marginal Probability . \\[p(X=x_i)=\\sum_{j=1}^Lp(X=x_i,Y=y_j)\\] Conditional Probability of $Y=y_j$ given $X=x_i$ . \\[p(Y=y_i|X=x_i)=\\frac{n_ij}{ci}\\] For single point probability . \\[p(X=x_i,Y=y_j)=\\frac{n_{ij}}{c_i}\\cdot\\frac{c_i}{N}=p(Y=y_j|X=x_i)P(X=x_i)\\] Sum Rule . \\[p(X)=\\sum_Yp(X,Y)\\] Product Rule . \\[p(X,Y)=p(Y|X)p(X)\\] Bayes’ Theorem . \\[p(Y|X)=\\frac{p(X|Y)p(Y)}{p(X)}\\] \\[p(X)=\\sum_Yp(X|Y)p(Y)\\] Continuous form: . \\[p(x)=\\int p(x,y)\\mathrm{d}y\\] \\[p(x,y)=p(y|x)p(x)\\] Probability Density over $x$: $p(x)$ . \\[p(x\\in(a,b))=\\int_a^bp(x)\\mathrm{d}x\\] \\[p(x)\\geq0\\] \\[\\int_{-\\infty}^{\\infty}p(x)=1\\] Cumulative Distribution Function $P(z)$ . \\[P(z)=\\int_{-\\infty}^zp(x)\\mathrm{d}x\\] \\[P'(x)=p(x)\\] Joint Probability Density in $\\mathbf{x}$ Space . \\[p(\\mathbf{x})=p(x_1,...,x_D)\\] \\[p(\\mathbf{x})\\geq0\\] \\[\\int_{-\\infty}^{\\infty}p(\\mathbf{x})=1\\] If $\\mathbf{x}$ is discrete, $p(\\mathbf{x})$ is called \\emph{probability mass function}. Expectation of $f(x)$: $\\mathbb{E}(f)$ . \\[\\mathbb{E}[f]=\\sum_xp(x)f(x)\\] \\[\\mathbb{E}[f]=\\int p(x)f(x)\\mathrm{d}x\\] Approximation: . \\[\\mathbb{E}[f]\\simeq\\frac{1}{N}\\sum_{n=1}^N f(x_n)\\] Multi-variable: . \\[\\mathbb{E}_x[f(x,y)]\\] \\[\\mathbb{E}_x[f|y]=\\sum_xp(x|y)f(x)\\] Variance . \\[\\mathrm{var}[f]=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]\\] \\[\\mathrm{var}[f]=\\mathbb{E}[f(x)^2]-\\mathbb{E}[f(x)]^2\\] In particular, . \\[\\mathrm{var}[x]=\\mathbb{E}[x^2]-\\mathbb{E}[x]^2\\] Covariance of two random variables: . \\[\\mathrm{cov}[x,y]=\\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}]=\\mathbb{E}_{x,y}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]\\] For two vectors: . \\[\\mathrm{cov}[\\mathbf{x},\\mathbf{y}]=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^\\mathrm{T}-\\mathbb{E}[\\mathbf{y}^\\mathrm{T}]\\}]=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\mathbf{x}\\mathbf{y}^\\mathrm{T}]-\\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^\\mathrm{T}]\\] \\[\\mathrm{cov}[\\mathbf{x}]\\equiv\\mathrm{cov}[\\mathbf{x,x}]\\] Bayesian Probabilities . Quantify the uncertainty that surrounds the appropriate choice for the model parameters $\\mathbf{w}$. \\[\\mathrm{posterior\\propto likelihood\\times prior}\\] \\[p(\\mathbf{w}|\\mathcal{D})=\\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})}\\] \\[p(\\mathcal{D})=\\int p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})\\mathrm{d}\\mathbf{w}\\] Data $\\mathcal{D}$ . Parameter $\\mathbf{w}$ . Likelihood Function . \\[p(\\mathcal{D}|\\mathbf{w})=f(\\mathbf{w})\\] It expresses how probable the observed data set is for different settings of the parameter vector $\\mathbf{w}$. Estimator: Maximum Likelihood . Set $\\mathbf{w}$ to make $p(\\mathcal{D}|\\mathbf{w})$ maximum. Error Function . \\[-\\log(p(\\mathcal{D}|\\mathbf{w}))\\] Decision Theory . Minimizing the misclassification rate. Class $\\mathcal{C}_k$ . Decision Region $\\mathcal{R}_k$ . Decision Boundaries / Decision Surfaces . ",
    "url": "http://localhost:4000/docs/Info-Prob#probability-theory",
    "relUrl": "/docs/Info-Prob#probability-theory"
  },"49": {
    "doc": "Information and Probability",
    "title": "Permutation and Combination",
    "content": "\\[0!=1\\] \\[A_n^m=\\frac{n!}{(n-m)!}\\] \\[C_n^m=\\frac{n!}{m!(n-m)!}\\] ",
    "url": "http://localhost:4000/docs/Info-Prob#permutation-and-combination",
    "relUrl": "/docs/Info-Prob#permutation-and-combination"
  },"50": {
    "doc": "Information and Probability",
    "title": "Information and Probability",
    "content": " ",
    "url": "http://localhost:4000/docs/Info-Prob",
    "relUrl": "/docs/Info-Prob"
  },"51": {
    "doc": "Linear Algebra",
    "title": "The matrix alphabet",
    "content": "\\[\\begin{array}{llcl} \\boldsymbol{A} &amp; \\text { Any Matrix } &amp; \\boldsymbol{P} &amp; \\text { Permutation Matrix } \\\\ \\boldsymbol{B} &amp; \\text { Basis Matrix } &amp; \\boldsymbol{P} &amp; \\text { Projection Matrix } \\\\ \\boldsymbol{C} &amp; \\text { Cofactor Matrix } &amp; \\boldsymbol{Q} &amp; \\text { Orthogonal Matrix } \\\\ \\boldsymbol{D} &amp; \\text { Diagonal Matrix } &amp; \\boldsymbol{R} &amp; \\text { Upper Triangular Matrix } \\\\ \\boldsymbol{E} &amp; \\text { Elimination Matrix } &amp; \\boldsymbol{R} &amp; \\text { Reduced Echelon Matrix } \\\\ \\boldsymbol{F} &amp; \\text { Fourier Matrix } &amp; \\boldsymbol{S} &amp; \\text { Symmetric Matrix } \\\\ \\boldsymbol{H} &amp; \\text { Hadamard Matrix } &amp; \\boldsymbol{T} &amp; \\text { Linear Transformation } \\\\ \\boldsymbol{I} &amp; \\text { Identity Matrix } &amp; \\boldsymbol{U} &amp; \\text { Upper Triangular Matrix } \\\\ \\boldsymbol{J} &amp; \\text { Jordan Matrix } &amp; \\boldsymbol{U} &amp; \\text { Left Singular Vectors } \\\\ \\boldsymbol{K} &amp; \\text { Stiffness Matrix } &amp; \\boldsymbol{V} &amp; \\text { Right Singular Vectors } \\\\ \\boldsymbol{L} &amp; \\text { Lower Triangular Matrix } &amp; \\boldsymbol{X} &amp; \\text { Eigenvector Matrix } \\\\ \\boldsymbol{M} &amp; \\text { Markov Matrix } &amp; \\boldsymbol{\\Lambda} &amp; \\text { Eigenvalue Matrix } \\\\ \\boldsymbol{N} &amp; \\text { Nullspace Matrix } &amp; \\boldsymbol{\\Sigma} &amp; \\text { Singular Value Matrix } \\end{array}\\] Minor $\\mathbf{M}_{ij}$ . Operational Definition: . \\[\\mathbf{M}_{2,3}=\\operatorname{det}\\left[\\begin{array}{ccc} 1 &amp; 4 &amp; \\square \\\\ \\square &amp; \\square &amp; \\square \\\\ -1 &amp; 9 &amp; \\square \\end{array}\\right]=\\operatorname{det}\\left[\\begin{array}{cc} 1 &amp; 4 \\\\ -1 &amp; 9 \\end{array}\\right]=9-(-4)=13\\] Adjugate Matrix $\\operatorname{adj}(\\mathbf{A})$ or $\\mathbf{A}^*$ . \\[\\mathbf{C}_{ij}=(-1)^{ij}\\mathbf{M}_{ij}\\] \\[\\operatorname{adj}(\\mathbf{A})=\\mathbf{C^T}\\] Determinant $\\operatorname{det}(\\mathbf{A})$ or$|\\mathbf{A}|$ . Inverse Matrix $\\mathbf{A}^{-1}$ . \\[\\mathbf{A}^{-1}=\\frac{\\operatorname{adj}(\\mathbf{A})}{\\operatorname{det}(\\mathbf{A})}\\] $2\\times2$ Matrix . \\[\\left[\\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array}\\right]^{-1}= \\left[\\begin{array}{cc} d &amp; -b \\\\ -c &amp; a \\end{array}\\right]\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra#the-matrix-alphabet",
    "relUrl": "/docs/Linear-Algebra#the-matrix-alphabet"
  },"52": {
    "doc": "Linear Algebra",
    "title": "Calculation Rules",
    "content": "Product . Hadamard Product / Schur Product / Element-wise Product / Entrywise Product . \\[(A\\circ B)_{ij}=(A\\odot B)_{ij}=(A)_{ij}(B)_{ij}\\] Dot Product / Scalar Product . \\[A\\cdot B\\] Matrix Multiplication . \\[AB\\] Cross Product . \\[A\\times B\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra#calculation-rules",
    "relUrl": "/docs/Linear-Algebra#calculation-rules"
  },"53": {
    "doc": "Linear Algebra",
    "title": "Vector Calculus",
    "content": "Scalar Fields . Vector Fields . Vectors and Pseudovectors . Vector Addition: 2 vectors $\\longrightarrow$ 1 vector . \\[\\textbf{v}_1+\\textbf{v}_2\\] Scalar Multiplication: 1 scalar $+$ 1 vector $\\longrightarrow$ 1 vector . \\[\\alpha\\textbf{v}_2\\] Dot Product: 2 vector $\\longrightarrow$ 1 scalar . \\[\\textbf{v}_1\\cdot\\textbf{v}_2\\] Cross Product: 2 vectors in $\\mathbb{R}^3\\longrightarrow$ 1 (pseudo)vector . \\[\\textbf{v}_1\\times\\textbf{v}_2\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra#vector-calculus",
    "relUrl": "/docs/Linear-Algebra#vector-calculus"
  },"54": {
    "doc": "Linear Algebra",
    "title": "Operators",
    "content": "Gradient: scalar fields $\\longrightarrow$ vector fields . \\[\\mathrm{grad}(f)=\\nabla f\\] Divergence: vector fields $\\longrightarrow$ scalar fields . \\[\\mathrm{div}(\\textbf{F})=\\nabla\\cdot\\textbf{F}\\] Curl: vector fields $\\longrightarrow$ vector fields . \\[\\mathrm{curl}(\\textbf{F})=\\nabla\\times\\textbf{F}\\] Laplacian: vector fields $\\longrightarrow$ vector fields . \\[\\Delta f=\\nabla^2f=\\nabla\\cdot\\nabla f\\] Vector Laplacian: vector fields $\\longrightarrow$ vector fields . \\[\\Delta \\textbf{F}=\\nabla^2\\textbf{F}=\\nabla(\\nabla\\cdot\\textbf{F})-\\nabla\\times(\\nabla\\times\\textbf{F})\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra#operators",
    "relUrl": "/docs/Linear-Algebra#operators"
  },"55": {
    "doc": "Linear Algebra",
    "title": "Integral Theorems",
    "content": "Gradient Theorem . \\[\\int_{L\\subset\\mathbb{R}^n}\\nabla\\phi\\cdot\\mathrm{d}r=\\phi(q)-\\phi(p) \\ for \\ L=L[p\\rightarrow q]\\] Divergence Theorem . \\[\\int...\\int_{V\\subset\\mathbb{R}^n}(\\nabla\\cdot\\textbf{F})\\mathrm{d}V=\\oint...\\oint_{\\partial V}F\\cdot\\mathrm{d}S\\] Curl (Kelvin-Stokes) Theorem . \\[\\iint_{\\Sigma\\subset\\mathbb{R}^3}(\\nabla\\times\\textbf{F})\\mathrm{d}\\Sigma=\\oint_{\\partial\\Sigma}\\textbf{F}\\mathrm{d}r\\] Green’s Theorem (Divergence/Curl Theorem in 2D) . \\[\\iint_{A\\subset\\mathbb{R}^2}-\\left(\\frac{\\partial L}{\\partial y}\\right)\\mathrm{d}A=\\oint_{\\partial A}(L\\mathrm{d}x+M\\mathrm{d}y)\\] Jacobian Matrix and Determinant . \\[\\textbf{f}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\] \\[\\textbf{J}_{ij}=\\frac{\\partial f_i}{\\partial x_j}\\] \\[\\textbf{J}=\\nabla ^\\mathrm{T}\\textbf{f}\\] \\[\\mathbf{J}=\\left[\\begin{array}{ccc} \\frac{\\partial \\mathbf{f}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial \\mathbf{f}}{\\partial x_{n}} \\end{array}\\right]=\\left[\\begin{array}{c} \\nabla^{\\mathrm{T}} f_{1} \\\\ \\vdots \\\\ \\nabla^{\\mathrm{T}} f_{m} \\end{array}\\right]=\\left[\\begin{array}{ccc} \\frac{\\partial f_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial f_{m}}{\\partial x_{n}} \\end{array}\\right]_{m \\times n}\\] If $m=n$, Jacobian matrix has its determinant $|\\textbf{J}|$. Its value represents the transformation ratio of $\\textbf{f}$. Hessian Matrix . \\[f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\] \\[(\\textbf{H}_{f})_{ij}=\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}\\] \\[\\mathbf{H}_{f}=\\left[\\begin{array}{cccc} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} &amp; \\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{2}} &amp; \\cdots &amp; \\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{n}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{1}} &amp; \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} &amp; \\cdots &amp; \\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{n}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{1}} &amp; \\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{2}} &amp; \\cdots &amp; \\frac{\\partial^{2} f}{\\partial x_{n}^{2}} \\end{array}\\right]_{n \\times n}\\] \\[\\textbf{H}(f(\\textbf{x}))=\\textbf{J}(\\nabla f(\\textbf{x}))=\\nabla ^\\mathrm{T}(\\nabla f(\\textbf{x}))\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra#integral-theorems",
    "relUrl": "/docs/Linear-Algebra#integral-theorems"
  },"56": {
    "doc": "Linear Algebra",
    "title": "Linear Algebra",
    "content": "Reference: Strang, Gilbert, et al. Introduction to linear algebra. Vol. 3. Wellesley, MA: Wellesley-Cambridge Press, 1993. ",
    "url": "http://localhost:4000/docs/Linear-Algebra",
    "relUrl": "/docs/Linear-Algebra"
  },"57": {
    "doc": "Logic",
    "title": "Proofs",
    "content": " ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#proofs",
    "relUrl": "/docs/Fundamental/Logic/#proofs"
  },"58": {
    "doc": "Logic",
    "title": "Logics",
    "content": "Propositions . A proposition is a statement that is either true or false. Boolean variables . $\\mathbf{T}$ (true) and $\\mathbf{F}$ (false) . Truth tables . Compound propositions . | English | Symbolic Notation | . | $\\operatorname{NOT}(P)$ | $\\neg P / \\bar{P}$ | . | $P \\text { AND } Q$ | $P \\wedge Q$ | . | $P \\text { OR } Q$ | $ P \\vee Q$ | . | $P \\text { IMPLIES } Q$ | $P \\longrightarrow Q$ | . | $\\text { if } P \\text { then } Q$ | $P \\longrightarrow Q$ | . | $P \\text { IFF } Q$ | $P \\longleftrightarrow Q$ | . NOT . | $P$ | $\\operatorname{NOT}(P)$ | . | T | F | . | F | T | . AND . | $P$ | $Q$ | $P\\operatorname{AND}Q$ | . | T | T | T | . | T | F | F | . | F | T | F | . | F | F | F | . OR . | $P$ | $Q$ | $P\\operatorname{OR}Q$ | . | T | T | T | . | T | F | T | . | F | T | T | . | F | F | F | . EXCLUSIVE-OR . | $P$ | $Q$ | $P\\operatorname{XOR}Q$ | . | T | T | F | . | T | F | T | . | F | T | T | . | F | F | F | . IMPLIES An implication is true exactly when the if-part is false or the then-part is true. | $P$ | $Q$ | $P\\operatorname{IMPLIES}Q$ | . | T | T | T | . | T | F | F | . | F | T | T | . | F | F | T | . IF AND ONLY IF . | $P$ | $Q$ | $P\\operatorname{IFF}Q$ | . | T | T | T | . | T | F | F | . | F | T | F | . | F | F | T | . Contrapositive Logically equivalent implications . Converse . | $P$ | $Q$ | $P\\operatorname{IMPLIES}Q$ | $Q\\operatorname{IMPLIES}P$ | . | T | T | T | T | . | T | F | F | T | . | F | T | T | F | . | F | F | T | T | . Predicates . A predicate is a proposition whose truth depends on the value of one or more variables. Quantiﬁers . | Universally quantiﬁed statement $\\forall x \\in D . P(x)$ An assertion that a predicate is always true . | Existentially quantiﬁed statement $\\exists x \\in D . P(x)$ An assertion that a predicate is sometimes true . | . Order of quantiﬁers . Swapping the order of different kinds of quantiﬁers (existential or universal) usually changes the meaning of a proposition. Domain of discourse / plain domain of the formula . The unnamed nonempty set that $x$ and $y$ range over: When all the variables in a formula are understood to take values from the same nonempty set, $D$, $\\forall x \\in D \\exists y \\in D . Q(x, y)$, we’d write $\\forall x \\exists y . Q(x, y)$. Negating quantiﬁers . \\[\\operatorname{NOT}(\\exists x . P(x))\\operatorname{IFF}\\forall x . \\operatorname{NOT}(P(x))\\] Moving a “not” across a quantiﬁer changes the kind of quantiﬁer. Validity . A propositional formula is called valid when it evaluates to $\\mathbf{T}$ no matter what truth values are assigned to the individual propositional variables. Satisﬁability . A proposition is satisﬁable if some setting of the variables makes the proposition true. SAT . The general problem of deciding whether a proposition is satisﬁable is called SAT. “P vs. NP” problem . ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#logics",
    "relUrl": "/docs/Fundamental/Logic/#logics"
  },"59": {
    "doc": "Logic",
    "title": "Patterns of Proof",
    "content": "Axioms . Propositions that are simply accepted as true. Proof . A proof is a sequence of logical deductions from axioms and previously-proved statements that concludes with the proposition in question. Theorems . Important propositions. Lemma . A lemma is a preliminary proposition useful for proving later propositions. Corollary . A corollary is a proposition that follows in just a few logical steps from a lemma or a theorem. Axiomatic method . Euclid’s axiom-and-proof approach . Zermelo-Frankel Set Theory with Choice (ZFC) . | Extensionality Two sets are equal if they have the same members. | Pairing For any two sets $x$ and $y$, there is a set, ${x, y}$, with $x$ and $y$ as its only elements. | Union The union, $u$, of a collection, $z$, of sets is also a set. | Inﬁnity There is an inﬁnite set. | Subset Given any set, $x$, and any proposition $P(y)$, there is a set containing precisely those elements $y\\in x$ for which $P(y)$ holds. | Power Set All the subsets of a set form another set: $\\forall x . \\exists p . \\forall u . u \\subseteq x \\text { IFF } u \\in p$. | Replacement Suppose a formula, $\\phi$, of set theory deﬁnes the graph of a function, that is, $\\forall x, y, z \\cdot[\\phi(x, y) \\text { AND } \\phi(x, z)] \\text { IMPLIES } y=z$ . Then the image of any set, $s$, under that function is also a set, $t$. Namely, $\\forall s \\exists t \\forall y \\cdot[\\exists x . \\phi(x, y) \\text { IFF } y \\in t]$. | Foundation Every nonempty set has a “member-minimal” element. $\\operatorname{member}-\\operatorname{minimal}(m, x)::=[m \\in x \\operatorname{AND} \\forall y \\in x, y \\notin m]$ $\\forall x . x \\neq \\emptyset \\text { IMPLIES } \\exists m . \\text { member-minimal }(m, x) .$ | Choice Given a set, $s$, whose members are nonempty sets no two of which have any element in common, then there is a set, $c$, consisting of exactly one element from each set in $s$. | . Logical deductions / inference rules . Logical deductions or inference rules are used to prove new propositions using previously proved ones. modus ponens . This rule says that a proof of $P$ together with a proof that $P\\operatorname{IMPLIES}Q$ is a proof of $Q$. \\[\\frac{P, \\quad P \\text { IMPLIES } Q}{Q}\\] \\[\\frac{antecedents}{conclusion/consequent}\\] Set builder notation . To deﬁne a set using a predicate. ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#patterns-of-proof",
    "relUrl": "/docs/Fundamental/Logic/#patterns-of-proof"
  },"60": {
    "doc": "Logic",
    "title": "Induction",
    "content": "The Well Ordering Principle . Every nonempty set of nonnegative integers has a smallest element. Ordinary Induction . \\[\\frac{P(0), \\, \\forall n \\in \\mathbb{N} . P(n) \\text { IMPLIES } P(n+1)}{\\forall m \\in \\mathbb{N} . P(m)}\\] Invariants . A property that is preserved through a series of operations or steps is known as an invariant. Strong Induction . \\[\\frac{P(0), \\, \\forall n \\in \\mathbb{N} .(P(0) \\text { AND } P(1) \\text { AND ... AND } P(m)) \\text { IMPLIES } P(n+1)]}{\\forall m \\in \\mathbb{N} . P(m)}\\] Recursive data types . They are speciﬁed by recursive deﬁnitions that say how to build something from its parts. Recursive definitions . | Base case(s) that don’t depend on anything else. | Constructor case(s) that depend on previous cases. | . Structural Induction . Structural induction is a method for proving that some property, $P$, holds for all the elements of a recursively-deﬁned data type. | Prove $P$ for the base cases of the deﬁnition. | Prove $P$ for the constructor cases of the deﬁnition, assuming that it is true for the component data item | . Functions . A function assigns an element of one set, called the domain, to elements of another set, called the codomain. Ambiguity . When a recursive deﬁnition of a data type allows the same element to be constructed in more than one way, the deﬁnition is said to be ambiguous. ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#induction",
    "relUrl": "/docs/Fundamental/Logic/#induction"
  },"61": {
    "doc": "Logic",
    "title": "Number Theory",
    "content": "Number theory is the study of the integers. Divisibility . If $a | b$, then we also say that b is a multiple of a. Greatest common divisor . The largest number that is a divisor of both $a$ and $b$. Euclid’s Algorithm . \\[\\mathrm{gcd}(a,b)=\\mathrm{gcd}(b,\\mathrm{rem}(a,b,))\\] Fundamental Theorem of Arithmetic . Every positive integer n can be written in a unique way as a product of primes. The Prime Number Theorem . Let $\\pi(x)$ denote the number of primes less than or equal to $x$. \\[\\lim _{x \\rightarrow \\infty} \\frac{\\pi(x)}{x / \\ln x}=1\\] As a rule of thumb, about 1 integer out of every ln x in the vicinity of x is a prime. Fermat’s Little Theorem . Suppose $p$ is a prime and $k$ is not a multiple of $p$. Then: $k^{p-1}\\equiv 1 \\ (\\mod{p})$ . Riemann Hypothesis . Every nontrivial zero of the zeta function $\\xi(s)$ lies on the line $s=1/2+ci$ in the complex plane. ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#number-theory",
    "relUrl": "/docs/Fundamental/Logic/#number-theory"
  },"62": {
    "doc": "Logic",
    "title": "Structure",
    "content": " ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#structure",
    "relUrl": "/docs/Fundamental/Logic/#structure"
  },"63": {
    "doc": "Logic",
    "title": "Graph Theory",
    "content": " ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#graph-theory",
    "relUrl": "/docs/Fundamental/Logic/#graph-theory"
  },"64": {
    "doc": "Logic",
    "title": "Relations and Partial Orders",
    "content": "Relation . A relation is a mathematical tool for describing associations between elements of sets. Image . The image of a set $Y$ under a relation $R:A\\rightarrow B$, written $R(Y)$, is the set of elements that are related to some element in $Y$. Surjective . If every element of B is assigned to at least one element of A. Total . when every element of A is assigned to some element of B. Injective . If every element of B is mapped at most once . Bijective . If R is total, surjective, injective, and a function . ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/#relations-and-partial-orders",
    "relUrl": "/docs/Fundamental/Logic/#relations-and-partial-orders"
  },"65": {
    "doc": "Logic",
    "title": "Logic",
    "content": "Reference: . | mathematics for computer science - MIT opencourseware | | . ",
    "url": "http://localhost:4000/docs/Fundamental/Logic/",
    "relUrl": "/docs/Fundamental/Logic/"
  },"66": {
    "doc": "Machine Learning",
    "title": "Best article to understand machine learning",
    "content": "http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.html . “Machine learning is the generalization of approximation theory” . “Neural network is a nonlinear-to-linear projector” . fitting-eigen-fitter-eigen-fitting-eigen-… . the layers are multiple transformations and projections between different spaces. parameterization the transformations. Reference: . | Book: Goodfellow, Bengio, Courville (2016). Deep Learning. MIT Press. | Bishop (2006). Pattern Recognition and Machine Learning. Springer, Cambridge, UK. | . Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. Tom Mitchell (1998). Well-posed Learning Problem: A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $\\mathrm{P}$, improves with experience $E$. | Supervised learning - regression . | Unsupervised learning . | . ",
    "url": "http://localhost:4000/docs/Machine-Learning#best-article-to-understand-machine-learning",
    "relUrl": "/docs/Machine-Learning#best-article-to-understand-machine-learning"
  },"67": {
    "doc": "Machine Learning",
    "title": "Basics",
    "content": "Bias . A preference or inclination for or against something . Prejudice . An assumption made without adequate knowledge . Discrimination . The actions taken based on a prejudice . Fairness . The absence of bias or discrimination on specific realms . Linear classiﬁers . Linear hyperplanes in feature space that partition the space in (two) classes . Task $T$ . How machine process examples with features. i.e. vector (example) $\\mathbf{x}\\in\\mathbb{R}^n$ with entry (feature) $\\mathbf{x}_i$. Classification . \\[f:\\mathbb{R}^n\\rightarrow\\{1,...,k\\}\\] Classification with Missing Inputs . Regression . \\[f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\] Transcription . Transcribe unstructured information into discrete textual form. i.e. optical character recognition. Machine Translation . Convert a sequence of symbols of some language into another language. i.e. English to French. Structured Output . Output several values that are all tightly interrelated. i.e. vector, tree. Anomaly Detaction . Sifting and find. Synthesis and Sampling . Generate new similar data. Imputation of Missing Values . Example $\\mathbf{x}\\in\\mathbb{R}^n$ with some entries $\\mathbf{x}_i$ of $\\mathbf{x}$ missing. Denoising . Corrupted example $\\tilde{\\mathbf{x}}\\in\\mathbb{R}^n$ from a clean example $\\mathbf{x}\\in\\mathbb{R}^n$, predict $\\mathbf{x}$ from $\\tilde{\\mathbf{x}}$ or conditional probability distribution $P(\\mathbf{x}|\\tilde{\\mathbf{x}})$. Density Estimation / Probability Mass Function Estimation . Probability density/mass($x$ continuous/discrete) function $p_{model}:\\mathbb{R}^n\\rightarrow\\mathbb{R}$. Performance Measure $P$ . Specific to the task $T$ being carried out by the system. Accuracy . The proportion of examples for which the model produces the correct output. Error rate . The proportion of of examples for which the model produces the incorrect output. For task as Classification, Classification with missing input, and Transcription. 0-1 loss . Average Log-probability . Test set . Being separated from data to measure the performance on data that ML algorithm has not seen before. Experience $E$ . Machine learning algorithm can be categorized as unsupervised / supervised by what kind of experience they are allowed to have during the learning process. Dataset . Collections of data points (examples). Designed matrix . row - example; column - feature. Unsupervised(uninstructed) Learning Algorithm - Finding Properties . Experience a dataset containing many features, then learn useful property of the structure of the dataset. i.e. clustering. Supervised(instructed) Learning Algorithm - Associating $p(\\mathbf{y}|\\textbf{x})$ for Predicting $\\mathbf{y}$: . Experience a dataset containing structured features label/target). Simi-supervised learning algorithm . Some examples include a supervision target but others do not. Multi-instance Learning . Reinforcement Learning Algorithm . Interact with an environment but not a fixed dataset - feedback loop. ",
    "url": "http://localhost:4000/docs/Machine-Learning#basics",
    "relUrl": "/docs/Machine-Learning#basics"
  },"68": {
    "doc": "Machine Learning",
    "title": "Machine Learning",
    "content": "Machine learning is still nothing greater that Hash table. Most of the concepts are renamed from the classical concepts, which likes the result of commercial bidding, for example, the activation function is actually the system function, and hypothesis is the system response. And the name neural network is just connected function compositions, to say more rigorously, transformation network. So it is just a fancy technique which distorted some concepts in the classical mathematics. If you want establish your great building of mathematics, then do not touch machine learning first! . ",
    "url": "http://localhost:4000/docs/Machine-Learning",
    "relUrl": "/docs/Machine-Learning"
  },"69": {
    "doc": "Mechanics",
    "title": "The Equation of Motion",
    "content": "The Equation of Motion . The relations between the accelerations, velocities and co-ordinates are called the \\emph{equations of motion}. ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#the-equation-of-motion",
    "relUrl": "/docs/Fundamental/Mechanics/#the-equation-of-motion"
  },"70": {
    "doc": "Mechanics",
    "title": "Generalised Co-ordinates",
    "content": "Particle . A body whose dimensions may be neglected in describing its motion. Cartesian Co-ordinates $x,y,z$ . Radius Vector . $\\mathbf{r}=(x,y,z)$ (Cartesian co-ordinates) . Velocity $\\mathbf{v}=\\mathrm{d}\\mathbf{r}/\\mathrm{d}t=\\dot{\\mathbf{r}}$ . Acceleration $\\mathbf{a}=\\mathrm{d}^2\\mathbf{r}/\\mathrm{d}t^2=\\ddot{\\mathbf{r}}$ . The Number of Degree of Freedom . The number of independent quantities which must be specified in order to define uniquely the position of any system. Quantities $q_i$ . | Generalised co-ordinates: Any $s$ quantities $q_1,q_2,…,q_s$ which completely define the position of a system with $s$ degrees of freedom. | Generalised velocities $\\dot{q_i}$ . | . ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#generalised-co-ordinates",
    "relUrl": "/docs/Fundamental/Mechanics/#generalised-co-ordinates"
  },"71": {
    "doc": "Mechanics",
    "title": "Principle of Least Action / Hamilton’s Principle",
    "content": "Lagrangian . A definite function which characterizes a mechanical system. \\[L(q_1,q_2,...,q_s,\\dot{q}_1,\\dot{q_2},...,\\dot{q_s},t)\\quad \\mathrm{or}\\quad L(q,\\dot{q},t)\\] Action . \\[S=\\int_{t_1}^{t_2}L(q,\\dot{q},t)\\mathrm{d}t\\] Principle of Least Action . The defineing condition of the system is that action $S$ takes the least possible value. Variation of funtion $q(t)$ $\\delta q(t)$ is a function which is small everywhere in the interval of time from $t_1$ to $t_2$. \\[\\mathbf{Principle \\ of \\ Least \\ Action\\longrightarrow Lagrange's \\ Equations}\\] \\[\\delta S=0\\] Replace $q(t)$ by $q(t)+\\delta q(t)$ to get: . \\[\\delta q(t_1)=\\delta q(t_2)=0\\] \\[\\begin{aligned} \\delta S=\\delta \\int_{t_{1}}^{t_{2}} L(q, \\dot{q}, t) \\mathrm{d} t &amp;=\\int_{t_{1}}^{t_{2}}\\left(\\frac{\\partial L}{\\partial q} \\delta q+\\frac{\\partial L}{\\partial \\dot{q}} \\delta \\dot{q}\\right) \\mathrm{d} t \\\\ &amp; \\stackrel{\\delta \\dot{q}=\\mathrm{d}(\\delta q) / \\mathrm{d} t}{=} \\int_{t_{1}}^{t_{2}}\\left(\\frac{\\partial L}{\\partial q} \\delta q+\\frac{\\partial L}{\\partial \\dot{q}} \\frac{\\mathrm{d} \\delta q}{\\mathrm{~d} t}\\right) \\mathrm{d} t \\\\ &amp; \\stackrel{\\text { integrating by part of 2nd term }}{=}\\left[\\frac{\\partial L}{\\partial \\dot{q}} \\delta q\\right]_{t_{1}}^{t_{2}}+\\int_{t_{1}}^{t_{2}}\\left[\\frac{\\partial L}{\\partial q}-\\frac{\\mathrm{d}}{\\mathrm{d} t}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right)\\right] \\delta q \\mathrm{~d} t=0 \\end{aligned}\\] \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{\\partial L}{\\partial\\dot{q}}\\right)-\\frac{\\partial L}{\\partial q}=0\\] For $s$ different functions $q_i(t)$: . Lagrange’s Equations . Give the relations between accelerations. velocities and coordinates. \\[\\color{red}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{\\partial L}{\\partial\\dot{q}_i}\\right)-\\frac{\\partial L}{\\partial q_i}=0}\\quad(i=1,2,...,s)\\] Properties of Lagrangians . Additivity . \\[\\lim L=L_A+L_B\\] The Lagrangian is defined only to within $\\frac{\\mathrm{d}}{\\mathrm{d}t}f(q,t)$} . \\[L'(q,\\dot{q},t)=L(q,\\dot{q},t)+\\frac{\\mathrm{d}}{\\mathrm{d}t}f(q,t)\\] \\[S'=\\int_{t_1}^{t_2}L'(q,\\dot{q},t)\\mathrm{d}t=\\int_{t_1}^{t_2}L(q,\\dot{q},t)\\mathrm{d}t+\\int_{t_1}^{t_2}\\frac{\\mathrm{d}f}{\\mathrm{d}t}\\mathrm{d}t=S+f(q^{(2)},t_2)-f(q^{(1)},t_1)\\] \\[\\delta S'=0\\longleftrightarrow\\delta S=0\\] Then the equation of motion unchanged. ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#principle-of-least-action--hamiltons-principle",
    "relUrl": "/docs/Fundamental/Mechanics/#principle-of-least-action--hamiltons-principle"
  },"72": {
    "doc": "Mechanics",
    "title": "Galileo’s Relativity Principle",
    "content": "An infinity of inertial frames moving, relative to one another, uniformly in a straight line. In all these frames the properties of space and time are the same, and the laws of mechanics are the same. A Frame of Reference . Finding a frame of reference in which the laws of mechanics take their simplest form. Inhomogeneous and Anisotropic . Even if a body interacted with no other bodies, different \\emph{time}, its various \\emph{positions} in space and its different \\emph{orientations} would not be mechanically equivalent. Inertial Frame of Reference . A frame of reference which space is homogeneous and isotropic and time is homogeneous. \\[\\mathbf{Lagrange's \\ Equations\\longrightarrow Law \\ of \\ Inertia}\\] Lagrangian . Independent of the radius vector $\\mathbf{r}$ $(\\frac{\\partial L}{\\partial \\mathbf{r}}=0)$ and time $t$. \\[L=L(v^2)\\] Lagrange’s Equation . \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{\\partial L}{\\partial\\mathbf{v}}\\right)=0\\] Hence $\\partial L/\\partial\\mathbf{v}=\\mathrm{constant}$. Since $\\partial L/\\partial\\mathbf{v}$ is a function of the velocity $\\mathbf{v}$ only, it folows that $\\mathbf{v}=\\mathrm{constant}$. Law of Inertia . In an inertial frarne, any free motion takes place with a velocity which is constant in both magnitude and direction. Properties of Inertial Frame of Reference . No “absolute” frame preferred. Galilean Transformation . \\[\\mathrm{Frame \\ of \\ reference \\ K}: \\mathrm{Coordinate \\ r},\\mathrm{Relative \\ velocity \\ \\mathbf{0}}\\] \\[\\mathrm{Frame \\ of \\ reference \\ K'}: \\mathrm{Coordinate \\ r'},\\mathrm{Relative \\ velocity \\ \\mathbf{V}}\\] \\[\\mathbf{r}=\\mathbf{r}'+\\mathbf{V}t\\] \\[t=t'\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#galileos-relativity-principle",
    "relUrl": "/docs/Fundamental/Mechanics/#galileos-relativity-principle"
  },"73": {
    "doc": "Mechanics",
    "title": "The Lagrangian for a Free Particle",
    "content": "\\[v'=v+\\epsilon\\] \\[L(v^2)\\] \\[L'=L(v'^2)=L(v^2+2\\mathbf{v}\\cdot\\mathbf{\\epsilon}+\\epsilon^2)\\] Expanding this expression in powers of $\\epsilon$ and neglecting terms above the first order: . \\[L(v'^2)=L(v^2)+\\frac{\\partial L}{\\partial v^2}2\\mathbf{v\\cdot\\epsilon}\\] The second term on the right of this equation is a total time derivative only if it is a linear function of the velocity $\\mathbf{v}$. Hence $\\frac{\\partial L}{\\partial v^2}$ is independent of the velocity. Mass $m$ . \\[L=\\frac{1}{2}mv^2\\] \\[L'=\\frac{1}{2}mv'^2=\\frac{1}{2}mv^2+m\\mathbf{v\\cdot V}+\\frac{1}{2}mV^2=L+\\mathrm{d}(m\\mathbf{r\\cdot V}+\\frac{1}{2}mV^2t)/\\mathrm{d}t\\] The second term is a total time derivative and may be omitted. Additive Property . \\[L=\\sum\\frac{1}{2}m_av_a^2\\] Co-ordinates . \\[v^2=(\\mathrm{d}l/\\mathrm{d}t)^2=(\\mathrm{d}l)^2/(\\mathrm{d}t)^2\\] Cartesian co-ordinates . \\[\\mathrm{d}l^2=\\mathrm{d}x^2+\\mathrm{d}y^2+\\mathrm{d}z^2\\] \\[L=\\frac{1}{2}m(\\dot{x}^2+\\dot{y}^2+\\dot{z}^2)\\] Cylindrical co-ordinates . \\[\\mathrm{d}l^2=\\mathrm{d}r^2+r^2\\mathrm{d}\\phi^2+\\mathrm{d}z^2\\] \\[L=\\frac{1}{2}m(\\dot{r}^2+r^2\\dot{\\phi}^2+\\dot{z}^2)\\] Spherical co-ordinates . \\[\\mathrm{d}l^2=\\mathrm{d}r^2+r^2\\mathrm{d}\\theta^2+r^2\\sin^2\\theta\\mathrm{d}\\phi^2\\] \\[L=\\frac{1}{2}m(\\dot{r}^2+r^2\\dot{\\theta}^2+r^2\\dot{\\phi}^2\\sin^2\\theta)\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#the-lagrangian-for-a-free-particle",
    "relUrl": "/docs/Fundamental/Mechanics/#the-lagrangian-for-a-free-particle"
  },"74": {
    "doc": "Mechanics",
    "title": "The Lagrangian for a System of Particles",
    "content": "Closed System . A system of particles which interact with one another but with no other bodies. \\[L=\\sum\\frac{1}{2}m_av_a^2-U(\\mathbf{r}_1,\\mathbf{r}_2,...)\\] Kinetic energy $T=\\sum\\frac{1}{2}m_av_a^2$ . Potential energy $U(\\mathbf{r}_1,\\mathbf{r}_2,…)$ . Lagrange’s equations . \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial L}{\\partial\\mathbf{v}_a}=\\frac{\\partial L}{\\partial \\mathbf{r}_a}\\] Newton’s Equations . \\[m_a\\frac{\\mathrm{d}\\mathbf{v}_a}{\\mathrm{d}t}=-\\frac{\\partial U}{\\partial\\mathbf{r}_a}\\] The Force on the ath Particle . \\[\\mathbf{F}=-\\frac{\\partial U}{\\partial\\mathbf{r}_a}\\] Generalised co-ordinates $q_i$ . \\[x_a=f_a(q_1,q_2,...,q_s)\\] \\[\\dot{x}_a=\\sum_k\\frac{\\partial f_a}{\\partial q_k}\\dot{q}_k\\] \\[L=\\frac{1}{2}\\sum_k m_a(\\dot{x}_a^2+\\dot{y}_a^2+\\dot{z}_a^2)-U\\] \\[L=\\frac{1}{2}\\sum_{i,k}a_{ik}\\dot{q}_i\\dot{q}_k-U(q)\\] $a_{ik}$: functions of the co-ordinates only . Single Particle Moves in an External Field . \\[\\mathrm{General \\ Form \\ of \\ the \\ Lagrangian}\\quad L=\\frac{1}{2}mv^2-U(\\mathbf{r},t)\\] \\[\\mathrm{Equation \\ of \\ Motion}\\quad\\dot{\\mathbf{v}}=-\\frac{\\partial U}{\\partial\\mathbf{r}}\\] \\[\\mathrm{Potential \\ Energy}\\quad U=-\\mathbf{F\\cdot r}\\quad(\\mathrm{uniform \\ field})\\] $A$ in field $B$ . \\[L_A=T_A(q_A,\\dot{q}_A)-U[q_A,q_B(t)]\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#the-lagrangian-for-a-system-of-particles",
    "relUrl": "/docs/Fundamental/Mechanics/#the-lagrangian-for-a-system-of-particles"
  },"75": {
    "doc": "Mechanics",
    "title": "Conservation Laws (Closed System)",
    "content": " ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#conservation-laws-closed-system",
    "relUrl": "/docs/Fundamental/Mechanics/#conservation-laws-closed-system"
  },"76": {
    "doc": "Mechanics",
    "title": "Energy - Homogeneous of Time",
    "content": "Integrals of Motion . Functions of quantities in $q_i$ and $\\dot{q}_i$ whose values remain constant during the motion, and depend only on the initial conditions. | energy $E\\equiv\\sum_a\\frac{1}{2}m_av_a^2+U(\\mathbf{r}_1,\\mathbf{r}_2,…)$ . | three components of momentum $P\\equiv\\sum_a m_a\\mathbf{v}_a$ . | three components of angular momentum $\\mathbf{M}\\equiv\\sum_a\\mathbf{r}_a\\times\\mathbf{p}_a$ . | . Lagrangian of Closed System . \\[\\frac{\\mathrm{d} L}{\\mathrm{d}t}=\\sum_i\\frac{\\partial L}{\\partial q_i}\\dot{q}_i+\\sum_i\\frac{\\partial L}{\\partial \\dot{q}_i}\\ddot{q}_i\\] Replace by Lagrange’s Equations, . \\[\\frac{\\mathrm{d} L}{\\mathrm{d}t}=\\sum_i\\dot{q}_i\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{\\partial L}{\\partial\\dot{q}_i}\\right)+\\sum_i\\frac{\\partial L}{\\partial \\dot{q}_i}\\ddot{q}_i=\\sum_i\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\dot{q}_i\\frac{\\partial L}{\\partial\\dot{q}_i}\\right)\\] \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\sum_i\\dot{q}_i\\frac{\\partial L}{\\partial\\dot{q}_i}-L\\right)=0\\] \\[E \\equiv \\sum_{i} \\dot{q}_{i} \\frac{\\partial L}{\\partial \\dot{q}_{i}}-L \\stackrel{\\text { Euler'sTheorem }}{=} T(q, \\dot{q})+U(q)=\\sum_{a} \\frac{1}{2} m_{a} v_{a}^{2}+U\\left(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots\\right)\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#energy---homogeneous-of-time",
    "relUrl": "/docs/Fundamental/Mechanics/#energy---homogeneous-of-time"
  },"77": {
    "doc": "Mechanics",
    "title": "Momentum - Homogeneous of Space",
    "content": "\\[\\delta L = \\sum_a\\frac{\\partial L}{\\partial\\mathbf{r}_a}\\cdot\\delta\\mathbf{r}_a=\\epsilon\\cdot\\sum_a\\frac{\\partial L}{\\partial\\mathbf{r}_a}=0\\] \\[\\sum_a\\frac{\\partial L}{\\partial\\mathbf{r}_a}=0\\] From Lagrange’s Equations, . \\[P\\equiv\\frac{\\partial L}{\\partial \\mathbf{v}_a}=\\sum_a m_a\\mathbf{v}_a\\] Newton’s Third Law . \\[\\frac{\\partial L}{\\partial\\mathbf{r}_a}=-\\frac{\\partial U}{\\partial\\mathbf{r}_a}=\\mathbf{F}_a=0\\] Generalized Momenta . \\[p_i=\\frac{\\partial L_i}{\\partial \\dot{q}}\\] Generalized Forces . \\[F_i=\\frac{\\partial L_i}{\\partial q}\\] Lagrange’s Equations . \\[\\dot{p}_i=F_i\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#momentum---homogeneous-of-space",
    "relUrl": "/docs/Fundamental/Mechanics/#momentum---homogeneous-of-space"
  },"78": {
    "doc": "Mechanics",
    "title": "Centre of Mass",
    "content": "Two Frames of Reference $K$ and $K’$ . \\[\\mathbf{P}=\\mathbf{P}'+\\mathbf{V}\\sum_am_a\\] At Rest . $\\mathbf{P}’=0$ The total momentum of a mechanical system in a given frame of reference is zero. \\[\\mathbf{V}=\\mathbf{P}/\\sum_am_a=\\sum_am_a\\mathbf{v}_a/\\sum_am_a\\] Additivity of Mass . \\[\\mu=\\sum_am_a\\] Cetre of Mass of the System . Velocity of the system as a whole is the rate of motion in space of \\emph{the point} whose radius vector is . \\[\\mathbf{R}\\equiv\\sum_am_a\\mathbf{r}_a/\\sum_am_a\\] Internal Energy $E_i$ . The energy of a mechanical system which is at rest as a whole. This includes the kinetic energy of the relative motion of the particles in the system and the potential energy of their interaction. Two Frames of Reference $K$ and $K’$ . \\[E=E'+\\mathbf{V\\cdot P'}+\\frac{1}{2}\\mu\\mathbf{V}^2\\] If the centre of mass is at rest in $K’$, then $P’=0$, $E’=E_i$, we have . \\[\\mathrm{total \\ energy}\\quad E=\\frac{1}{2}\\mu\\mathbf{V}^2+E_i\\quad\\mathrm{System \\ moves \\ with \\ velocity \\ of \\ \\mathbf{V}}\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#centre-of-mass",
    "relUrl": "/docs/Fundamental/Mechanics/#centre-of-mass"
  },"79": {
    "doc": "Mechanics",
    "title": "Angular Momentum",
    "content": "\\[|\\delta \\mathbf{r}|=r\\sin\\theta\\delta\\phi\\] \\[\\delta \\mathbf{r}=\\delta\\mathbf{\\Phi}\\times\\mathbf{r}\\] \\[\\delta \\mathbf{v}=\\delta\\mathbf{\\Phi}\\times\\mathbf{v}\\] \\[\\delta L=\\sum_a\\left(\\frac{\\partial L}{\\partial\\mathbf{r}_a}\\cdot\\delta\\mathbf{r}_a+\\frac{\\partial L}{\\partial\\mathbf{v}_a}\\cdot\\delta\\mathbf{v}_a\\right)=\\delta\\mathbf{\\Phi}\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}\\sum_a\\mathbf{r}_a\\times\\mathbf{p}_a=0\\] Angular Momentum / Moment of Momentum . \\[\\mathbf{M}\\equiv\\sum_a\\mathbf{r}_a\\times\\mathbf{p}_a\\] Two Frames of Reference $K$ and $K’$ . \\[\\mathbf{r}_a=\\mathbf{r}_a'+\\mathbf{a}$, $\\mathbf{v}_a=\\mathbf{v}_a'+\\mathbf{V}\\] \\[\\mathbf{M}=\\mathbf{M}'+\\mathbf{a}\\times\\mathbf{P}\\] \\[\\mathbf{M}=\\mathbf{M}'+\\mu\\mathbf{R}\\times\\mathbf{V}\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#angular-momentum",
    "relUrl": "/docs/Fundamental/Mechanics/#angular-momentum"
  },"80": {
    "doc": "Mechanics",
    "title": "Mechanical Similarity",
    "content": "Homogeneous Function . $\\alpha$ is any constant and $k$ the degree of homogeneity of the function. \\[U(\\alpha\\mathbf{r}_1,\\alpha\\mathbf{r}_2,...\\alpha\\mathbf{r}_n)=\\alpha^kU(\\alpha\\mathbf{r}_1,\\alpha\\mathbf{r}_2,...\\alpha\\mathbf{r}_n)\\] Transformation . \\[\\mathbf{r}_a\\rightarrow\\alpha\\mathbf{r}_a\\] \\[t\\rightarrow\\beta t\\] \\[L=\\sum\\frac{1}{2}m_av_a^2-U(\\mathbf{r}_1,\\mathbf{r}_2,...)\\] \\[\\frac{\\alpha^2}{\\beta^2}=\\alpha^k\\] \\[\\beta=\\alpha^{1-\\frac{1}{2}k}\\] \\[\\frac{t'}{t}=\\left(\\frac{l'}{l}\\right)^{1-\\frac{1}{2}k}\\] \\[\\frac{v'}{v}=\\left(\\frac{l'}{l}\\right)^{\\frac{1}{2}k}\\] \\[\\frac{E'}{E}=\\left(\\frac{l'}{l}\\right)^k\\] \\[\\frac{M'}{M}=\\left(\\frac{l'}{l}\\right)^{1+\\frac{1}{2}k}\\] Homogeneous Function . \\[f(sx_1,...,sx_n)=s^kf(x_1,...,x_n)\\] Euler’s Homogeneous Function Theorem . If $f$ is a (partial) function of n real variables that is positively homogeneous of degree $k$, and continuously differentiable in some open subset of $\\mathbb{R}^{n}$ then it satisfies in this open set the partial differential equation . \\[kf(x_1,...,x_n)=\\sum_{i=1}^nx_i\\frac{\\partial f}{\\partial x_i}(x_1,...,x_n)\\] Virial Theorem . \\[2\\overline{T}=k\\overline{U}\\] ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#mechanical-similarity",
    "relUrl": "/docs/Fundamental/Mechanics/#mechanical-similarity"
  },"81": {
    "doc": "Mechanics",
    "title": "Integration of the Equations of Motion",
    "content": " ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#integration-of-the-equations-of-motion",
    "relUrl": "/docs/Fundamental/Mechanics/#integration-of-the-equations-of-motion"
  },"82": {
    "doc": "Mechanics",
    "title": "Motion in One Dimension",
    "content": ". V. I. Arnold . ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#motion-in-one-dimension",
    "relUrl": "/docs/Fundamental/Mechanics/#motion-in-one-dimension"
  },"83": {
    "doc": "Mechanics",
    "title": "Newtonian Mechanics",
    "content": "Newtonian mechanics studies the motion of a system of point masses in three-dimensional euclidean space. A newtonian potential mechanical system is specified by the masses of the points and by the potential energy. The motions of space which leave the potential energy invariant correspond to laws of conservation. Affine N-dimensional Space $A^n$ . ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/#newtonian-mechanics",
    "relUrl": "/docs/Fundamental/Mechanics/#newtonian-mechanics"
  },"84": {
    "doc": "Mechanics",
    "title": "Mechanics",
    "content": ". Landau . ",
    "url": "http://localhost:4000/docs/Fundamental/Mechanics/",
    "relUrl": "/docs/Fundamental/Mechanics/"
  },"85": {
    "doc": "Norms",
    "title": "Norms",
    "content": "Norm . A function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ with $\\mathbf{d o m} f=\\mathbf{R}^{n}$ is called a norm . \\[\\|x\\|_{\\mathrm{symb}}\\] if . | $f$ is nonnegative: $f(x) \\geq 0$ for all $x \\in \\mathbf{R}^{n}$ | $f$ is definite: $f(x)=0$ only if $x=0$ | $f$ is homogeneous: $f(t x)=|t|f(x)$, for all $x \\in \\mathbf{R}^{n}$ and $t \\in \\mathbf{R}$ | $f$ satisfies the triangle inequality: $f(x+y) \\leq f(x)+f(y)$, for all $x, y \\in \\mathbf{R}^{n}$ | . A norm is a generalization of the absolute value on $\\mathbf{R}$. ",
    "url": "http://localhost:4000/docs/Fundamental/Norms/",
    "relUrl": "/docs/Fundamental/Norms/"
  },"86": {
    "doc": "Numerical",
    "title": "Numerical",
    "content": "partial pivoting . Small pivot brings large round off error . Choose the largest number in row $k$ or below. Exchange its row with row $k$. partial pivoting exchange rows, complete pivoting exchange both rows and columns. Elimination . | Elimination requires $\\frac{1}{2}n^3$ multiply-subtracts, compared to $n^3$ for $A^{-1}$• | If $A$ is banded so are $L$ and $U$: by comparison $A^{- 1}$ is full of nonzeros. | . Band Matrices . A band matrix has $a_{ij} = 0$ when $|i - j |&gt; w$. Elimination . Matrix norm . condition number . \\[c=\\frac{\\|A\\|}{|]A^{-1}\\|}\\] . iterative problem . \\[A=S-T\\] . residual . \\[r_k = b - Ax_k\\] preconditioner $S$ . diagonal or trianular part of $A$, many zeros. error . \\[e_k=x-x_k\\] . spectral radius . \\[B=S^{-1}T\\] . Jacobi’s method . Keep the diagonal of $A$ on the left side (this is $S$). Move the off-diagonal part of $A$ to the right side (this is $T$). Jacobi’s method works well when the main diagonal of $A$ is large compared to the off-diagonal part. (diagonal dominate) . Gauss-Seidel method . Keeps the whole lower triangular part of $A$ as $S$. no $u_k$, save storage of data. Twice faster than Jacobi. \\[\\rho_\\text{GS}=(\\rho_\\text{J})^2 \\ \\text{when A is positive definite tridiagonal}\\] successive overrelaxation method (SOR) . Gauss-Seidel, with an adjustable number $\\omega$ . \\[\\omega Ax=\\omega b\\] \\[T=S-\\omega A\\] . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Num/",
    "relUrl": "/docs/Linear-Algebra/Num/"
  },"87": {
    "doc": "Orthogonality",
    "title": "Fundamental Theorem of Linear Algebra. Pt.2",
    "content": "$\\boldsymbol{N}(\\boldsymbol{A})$ is the orthogonal complement of the row space $\\boldsymbol{C}\\left(A^{\\mathrm{T}}\\right)$ (in $\\mathbf{R}^{n}$ ). $\\boldsymbol{N}\\left(A^{\\mathrm{T}}\\right)$ is the orthogonal complement of the column space $C(A)\\left(\\right.$ in $\\left.\\mathbf{R}^{m}\\right) .$ . After the transformation, the information of nullspace is annihilated. For example after the transformation of first differentiation, the information of position has dissappeared, then second differentiation crash the information of velocity, third accelaration… . And if we do integration, we need to add the constant C, that integration process is the back-transformation of differentiation, which bring back to the general null space - C position, C velocity, C acceleration… . Remember the particular solution of integration, it is the information brought back by the transformation feature of the system from the “b”; the general solution is the information of nullspace brought back by the annihilation feature of the system from 0! . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#fundamental-theorem-of-linear-algebra-pt2",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#fundamental-theorem-of-linear-algebra-pt2"
  },"88": {
    "doc": "Orthogonality",
    "title": "Projection",
    "content": "Projection Matrix $P$ . \\[\\boldsymbol{p}_{1}=P_{1} \\boldsymbol{b}=\\left[\\begin{array}{lll} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]\\left[\\begin{array}{l} x \\\\ y \\\\ z \\end{array}\\right]=\\left[\\begin{array}{l} \\mathbf{0} \\\\ \\mathbf{0} \\\\ \\boldsymbol{z} \\end{array}\\right] \\quad \\boldsymbol{p}_{2}=P_{2} \\boldsymbol{b}=\\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right]\\left[\\begin{array}{l} x \\\\ y \\\\ z \\end{array}\\right]=\\left[\\begin{array}{l} \\boldsymbol{x} \\\\ \\boldsymbol{y} \\\\ \\mathbf{0} \\end{array}\\right]\\] . The vectors give $\\boldsymbol{p}{1}+\\boldsymbol{p}{2}=\\boldsymbol{b}$. The matrices give $P_{1}+P_{2}=I$. The vector $b$ is being split into the projection $p$ and the error $e = b- p$. \\[P \\boldsymbol{b}=\\boldsymbol{p}\\] Projection again is still the projection: . \\[P^{2}=P\\] \\[P^{\\mathrm{T}}=P\\] Error $e$ . The distance from $b$ to the the subspace $C(A)$ is $||e||$ . \\[e=b-\\widehat{x} a\\] best/closest choice $\\widehat{x}$ . \\[\\begin{gathered} \\boldsymbol{a}_{1}^{\\mathrm{T}}(\\boldsymbol{b}-A \\widehat{\\boldsymbol{x}})=0 \\\\ \\vdots \\\\ \\boldsymbol{a}_{n}^{\\mathrm{T}}(\\boldsymbol{b}-A \\widehat{\\boldsymbol{x}})=0 \\end{gathered} \\quad \\text { or } \\quad\\left[\\begin{array}{c} -\\boldsymbol{a}_{1}^{\\mathrm{T}}- \\\\ \\vdots \\\\ -\\boldsymbol{a}_{n}^{\\mathrm{T}}- \\end{array}\\right][\\boldsymbol{b}-A \\widehat{\\boldsymbol{x}}]=\\left[\\begin{array}{l} 0 \\end{array}\\right]\\] Rewrite above (orthogonality): . \\[A^{\\mathrm{T}}(\\boldsymbol{b}-A \\widehat{\\boldsymbol{x}})=\\mathbf{0}\\] \\[A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b}\\] . When $P$ projects onto one subspace, $I - P$ projects onto the perpendicular subspace. 3 steps to find $P$ . | Find the vector $\\widehat{x}$ . | Find the projection $p=A \\widehat{x}$ | Find the projection matrix $P$ | . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#projection",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#projection"
  },"89": {
    "doc": "Orthogonality",
    "title": "Least Squares Approximations",
    "content": "When the length of $\\boldsymbol{e}$ is as small as possible, $\\widehat{\\boldsymbol{x}}$ is $a$ least squares solution. Minimizing the error . | By geometry . The smallest possible error is $e = b - p$, perpendicular to the columns space of $A$. | By algebra . \\[A \\boldsymbol{x}=\\boldsymbol{b}=\\boldsymbol{p}+\\boldsymbol{e} \\text { is impossible } \\quad A \\widehat{\\boldsymbol{x}}=\\boldsymbol{p} \\text { is solvable } \\quad \\widehat{\\boldsymbol{x}} \\text { is }\\left(A^{\\mathrm{T}} A\\right)^{-1} A^{\\mathrm{T}} \\boldsymbol{b} \\text {. }\\] Squared length for any $x$ . \\[\\|A \\boldsymbol{x}-\\boldsymbol{b}\\|^2=\\|A \\boldsymbol{x}-\\boldsymbol{p}\\|^2+\\|\\boldsymbol{e}\\|^2\\] \\[\\text { The least squares solution } \\widehat{x} \\text { makes } E=\\|A x-b\\|^{2} \\text { as small as possible. }\\] \\[\\|A \\boldsymbol{x}-\\boldsymbol{b}\\|^{2}=\\boldsymbol{x}^{\\mathrm{T}} A^{\\mathrm{T}} A \\boldsymbol{x}-2 \\boldsymbol{x}^{\\mathrm{T}} A^{\\mathrm{T}} \\boldsymbol{b}+\\boldsymbol{b}^{\\mathrm{T}} \\boldsymbol{b}\\] | By calculus . \\[\\text { The partial derivatives of }\\|A x-b\\|^{2} \\text { are zeno when } A^{\\mathrm{T}} A \\hat{x}=A^{\\mathrm{T}} b \\text {. } \\\\\\] | . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#least-squares-approximations",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#least-squares-approximations"
  },"90": {
    "doc": "Orthogonality",
    "title": "Fitting a Straight Line",
    "content": "\\[\\begin{equation} A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\end{equation}\\] \\[\\begin{equation} \\text { Dot-product matrix } \\boldsymbol{A}^{\\mathbf{T}} \\boldsymbol{A}=\\left[\\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ t_{1} &amp; \\cdots &amp; t_{m} \\end{array}\\right]\\left[\\begin{array}{cc} 1 &amp; t_{1} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; t_{m} \\end{array}\\right]=\\left[\\begin{array}{cc} m &amp; \\sum t_{i} \\\\ \\sum t_{i} &amp; \\sum t_{i}^{2} \\end{array}\\right] \\text {. } \\end{equation}\\] \\[\\begin{equation} A^{\\mathrm{T}} \\boldsymbol{b}=\\left[\\begin{array}{ccc} 1 &amp; \\cdots &amp; 1 \\\\ t_{1} &amp; \\cdots &amp; t_{m} \\end{array}\\right]\\left[\\begin{array}{c} b_{1} \\\\ \\vdots \\\\ b_{m} \\end{array}\\right]=\\left[\\begin{array}{c} \\sum b_{i} \\\\ \\sum t_{i} b_{i} \\end{array}\\right] \\end{equation}\\] \\[\\begin{equation} A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\quad\\left[\\begin{array}{cc} m &amp; \\sum t_{i} \\\\ \\sum t_{i} &amp; \\sum t_{i}^{2} \\end{array}\\right]\\left[\\begin{array}{l} C \\\\ D \\end{array}\\right]=\\left[\\begin{array}{c} \\sum b_{i} \\\\ \\sum t_{i} b_{i} \\end{array}\\right] \\end{equation}\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#fitting-a-straight-line",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#fitting-a-straight-line"
  },"91": {
    "doc": "Orthogonality",
    "title": "Fitting by a Parabola",
    "content": "\\[\\begin{equation} A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\end{equation}\\] \\[\\begin{array}{cl}C+D t_{1}+E t_{1}^{2}=b_{1} &amp; \\text { is } A \\boldsymbol{x}=\\boldsymbol{b} \\text { with } \\\\ \\vdots &amp; \\text { the } m \\text { by } 3 \\text { matrix } \\\\ C+D t_{m}+E t_{m}^{2}=b_{m} &amp; \\end{array} A=\\left[\\begin{array}{ccc}1 &amp; t_{1} &amp; t_{1}^{2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; t_{m} &amp; t_{m}^{2}\\end{array}\\right]\\] Least squares: The closest parabola $C+D t+E t^{2}$ chooses $\\widehat{\\boldsymbol{x}}=(C, D, E)$ to satisfy the three normal equations $A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b}$. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#fitting-by-a-parabola",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#fitting-by-a-parabola"
  },"92": {
    "doc": "Orthogonality",
    "title": "Q replace A",
    "content": "\\[\\begin{equation} A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\end{equation}\\] \\[A^{\\mathrm{T}} A=Q^{\\mathrm{T}} Q=I\\] \\[\\widehat{\\boldsymbol{x}}=Q^{\\mathrm{T}} \\boldsymbol{b}\\] \\[P=A\\left(A^{\\mathrm{T}} A\\right)^{-1} A^{\\mathrm{T}}=Q Q^{\\mathrm{T}}\\] When $Q$ is square and $m=n$. In this case $\\boldsymbol{p}=\\boldsymbol{b}$ and $P=Q Q^{\\mathrm{T}}=I$. \\[\\boldsymbol{p}=Q \\widehat{\\boldsymbol{x}}=Q Q^{\\mathrm{T}} \\boldsymbol{b}\\] Every $\\boldsymbol{b}=Q Q^{\\mathrm{T}} \\boldsymbol{b}$ is the sum of its components along the q’s: . \\[\\boldsymbol{b}=\\boldsymbol{q}_{1}\\left(\\boldsymbol{q}_{1}^{\\mathrm{T}} \\boldsymbol{b}\\right)+\\boldsymbol{q}_{2}\\left(\\boldsymbol{q}_{2}^{\\mathrm{T}} \\boldsymbol{b}\\right)+\\cdots+\\boldsymbol{q}_{n}\\left(\\boldsymbol{q}_{n}^{\\mathrm{T}} \\boldsymbol{b}\\right)\\] Then $b$ is decomposited into the composition of different orthogonal basis. Transforms: $Q Q^{\\mathrm{T}}=I$ is the foundation of Fourier series and all the great “transforms” of applied mathematics. They break vectors $\\boldsymbol{b}$ or functions $f(x)$ into perpendicular pieces. Then by adding the pieces, the inverse transform puts $b$ and $f(x)$ back together. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#q-replace-a",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#q-replace-a"
  },"93": {
    "doc": "Orthogonality",
    "title": "Use QR decomposition",
    "content": "\\[\\begin{equation} A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\end{equation}\\] \\[\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}=(\\boldsymbol{Q} \\boldsymbol{R})^{\\mathrm{T}} \\boldsymbol{Q} R=\\boldsymbol{R}^{\\mathrm{T}} \\boldsymbol{Q}^{\\mathrm{T}} \\boldsymbol{Q} \\boldsymbol{R}=\\boldsymbol{R}^{\\mathrm{T}} \\boldsymbol{R}\\] \\[R^{\\mathrm{T}} R \\widehat{\\boldsymbol{x}}=R^{\\mathrm{T}} Q^{\\mathrm{T}} \\boldsymbol{b} \\text { or } R \\widehat{\\boldsymbol{x}}=Q^{\\mathrm{T}} \\boldsymbol{b} \\text { or } \\widehat{\\boldsymbol{x}}=R^{-1} Q^{\\mathrm{T}} \\boldsymbol{b}\\] We can solve it by backward substitution. The cost is the $mn^2$ multiplications in the Gram-Schmidt process, which are needed to construct the orthogonal $Q$ and the triangular $R$ with $A = QR$. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#use-qr-decomposition",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#use-qr-decomposition"
  },"94": {
    "doc": "Orthogonality",
    "title": "The Gram-Schmidt Process",
    "content": "One and only one idea: Subtract from every new vector its projections in the directions already set. Producing unit orthogonal vectors from existing vectors. | choosing $A=a$ . | Start with $b$ and subtract its projection along $A$ . | . $\\boldsymbol{B}=\\boldsymbol{b}-\\frac{\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{b}}{\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}} \\boldsymbol{A}$ . | same process with $c, d, e,\\ldots$ | . \\[\\boldsymbol{C}=\\boldsymbol{c}-\\frac{\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{c}}{\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}} \\boldsymbol{A}-\\frac{\\boldsymbol{B}^{\\mathrm{T}} \\boldsymbol{c}}{\\boldsymbol{B}^{\\mathrm{T}} \\boldsymbol{B}} \\boldsymbol{B}\\] Final step. divide the orthogonal vectors by their lengths. $A=QR$ . Gram-Schmidt in a nutshell . \\[A=Q R=(\\text { orthogonal } Q)(\\text { triangular } R)\\] \\[\\left[\\begin{array}{lll}a &amp; b &amp; c\\end{array}\\right]=\\left[\\begin{array}{lll} &amp; &amp; \\\\ \\boldsymbol{q}_{1} &amp; \\boldsymbol{q}_{2} &amp; \\boldsymbol{q}_{3}\\\\ &amp; &amp; \\end{array}\\right]\\left[\\begin{array}{lll}\\boldsymbol{q}_{1}^{\\mathrm{T}} \\boldsymbol{a} &amp; \\boldsymbol{q}_{1}^{\\mathrm{T}} \\boldsymbol{b} &amp; \\boldsymbol{q}_{1}^{\\mathrm{T}} \\boldsymbol{c} \\\\ &amp; \\boldsymbol{q}_{2}^{\\mathrm{T}} \\boldsymbol{b} &amp; \\boldsymbol{q}_{2}^{\\mathrm{T}} \\boldsymbol{c} \\\\ &amp; &amp; \\boldsymbol{q}_{3}^{\\mathrm{T}} \\boldsymbol{c}\\end{array}\\right] \\quad \\text{or} \\quad \\boldsymbol{A}=\\boldsymbol{Q} \\boldsymbol{R}\\] Any $m$ by $n$ matrix $A$ with independent columns can be factored into $A = QR$. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#the-gram-schmidt-process",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#the-gram-schmidt-process"
  },"95": {
    "doc": "Orthogonality",
    "title": "Gram-Schmidt",
    "content": "From independent vectors $a_{1}, \\ldots, a_{n}$, Gram-Schmidt constructs orthonormal vectors $q_{1}, \\ldots, q_{n}$. The matrices with these columns satisfy $A=Q R$. Then $R=Q^{\\mathrm{T}} A$ is upper triangular because later $\\boldsymbol{q}$ ‘s are orthogonal to earlier $\\boldsymbol{a}$ ‘s. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/#gram-schmidt",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/#gram-schmidt"
  },"96": {
    "doc": "Orthogonality",
    "title": "Orthogonality",
    "content": "Orthogonal vectors . \\[\\boldsymbol{v}^{\\mathrm{T}} \\boldsymbol{w}=0 \\quad \\text { and } \\quad\\|\\boldsymbol{v}\\|^{2}+\\|\\boldsymbol{w}\\|^{2}=\\|\\boldsymbol{v}+\\boldsymbol{w}\\|^{2}\\] Orthogonal . Definition: Two subspaces $\\boldsymbol{V}$ and $\\boldsymbol{W}$ of a vector space are orthogonal if every vector $\\boldsymbol{v}$ in $V$ is perpendicular to every vector $w$ in $W$. The unit vectors $q_{1}, \\ldots, q_{n}$ are orthonormal if . \\[\\boldsymbol{q}_{i}^{\\mathrm{T}} \\boldsymbol{q}_{j}=\\left\\{\\begin{array}{lll} 0 &amp; \\text { when } i \\neq j &amp; \\text { (orthogonal vectors) } \\\\ 1 &amp; \\text { when } i=j &amp; \\left(\\text { unit vectors: }\\left\\|\\boldsymbol{q}_{i}\\right\\|=1\\right) \\end{array}\\right.\\] A matrix with unit orthonormal columns is assigned the special letter $Q$. \\[\\boldsymbol{Q}^{\\mathrm{T}} \\boldsymbol{Q}=\\left[\\begin{array}{c}-\\boldsymbol{q}_{1}^{\\mathrm{T}}- \\\\ -\\boldsymbol{q}_{2}^{\\mathrm{T}}- \\\\ -\\boldsymbol{q}_{n}^{\\mathrm{T}}-\\end{array}\\right]\\left[\\begin{array}{ccc}1 &amp; \\mid &amp; 1 \\\\ \\boldsymbol{q}_{1} &amp; \\boldsymbol{q}_{2} &amp; \\boldsymbol{q}_{n} \\\\ 1 &amp; \\mid &amp; 1\\end{array}\\right]=\\left[\\begin{array}{cccc}1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1\\end{array}\\right]=\\boldsymbol{I}\\] \\[\\boldsymbol{v}^{\\mathrm{T}} \\boldsymbol{w}=0 \\text { for all } v \\text { in } \\boldsymbol{V} \\text { and all } \\boldsymbol{w} \\text { in } W\\] . Orthogonal Complements . Definition:The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^{\\perp}$ (pronounced “ $V$ perp”). Orthogonal matrix . When $Q$ is square, $Q^{\\mathrm{T}} Q=I$ means that $Q^{\\mathrm{T}}=Q^{-1}$ : transpose $=$ inverse, the square matrix is called orthogonal matrix. Does multiplication by any orthogonal matrix $Q$ - lengths and angles don’t change. Rotation . $Q$ rotates every vector in the plane by the angle $\\theta$ : . $Q=\\left[\\begin{array}{rr}\\cos \\theta &amp; -\\sin \\theta \\ \\sin \\theta &amp; \\cos \\theta\\end{array}\\right] \\quad$ and $\\quad Q^{\\mathrm{T}}=Q^{-1}=\\left[\\begin{array}{rr}\\cos \\theta &amp; \\sin \\theta \\ -\\sin \\theta &amp; \\cos \\theta\\end{array}\\right]$ . Permutation . Every permutation matrix is an orthogonal matrix. Reflection . If $\\boldsymbol{u}$ is any unit vector, set $Q=I-2 u u^{\\mathrm{T}}$ . $Q^{\\mathrm{T}}=I-2 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}=Q \\quad$ and $\\quad Q^{\\mathrm{T}} Q=I-4 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}+4 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}} \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}=I$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Orthogonality/",
    "relUrl": "/docs/Linear-Algebra/Orthogonality/"
  },"97": {
    "doc": "Probability & Statistics",
    "title": "Continuous",
    "content": ". Uniform distribution . Normal distribution . . Monte Carlo Estimation Methods . accepting uncertainty in the inputs and estimating the variance in the outputs. try different inputs $b$ and compute the outputs $x$ and take an average. Monte Carlo approximates an expected value $E[x]$ by a sample average $(x_1 + · · · +x_N )/ N$. Covariance . Diagonalizing the covariance matrix means finding $M$ independent experiments as combinations of the original $M$ experiments. “whitening” the noise: . Kalman Filter . dynamic: new measurements $b_k$ keep coming. matrix $A$ is also changing. recursive least squares: adding new data $b_k$ and updating both $\\hat{x}$ and $W$. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/PS/#continuous",
    "relUrl": "/docs/Linear-Algebra/PS/#continuous"
  },"98": {
    "doc": "Probability & Statistics",
    "title": "Probability & Statistics",
    "content": "mean . the average value or expected value . expected value . variance $\\sigma^2$ . measures the average squared distance from the mean $m$ . measures expected distance (squared) from the expected mean $E[x]$. sample variance $S^2$ . measures actual distance (squared) from the sample mean. standard deviation $\\sigma$ or $S$ . probabilities of $n$ different outcomes . positive numbers $p_1, … , P_n$ adding to 1. sample values, expected values . Law of Large Numbers . With probability 1, the sample mean will converge to its expected value $E[x]$ as the sample size $N$ increases. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/PS/",
    "relUrl": "/docs/Linear-Algebra/PS/"
  },"99": {
    "doc": "SVD",
    "title": "Principal Component Analysis (singular value = variance)",
    "content": "sample covariance matrix . The covariance matrix predicts the spread of future measurements around their mean. Find the average (the mean) $\\mu_{1}, \\mu_{2}, \\ldots, \\mu_{m}$ of each row. Subtract each mean $\\mu_{i}$ from row $i$ to center the data. \\[S=\\frac{A A^{\\mathrm{T}}}{n-1}\\] $A$ shows the distance $a_{i j}-\\mu_{i}$ from each measurement to the row average $\\mu_{i}$. \\[\\left(A A^{\\mathrm{T}}\\right)_{11}, \\left(A A^{\\mathrm{T}}\\right)_{22}\\] show the sum of squared distances (sample variances $s_1^2, s_2^2$ ). $\\left(A A^{\\mathrm{T}}\\right)_{12}$ shows the sample covariance: . \\[s_{12}=(\\operatorname{row} 1 \\text { of } A) \\cdot(\\operatorname{row} 2 \\text { of } A)\\] one degree of freedom was used by the mean so $n-1$ . Since the rows of $A$ have $n$ entries, the numbers in $A A^{\\mathrm{T}}$ have size growing like $n$ and the division by $n-1$ keeps them steady. | The total variance in the data is the sum of all eigenvalues and of sample variances $s^{2}$ : Total variance $T=\\sigma_1^2+\\cdots+\\sigma_m^2=s_1^2+\\cdots+s_m^2=$ trace (diagonal sum). | The first eigenvector $\\boldsymbol{u}_1$ of $S$ points in the most significant direction of the data. That direction accounts for (or explains) a fraction $\\sigma_1^2 / T$ of the total variance. | The next eigenvector $\\boldsymbol{u}_2$ (orthogonal to $\\boldsymbol{u}_1$ ) accounts for a smaller fraction $\\sigma_2^2 / T$. | Stop when those fractions are small. You have the $R$ directions that explain most of the data. The $n$ data points are very near an $R$-dimensional subspace with basis $\\boldsymbol{u}_1$ to $\\boldsymbol{u}_R$. These $\\boldsymbol{u}$ ‘s are the principal components in $m$-dimensional space. | $R$ is the “effective rank” of $A$. The true rank $r$ is probably $m$ or $n$ : full rank matrix. | . Perpendicular Least Squares / orthogonal regression . \\[\\text { Right triangles } \\sum_{j=1}^{n}\\left\\|\\boldsymbol{a}_{j}\\right\\|^{2}=\\sum_{j=1}^{n}\\left|\\boldsymbol{a}_{j}^{\\mathrm{T}} \\boldsymbol{u}_{1}\\right|^{2}+\\sum_{j=1}^{n}\\left|\\boldsymbol{a}_{j}^{\\mathrm{T}} \\boldsymbol{u}_{2}\\right|^{2}\\] RHS is fixed, choosing maximum $u_1$ term will result in minimum $u_2$ term. Sample Correlation Matrix (nomalized) . A diagonal matrix $D$ rescales $A$. Each row of $D A$ has length $\\sqrt{n-1}$. The sample correlation matrix $C=D A A^{\\mathrm{T}} D /(n-1)$ has $1$ ‘s on its diagonal. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/SVD/#principal-component-analysis-singular-value--variance",
    "relUrl": "/docs/Linear-Algebra/SVD/#principal-component-analysis-singular-value--variance"
  },"100": {
    "doc": "SVD",
    "title": "Model Order Reduction",
    "content": "A reduced model tries to identify important states of the system. Dynamic . “Dynamic” means that the solution $u ( t )$ evolves as time goes forward. snapshot . A snapshot is a column vector that describes the state of the system . It can be an approximation to a typical true state $\\boldsymbol{u}\\left(t^{*}\\right)$ . From $n$ snapshots, build a matrix $A$ whose columns span a useful range of states . Proper Orthogonal Decomposition . The first $R$ left singular vectors $\\boldsymbol{u}_1$ to $\\boldsymbol{u}_R$ of snapshot $A$ are a basis for POD. Variance $\\approx$ Energy . $\\sigma_{1}^{2}+\\cdots+\\sigma_{R}^{2}$ is $99 \\%$ or $99.9 \\%$ of $\\sigma_{1}^{2}+\\cdots+\\sigma_{n}^{2}$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/SVD/#model-order-reduction",
    "relUrl": "/docs/Linear-Algebra/SVD/#model-order-reduction"
  },"101": {
    "doc": "SVD",
    "title": "The Geometry of the SVD",
    "content": "\\[(\\text { orthogonal }) \\times(\\text { diagonal }) \\times(\\text { orthogonal })\\] \\[(\\text { rotation }) \\times(\\text { stretching }) \\times(\\text { rotation })\\] \\[A \\boldsymbol{x}=U \\Sigma V^{\\mathrm{T}} \\boldsymbol{x}\\] . The Norm of a Matrix . largest singular value $\\sigma_{1}$ . \\[\\text { The norm }\\|A\\| \\text { is the largest ratio } \\frac{\\|A x\\|}{\\|x\\|} \\quad\\|A\\|=\\max _{x \\neq 0} \\frac{\\|A x\\|}{\\|x\\|}=\\sigma_{1}\\] Eckart-Young-Mirsky Theorem . \\[\\text { The closest rank } \\boldsymbol{k} \\text { matrix to } \\boldsymbol{A} \\text { is } \\boldsymbol{A}_{\\boldsymbol{k}}=\\sigma_{1} \\boldsymbol{u}_{1} \\boldsymbol{v}_{1}^{\\mathrm{T}}+\\cdots+\\sigma_{k} \\boldsymbol{u}_{k} \\boldsymbol{v}_{k}^{\\mathrm{T}}\\] \\[\\|A-B\\| \\geq\\left\\|A-A_{k}\\right\\|=\\sigma_{k+1} \\text { for all matrices } B \\text { of rank } k \\text {. }\\] Polar Decomposition $A=QS$ . Every real square matrix can be factored into $A=Q S$, where $Q$ is orthogonal and $S$ is symmetric positive semidefinite. If $A$ is invertible, $S$ is positive definite. \\[A=U \\Sigma V^{\\mathrm{T}}=\\left(U V^{\\mathrm{T}}\\right)\\left(V \\Sigma V^{\\mathrm{T}}\\right)=(Q)(S)\\] \\[A=U \\Sigma V^{\\mathrm{T}}=\\left(U \\Sigma U^{\\mathrm{T}}\\right)\\left(U V^{\\mathrm{T}}\\right)=(K)(Q)\\] The product of orthogonal matrices is orthogonal. In mechanics, the polar decomposition separates the rotation (in $Q$ ) from the stretching (in $S$ ). The eigenvalues of $S$ give the stretching factors. The eigenvectors of $S$ give the stretching directions (the principal axes of the ellipse). The orthogonal matrix $Q$ includes both rotations $U$ and $V^{\\mathrm{T}}$. Here is a fact about rotations. $Q=U V^{\\mathrm{T}}$ is the nearest orthogonal matrix to $A$. This $Q$ makes the norm $|Q-A|$ as small as possible. That corresponds to the fact that $e^{i \\theta}$ is the nearest number on the unit circle to $r e^{i \\theta}$. \\[\\text { The nearest singular matrix } A_{0} \\text { to } A \\text { comes by changing the smallest } \\sigma_{\\min } \\text { to zero. }\\] So $\\sigma_{\\min }$ is measuring the distance from $A$ to singularity. Pseudoinverse $A^{+}$ . \\[A^{+} \\boldsymbol{u}_{i}=\\frac{1}{\\sigma_{i}} \\boldsymbol{v}_{i} \\quad \\text { for } i \\leq r \\quad \\text { and } \\quad A^{+} \\boldsymbol{u}_{i}=\\mathbf{0} \\text { for } i&gt;r\\] . \\[\\boldsymbol{x}^{+}=\\boldsymbol{A}^{+} \\boldsymbol{b} \\text { is the shortest solution to } A^{\\mathrm{T}} A \\widehat{\\boldsymbol{x}}=A^{\\mathrm{T}} \\boldsymbol{b} \\text { and } A \\widehat{\\boldsymbol{x}}=\\boldsymbol{p}\\] The least square approximation decomposite the vectors, psudoinverse faked a matrix that can operate like a real inverse matrix. Principally, the inverse will bring the left null space to a certain space, but the psudoinverse brings it to zero. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/SVD/#the-geometry-of-the-svd",
    "relUrl": "/docs/Linear-Algebra/SVD/#the-geometry-of-the-svd"
  },"102": {
    "doc": "SVD",
    "title": "SVD",
    "content": "The singular value theorem for $A$ is the eigenvalue theorem for $A^{\\mathrm{T}} A$ and $A A^{\\mathrm{T}}$. \\[\\text { Use the eigenvectors } u \\text { of } A A^{\\mathrm{T}} \\text { and the eigenvectors } v \\text { of } A^{\\mathrm{T}} A\\] left singular vectors $u$’s . the principal components . \\[\\text { unit eigenvectors of } A A^{\\mathrm{T}}\\] Right singular vectors $v$’s . the combination if principal components . \\[\\text { unit eigenvectors of } A^{\\mathrm{T}} A\\] \\[A A^{\\mathrm{T}} \\boldsymbol{u}_{i}=\\sigma_{i}^{2} \\boldsymbol{u}_{i} \\quad A^{\\mathrm{T}} A \\boldsymbol{v}_{i}=\\sigma_{i}^{2} \\boldsymbol{v}_{i} \\quad A \\boldsymbol{v}_{i}=\\sigma_{i} \\boldsymbol{u}_{i}\\] $\\sigma_{i}$is the length of $A \\boldsymbol{v}_{i}$ . A is diagonalized: . \\[A =\\sum_i\\boldsymbol{u}_{i}\\sigma_{i} \\boldsymbol{v}_{i}^\\mathrm{T}\\] . \\[\\sigma_{1} \\geq \\sigma_{2} \\geq \\ldots \\sigma_{r}&gt;0\\] include all the $v$’s and $u$’s: . \\[A=U \\boldsymbol{\\Sigma} V^{\\mathbf{T}}=u_{1} \\sigma_{1} v_{1}^{\\mathbf{T}}+\\cdots+u_{r} \\sigma_{r} v_{r}^{\\mathbf{T}}\\] . If $A$ is positive semidefinite (or definite) symmetric matrix, . \\[A=X \\Lambda X^{-1}=Q \\Lambda Q^{\\mathrm{T}}=U \\Sigma V^{\\mathrm{T}}\\] \\[\\begin{array}{ll} \\text { Symmetric } \\boldsymbol{S} &amp; S=Q \\Lambda Q^{\\mathrm{T}}=\\lambda_{1} \\boldsymbol{q}_{1} \\boldsymbol{q}_{1}^{\\mathrm{T}}+\\lambda_{2} \\boldsymbol{q}_{2} \\boldsymbol{q}_{2}^{\\mathrm{T}}+\\cdots+\\lambda_{r} \\boldsymbol{q}_{r} \\boldsymbol{q}_{r}^{\\mathrm{T}} \\\\ \\text { Any matrix } \\boldsymbol{A} &amp; A=U \\Sigma V^{\\mathrm{T}}=\\sigma_{1} \\boldsymbol{u}_{1} \\boldsymbol{v}_{1}^{\\mathrm{T}}+\\sigma_{2} \\boldsymbol{u}_{2} \\boldsymbol{v}_{2}^{\\mathrm{T}}+\\cdots+\\sigma_{r} \\boldsymbol{u}_{r} \\boldsymbol{v}_{r}^{\\mathrm{T}} \\end{array}\\] normal matrix . \\[A^{\\mathrm{T}} A=A A^{\\mathrm{T}}\\] the eigenvectors of $A$ are orthogonal and the eigenvalues of $A$ are totally stable. but the singular values of any matrix are stable. Rayleigh quotient . \\[r(\\boldsymbol{x})=\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x} / \\boldsymbol{x}^{\\mathrm{T}} \\boldsymbol{x}\\] The derivatives of $r(x)=\\frac{x^{\\mathrm{T}} S x}{x^{\\mathrm{T}} x}$ are zero when $S \\boldsymbol{x}=r(\\boldsymbol{x}) \\boldsymbol{x}$ . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/SVD/",
    "relUrl": "/docs/Linear-Algebra/SVD/"
  },"103": {
    "doc": "Signals and Systems",
    "title": "Signals and Systems",
    "content": "Reference: Oppenheim, Alan V., and A. S. Willsky. Signals and Systems. Prentice Hall, 1982. ISBN: 9780138097318. ",
    "url": "http://localhost:4000/docs/Signals-Systems",
    "relUrl": "/docs/Signals-Systems"
  },"104": {
    "doc": "Spaces",
    "title": "Transfroming things between subspaces!",
    "content": ". ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Spaces/#transfroming-things-between-subspaces",
    "relUrl": "/docs/Linear-Algebra/Spaces/#transfroming-things-between-subspaces"
  },"105": {
    "doc": "Spaces",
    "title": "The Reduced Row Echelon Form $R=\\mathrm{rref}(A)$",
    "content": "The triangular echelon matrix $U$ . $U$ tells which columns are combinations of earlier columns (pivots are missing). Then $R$ tells us what those combinations are. The pivot columns of $R$ contain $I$. Easy to find special solutions. Every ‘‘free column” is a combination of earlier pivot columns. R reveals a “basis” for three fundamental subspaces: . | The column space of $A$ - choose the pivot columns of $A$ as a basis. | The row space of $A$ - choose the nonzero rows of $R$ as a basis. | The nullspace of $A$ - choose the special solutions to $Rx = 0$ (and $Ax = 0$). | . Suppose $Ax = 0$ has more unknowns than equations ($n &gt; m$, more columns than rows). There must be at least one free column. Then $Ax = 0$ has nonzero solutions. “Dimension” of nullspace is the number of free variables. $A=CR$ . \\[EA=R\\rightarrow A=E^{-1}R\\rightarrow A=CR\\] \\[\\left[\\begin{array}{ccc} \\operatorname{col basis} 1 &amp; \\text { col basis } 2 &amp; \\text { col basis 3} \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\end{array}\\right]\\left[\\begin{array}{ccc} \\text { row basis } 1 &amp; \\cdots \\cdot \\\\ \\text { row basis } 2 &amp; \\cdots \\cdot \\\\ \\text { row basis } 3 &amp; \\cdots \\cdot \\end{array}\\right]\\\\=(\\operatorname{col basis} 1)(\\text { row basis 1) }+(\\operatorname{col basis} 2)(\\text { row basis 2) }+(\\operatorname{col basis} 3)(\\text { row basis 3) }\\] Space . Definition: The space $\\mathbf{R}^{n}$ consists of all column vectors $v$ with $n$ components. (for complex numbers $\\mathbf{C}^{n}$) . The eight conditions are required: . | Axiom | Meaning | . | Commutative Low | $\\boldsymbol{v}+\\boldsymbol{w}=\\boldsymbol{w}+\\boldsymbol{v}$ | . | Distributive Law | $c(\\boldsymbol{v}+\\boldsymbol{w})=c \\boldsymbol{v}+c \\boldsymbol{w}$ | . | Unique “zero vector” | $0+\\boldsymbol{v}=\\boldsymbol{v}$ | . | … | … | . $\\mathbf{M}$ The vector space of all real 2 by 2 matrices. $\\mathbf{F}$ The vector space of all real functions $f(x)$. $\\mathbf{Z}$ The vector space that consists only of a zero vector. Subspace . Definition: A subspace of a vector space is a set of vectors (including 0) that satisfies two requirements: If v and ware vectors in the subspace and c is any scalar, then . | $\\boldsymbol{v}+\\boldsymbol{w}$ is in the subspace | $c \\boldsymbol{v}$ is in the subspace. | . | All linear combinations stay in the subspace. | Every subspace contains the zero vector. | Lines through the origin are also subspaces. | . Column space of $A$ - the ‘b’s . Definition: The column space consists of all linear combinations of the columns. ($\\mathbf{R}^{m}$) . The combinations are all possible vectors $A \\boldsymbol{x}$. They fill the column space $\\boldsymbol{C}(A)$. | The system $A x=b$ is solvable if and only if $b$ is in the column space of $A$. | . Any set of column vectors: $\\mathbf{S}=$ set of vectors in $\\mathbf{V}$ (probably not a subspace) . Subspace: $\\mathbf{S S}=$ all combinations of vectors in $\\mathbf{S}$ (definitely a subspace) . \\[\\mathbf{S S}=\\text { all } c_{1} \\boldsymbol{v}_{1}+\\cdots+c_{N} \\boldsymbol{v}_{N}=\\text { the subspace of } \\mathbf{V} \\text { \"spanned\" by } \\mathbf{S}\\] $\\mathbf{S S}$ is the smallest subspace containing $\\mathbf{S}$. Nullspace of $A$ - the ‘x’s . The nullspace $N(A)$ consists of all solutions to $A x=0$. These vectors $x$ are in $\\mathbf{R}^{n}$. The nullspace of $A$ consists of all combinations of the special solutions to $A x=0$. The free components correspond to columns with no pivots. The special choice (one or zero) is only for the free variables in the special solutions. Row space . \\[\\boldsymbol{v}^{\\mathrm{T}} \\boldsymbol{x}=\\mathbf{0}\\] All vectors $\\boldsymbol{x}$ in the nullspace must be orthogonal to $\\boldsymbol{v}$ in the row space. Left nullspace . \\[R^{\\mathrm{T}} \\boldsymbol{y}=\\mathbf{0}\\] \\[\\boldsymbol{y}^{\\mathrm{T}} R=\\mathbf{0}^{\\mathrm{T}}\\] The $y$’s in the left nullspace combine the rows to give the zero row. Matrix spaces . Function spaces . Rank . The true size of linear space $A$ is given by its rank. Definition: . | (computational) The rank of $A$ is the number of pivots. This number is $r$. | $A$ and $U$ and $R$ also have $r$ independent columns (the pivot columns) and independent rows. | The rank $r$ is the “dimension” of the column space. It is also the dimension of the row space. ($n - r$ is the dimension of the nullspace.) | . Rank 1 . Every rank one matrix is one column times one row： . \\[\\boldsymbol{A}=\\text { column times row }=u v^{\\mathrm{T}}\\] Rank 2 = rank 1 + rank 1 (A=CR) . \\[\\begin{array}{lll} \\text { Matrix } \\boldsymbol{A} \\\\ \\text { Rank two } \\end{array} \\quad A=\\left[\\begin{array}{lll} \\boldsymbol{u}_{1} &amp; \\boldsymbol{u}_{2} &amp; \\boldsymbol{u}_{3} \\end{array}\\right]\\left[\\begin{array}{c} \\boldsymbol{v}_{1}^{\\mathrm{T}} \\\\ \\boldsymbol{v}_{2}^{\\mathrm{T}} \\\\ \\text { zero row } \\end{array}\\right]=\\boldsymbol{u}_{1} \\boldsymbol{v}_{1}^{\\mathrm{T}}+\\boldsymbol{u}_{2} \\boldsymbol{v}_{2}^{\\mathrm{T}}=(\\operatorname{rank} 1)+(\\operatorname{rank} 1)\\] Every rank $r$ matrix is a sum of $r$ rank one matrices . Pivot columns of $A$ times nonzero rows of $R$. The row $\\left[\\begin{array}{lll} 0 &amp; 0 &amp; 0 \\end{array}\\right]$ simply disappeared. \\[\\boldsymbol{u}_{1}, \\ldots, \\boldsymbol{u}_{n}\\] are bases for the column space, . \\[\\boldsymbol{v}_{1}^{\\mathrm{T}}, \\ldots, \\boldsymbol{v}_{n}^{\\mathrm{T}}\\] are bases for the row space. Rank Theorem . The number of independent columns =the number of independent rows. Particular solution $Ax_p=b$ . Set free variables to be zero. \\[R x_{p}=\\left[\\begin{array}{cccc} \\mathbf{1} &amp; 3 &amp; \\mathbf{0} &amp; 2 \\\\ \\mathbf{0} &amp; 0 &amp; \\mathbf{1} &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right]\\left[\\begin{array}{l} \\mathbf{1} \\\\ 0 \\\\ \\mathbf{6} \\\\ 0 \\end{array}\\right]=\\left[\\begin{array}{l} \\mathbf{1} \\\\ \\mathbf{6} \\\\ 0 \\end{array}\\right] \\quad \\begin{aligned} &amp;\\text { Pivot variables } \\mathbf{1 , 6} \\\\ &amp;\\text { Solution } \\boldsymbol{x}_{\\boldsymbol{p}}=(\\mathbf{1}, \\mathbf{0}, \\mathbf{6}, \\mathbf{0}) . \\end{aligned}\\] \\[\\left[\\begin{array}{ll} A &amp; \\boldsymbol{b} \\end{array}\\right]\\rightarrow \\left[\\begin{array}{ll} R &amp; \\boldsymbol{d} \\end{array}\\right]\\] For invertible matrix $A$, . \\[\\left[\\begin{array}{ll} A &amp; \\boldsymbol{b} \\end{array}\\right] \\rightarrow \\left[\\begin{array}{ll} I &amp; A^{-1} \\boldsymbol{b} \\end{array}\\right]\\] . Full column rank ($m&gt;=n$) - overdetermined - one solution / no solution . Independent columns . \\[\\text { Full column rank } R=\\left[\\begin{array}{l} I \\\\ 0 \\end{array}\\right]=\\left[\\begin{array}{l} n \\text { by } n \\text { identity matrix } \\\\ m-n \\text { rows of zeros } \\end{array}\\right]\\] Every matrix A with full column rank ($r=n$) has all these properties: . | All columns of $A$ are pivot columns. | There are no free variables or special solutions. | The nullspace $N(A)$ contains only the zero vector $x=0$. | If $Ax=b$ has a solution (it might not) then it has only one solution. | . Full row rank ($m&lt;=n$) - underdetermined - one solution / infinitely many solutions . independent rows . Every matrix $A$ with full row rank ($r=m$) has all these properties: . | All rows have pivots, and $R$ has no zero rows. | $Ax = b$ has a solution for every right side $b$. | The column space is the whole space $\\mathbf{R}^{m}$ . | There are $n - r=n - m$ special solutions in the nullspace of $A$. | . The number of solutions depends on if the linear system has free variables. . Dimension . The dimension is measured by counting independent columns - rank $r$. \\[\\mathrm{Dimension \\ of \\ subspace = Rank \\ of \\ matrix}\\] The dimension of a space is the number of vectors in every basis. Basis . Independent vectors that “span the space”. Every vector in the space is a unique combination of the basis yectors. The columns of every invertible $n$ by $n$ matrix give a basis for $\\mathbf{R}^{n}$. The pivot columns of invertible $n$ by $n$ matrix $A$ are a basis for its column space. | The $r$ pivot rows of $R$ are a basis for the row spaces of $R$ and $A$ (same space). | The $r$ pivot columns of $A(!)$ are a basis for its column space $\\boldsymbol{C}(A)$. | The $n-r$ special solutions are a basis for the nullspaces of $A$ and $R$ (same space). | If $E A=R$, the last $m-r$ rows of $E$ are a basis for the left nullspace of $A$. | . Spanning a space . A set of vectors spans a space if their linear combinations fill the space. Linear Independence . Definition: . | The columns of $A$ are linearly independent when the only solution to $Ax = 0$ is $x = 0$. No other combination $Ax$ of the columns gives the zero vector. | The sequence of vectors $v_1 , \\ldots , v_n$ is linearly independent if the only combination that gives the zero vector is $0v_1 + 0v_2 + \\ldots + 0v_n$ . \\[x_{1} \\boldsymbol{v}_{1}+x_{2} \\boldsymbol{v}_{2}+\\cdots+x_{n} \\boldsymbol{v}_{n}=\\mathbf{0} \\quad \\text { only happens when all } x \\text { 's are zero. }\\] | . Full column rank: The columns of $A$ are independent exactly when the rank is $r = n$. There are $n$ pivots and no free variables. Only $x = 0$ is in the nullspace. ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Spaces/#the-reduced-row-echelon-form-rmathrmrrefa",
    "relUrl": "/docs/Linear-Algebra/Spaces/#the-reduced-row-echelon-form-rmathrmrrefa"
  },"106": {
    "doc": "Spaces",
    "title": "Fundamental Theorem of Linear Algebra. Pt.1",
    "content": "The column space and row space both have dimension $r$. The nullspaces have dimensions $n-r$ and $m-r$. Four Fundamental Subspaces . | The row space is $C(A^{\\mathrm{T}})$, a subspace of $\\mathbf{R}^{n}$. | The column space is $C(A)$, a subspace of $\\mathbf{R}^{m}$. | The nullspace is $N(A)$, a subspace of $\\mathbf{R}^{n}$. | The left nullspace is $N(A^{\\mathrm{T}})$, a subspace of $\\mathbf{R}^{m}$. | . Incidence matrix . Kirchhoff’s Voltage Law . \\[A \\boldsymbol{x}=\\boldsymbol{b}\\] Kirchhoff’s Current Law . \\[A^{\\mathrm{T}} \\boldsymbol{y}=\\mathbf{0}\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Spaces/#fundamental-theorem-of-linear-algebra-pt1",
    "relUrl": "/docs/Linear-Algebra/Spaces/#fundamental-theorem-of-linear-algebra-pt1"
  },"107": {
    "doc": "Spaces",
    "title": "Spaces",
    "content": " ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Spaces/",
    "relUrl": "/docs/Linear-Algebra/Spaces/"
  },"108": {
    "doc": "x",
    "title": "linear regression",
    "content": "Cost function . Gradient descent . Multivariable linear function . ",
    "url": "http://localhost:4000/docs/Machine-Learning/SuLearning/#linear-regression",
    "relUrl": "/docs/Machine-Learning/SuLearning/#linear-regression"
  },"109": {
    "doc": "x",
    "title": "classification",
    "content": "Cost function . Gradient descent . Multivariable classifier ( multiclassifier) . Sigmoid Function / Logistic Function . “transforming an arbitrary-valued function into a function better suited for classification.” . Transform the linear space into a curved space, still regression but additional extreme value won’t distort the prediction much as linear regression. Regularization . ",
    "url": "http://localhost:4000/docs/Machine-Learning/SuLearning/#classification",
    "relUrl": "/docs/Machine-Learning/SuLearning/#classification"
  },"110": {
    "doc": "x",
    "title": "x",
    "content": ". underfitting to overfitting . ",
    "url": "http://localhost:4000/docs/Machine-Learning/SuLearning/",
    "relUrl": "/docs/Machine-Learning/SuLearning/"
  },"111": {
    "doc": "Transformation",
    "title": "Good Basis",
    "content": "\\(X^{-1} A X=\\text { eigenvalues in } \\Lambda\\) . \\(U^{-1} A V=\\text { diagonal } \\Sigma\\) . Jordan form . independent eigenvectors $s$, constructed $n -s$ additional “ generalized” eigenvectors . \\[B^{-1} A B=\\text { Jordan form } J (B_{\\text {in }}=B_{\\text {out }}=\\text { generalized eigenvectors of } A)\\] . the generalized eigenvector is tied to the previous eigenvector. Like Laplace transform is a generalization of Fourier transform, Jordan form is a generalization of finding eigenbases. The rank of $J$ (and $A$) will be the total number of 1’s. The maximum rank is $n/2$. \\(B_{\\text {in }}=B_{\\text {out }}=\\text { Forier matrix } F\\) . Discrete Fourier Transform $Fx$ . . basis for function space . Orthogonality between even and odd functions . Fourier basis . \\[1, \\sin x, \\cos x, \\sin 2x, \\cos 2x, ...\\] linear algebra in function space, projecting vector $b$ onto the line through $a$. \\[a_i=\\boldsymbol{b}^\\mathrm{T}\\boldsymbol{a}/\\boldsymbol{a}^\\mathrm{T}\\boldsymbol{a}\\] Legendre basis . \\[1,x, x^2-\\frac{1}{3}, x^3-\\frac{3}{5}x,\\ldots\\] is the result of Grant-Schmidt: orthogonalize the power $1, x, x^2$ . Chebyshev basis . \\[1,x,2x^2-1,4x^3-3x,\\ldots\\] set $x=\\cos \\theta$ . \\[n^\\text{th} \\, \\text{degree Chebyshev polynomial}\\quad\\mathbf{T}_n(x)\\stackrel{\\mathcal{F}}\\longrightarrow \\mathbf{T}_n(\\cos \\theta) =\\cos n\\theta\\] ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Transformation/#good-basis",
    "relUrl": "/docs/Linear-Algebra/Transformation/#good-basis"
  },"112": {
    "doc": "Transformation",
    "title": "Transformation",
    "content": "linear transformation . \\[T(c \\boldsymbol{v}+d \\boldsymbol{w}) \\quad \\text { must equal } \\quad c T(\\boldsymbol{v})+d T(\\boldsymbol{w})\\] Shift is not linear: . \\[\\boldsymbol{v}+\\boldsymbol{w}+\\boldsymbol{u}_{0} \\quad \\text { is not } T(\\boldsymbol{v})+T(\\boldsymbol{w})=\\left(\\boldsymbol{v}+\\boldsymbol{u}_{0}\\right)+\\left(\\boldsymbol{w}+\\boldsymbol{u}_{0}\\right)\\] Equally spaced points go to equally spaced points. affine . linear-plus-shift transformation $T(v)=A v+u_{0}$. Straight lines stay straight although $T$ is not linear. affine provides the ability to move things. Linearity . \\[\\boldsymbol{u} =c_{1} \\boldsymbol{v}_{1}+c_{2} \\boldsymbol{v}_{2}+\\cdots+c_{n} \\boldsymbol{v}_{n}\\] must transform to: . \\[T(\\boldsymbol{u}) =c_{1} T\\left(\\boldsymbol{v}_{1}\\right)+c_{2} T\\left(\\boldsymbol{v}_{2}\\right)+\\cdots+c_{n} T\\left(\\boldsymbol{v}_{n}\\right)\\] Suppose you know $T(v)$ for all vectors $v_{1}, \\ldots, v_{n}$ in a basis Then you know $T(u)$ for every vector $u$ in the space. Calculus . \\[T=\\text { derivative}\\] \\[T^{+}=\\text {integral}\\] Fundamental Theorem of Calculus . Integration is the (pseudo)inverse of differentiation. The derivative of a constant function is zero. That zero is on the diagonal of $A^{+} A$. Calculus wouldn’t be calculus without that 1 -dimensional nullspace of $T=d / d x$. range . column space . Set of all outputs $T(v)$. kernel . nullspace . Set of all inputs for which $T(\\boldsymbol{v})=0$. Trigonometry (the double angle rule) . \\[A^2=\\left[\\begin{array}{rr} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{array}\\right]\\left[\\begin{array}{rr} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{array}\\right]=\\left[\\begin{array}{cc} \\cos ^{2} \\theta-\\sin ^{2} \\theta &amp; -2 \\sin \\theta \\cos \\theta \\\\ 2 \\sin \\theta \\cos \\theta &amp; \\cos ^{2} \\theta-\\sin ^{2} \\theta \\end{array}\\right]=\\left[\\begin{array}{rr} \\cos 2 \\theta &amp; -\\sin 2 \\theta \\\\ \\sin 2 \\theta &amp; \\cos 2 \\theta \\end{array}\\right]\\] Rotate back . \\[A B=\\left[\\begin{array}{rr} \\cos \\theta &amp; \\sin \\theta \\\\ -\\sin \\theta &amp; \\cos \\theta \\end{array}\\right]\\left[\\begin{array}{rr} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{array}\\right]=\\left[\\begin{array}{cc} \\cos ^{2} \\theta+\\sin ^{2} \\theta &amp; 0 \\\\ 0 &amp; \\cos ^{2} \\theta+\\sin ^{2} \\theta \\end{array}\\right]=I\\] isometric . $C=Q_{1}^{-1} A Q_{2}$ is isometric to $A$ if $Q_{1}$ and $Q_{2}$ are orthogonal. $A_{\\text {new }}=B^{-1} A B$ in the new basis of $b$ ‘s is similar to $A$ in the standard basis: . The best input-output bases are eigenvectors and/or singular vectors of $A$. \\[B^{-1} A B=\\Lambda= \\text{eigenvalues}\\] \\[B_{\\text {out }}^{-1} A B_{\\text {in }}=\\Sigma=$ \\text{singular values}\\] . ",
    "url": "http://localhost:4000/docs/Linear-Algebra/Transformation/",
    "relUrl": "/docs/Linear-Algebra/Transformation/"
  },"113": {
    "doc": "Transformations",
    "title": "Fourier transformation",
    "content": " ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#fourier-transformation",
    "relUrl": "/docs/Signals-Systems/Transformations/#fourier-transformation"
  },"114": {
    "doc": "Transformations",
    "title": "Continuous",
    "content": "Properties . Pairs . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#continuous",
    "relUrl": "/docs/Signals-Systems/Transformations/#continuous"
  },"115": {
    "doc": "Transformations",
    "title": "Discrete",
    "content": "Properties . Pairs . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#discrete",
    "relUrl": "/docs/Signals-Systems/Transformations/#discrete"
  },"116": {
    "doc": "Transformations",
    "title": "Duality",
    "content": ". ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#duality",
    "relUrl": "/docs/Signals-Systems/Transformations/#duality"
  },"117": {
    "doc": "Transformations",
    "title": "Laplace transform",
    "content": "Properties . Pairs . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#laplace-transform",
    "relUrl": "/docs/Signals-Systems/Transformations/#laplace-transform"
  },"118": {
    "doc": "Transformations",
    "title": "z-transform",
    "content": "Properties . Pairs . ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/#z-transform",
    "relUrl": "/docs/Signals-Systems/Transformations/#z-transform"
  },"119": {
    "doc": "Transformations",
    "title": "Transformations",
    "content": " ",
    "url": "http://localhost:4000/docs/Signals-Systems/Transformations/",
    "relUrl": "/docs/Signals-Systems/Transformations/"
  },"120": {
    "doc": "Home",
    "title": "Home",
    "content": "路漫漫其修远兮，吾将上下而求索。 . ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
}
